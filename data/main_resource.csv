link,title,description
http://arxiv.org/abs/2102.06219,Title: Silentium! Run-Analyse-Eradicate the Noise out of the DB/OS Stack,"When multiple tenants compete for resources, database performance tends to
suffer. Yet there are scenarios where guaranteed sub-millisecond latencies are
crucial, such as in real-time data processing, IoT devices, or when operating
in safety-critical environments. In this paper, we study how to make query
latencies deterministic in the face of noise (whether caused by other tenants
or unrelated operating system tasks). We perform controlled experiments with an
in-memory database engine in a multi-tenant setting, where we successively
eradicate noisy interference from within the system software stack, to the
point where the engine runs close to bare-metal on the underlying hardware.
We show that we can achieve query latencies comparable to the database engine
running as the sole tenant, but without noticeably impacting the workload of
competing tenants. We discuss these results in the context of ongoing efforts
to build custom operating systems for database workloads, and point out that
for certain use cases, the margin for improvement is rather narrow. In fact,
for scenarios like ours, existing operating systems might just be good enough,
provided that they are expertly configured. We then critically discuss these
findings in the light of a broader family of database systems (e.g., including
disk-based), and how to extend the approach of this paper accordingly."
http://arxiv.org/abs/2102.06228,Title: Learning Gaussian-Bernoulli RBMs using Difference of Convex Functions  Optimization,"The Gaussian-Bernoulli restricted Boltzmann machine (GB-RBM) is a useful
generative model that captures meaningful features from the given
$n$-dimensional continuous data. The difficulties associated with learning
GB-RBM are reported extensively in earlier studies. They indicate that the
training of the GB-RBM using the current standard algorithms, namely,
contrastive divergence (CD) and persistent contrastive divergence (PCD), needs
a carefully chosen small learning rate to avoid divergence which, in turn,
results in slow learning. In this work, we alleviate such difficulties by
showing that the negative log-likelihood for a GB-RBM can be expressed as a
difference of convex functions if we keep the variance of the conditional
distribution of visible units (given hidden unit states) and the biases of the
visible units, constant. Using this, we propose a stochastic {\em difference of
convex functions} (DC) programming (S-DCP) algorithm for learning the GB-RBM.
We present extensive empirical studies on several benchmark datasets to
validate the performance of this S-DCP algorithm. It is seen that S-DCP is
better than the CD and PCD algorithms in terms of speed of learning and the
quality of the generative model learnt."
http://arxiv.org/abs/2102.06229,Title: To Reuse or Not To Reuse? A Framework and System for Evaluating  Summarized Knowledge,"As the amount of information online continues to grow, a correspondingly
important opportunity is for individuals to reuse knowledge which has been
summarized by others rather than starting from scratch. However, appropriate
reuse requires judging the relevance, trustworthiness, and thoroughness of
others' knowledge in relation to an individual's goals and context. In this
work, we explore augmenting judgements of the appropriateness of reusing
knowledge in the domain of programming, specifically of reusing artifacts that
result from other developers' searching and decision making. Through an
analysis of prior research on sensemaking and trust, along with new interviews
with developers, we synthesized a framework for reuse judgements. The
interviews also validated that developers express a desire for help with
judging whether to reuse an existing decision. From this framework, we
developed a set of techniques for capturing the initial decision maker's
behavior and visualizing signals calculated based on the behavior, to
facilitate subsequent consumers' reuse decisions, instantiated in a prototype
system called Strata. Results of a user study suggest that the system
significantly improves the accuracy, depth, and speed of reusing decisions.
These results have implications for systems involving user-generated content in
which other users need to evaluate the relevance and trustworthiness of that
content."
http://arxiv.org/abs/2102.06231,Title: Optimization Issues in KL-Constrained Approximate Policy Iteration,"Many reinforcement learning algorithms can be seen as versions of approximate
policy iteration (API). While standard API often performs poorly, it has been
shown that learning can be stabilized by regularizing each policy update by the
KL-divergence to the previous policy. Popular practical algorithms such as
TRPO, MPO, and VMPO replace regularization by a constraint on KL-divergence of
consecutive policies, arguing that this is easier to implement and tune. In
this work, we study this implementation choice in more detail. We compare the
use of KL divergence as a constraint vs. as a regularizer, and point out
several optimization issues with the widely-used constrained approach. We show
that the constrained algorithm is not guaranteed to converge even on simple
problem instances where the constrained problem can be solved exactly, and in
fact incurs linear expected regret. With approximate implementation using
softmax policies, we show that regularization can improve the optimization
landscape of the original objective. We demonstrate these issues empirically on
several bandit and RL environments."
http://arxiv.org/abs/2102.06234,Title: Robotic Tool Tracking under Partially Visible Kinematic Chain: A Unified  Approach,"Anytime a robot manipulator is controlled via visual feedback, the
transformation between the robot and camera frame must be known. However, in
the case where cameras can only capture a portion of the robot manipulator in
order to better perceive the environment being interacted with, there is
greater sensitivity to errors in calibration of the base-to-camera transform. A
secondary source of uncertainty during robotic control are inaccuracies in
joint angle measurements which can be caused by biases in positioning and
complex transmission effects such as backlash and cable stretch. In this work,
we bring together these two sets of unknown parameters into a unified problem
formulation when the kinematic chain is partially visible in the camera view.
We prove that these parameters are non-identifiable implying that explicit
estimation of them is infeasible. To overcome this, we derive a smaller set of
parameters we call Lumped Error since it lumps together the errors of
calibration and joint angle measurements. A particle filter method is presented
and tested in simulation and on two real world robots to estimate the Lumped
Error and show the efficiency of this parameter reduction."
http://arxiv.org/abs/2102.06235,Title: Deep Reinforcement Agent for Scheduling in HPC,"Cluster scheduler is crucial in high-performance computing (HPC). It
determines when and which user jobs should be allocated to available system
resources. Existing cluster scheduling heuristics are developed by human
experts based on their experience with specific HPC systems and workloads.
However, the increasing complexity of computing systems and the highly dynamic
nature of application workloads have placed tremendous burden on manually
designed and tuned scheduling heuristics. More aggressive optimization and
automation are needed for cluster scheduling in HPC. In this work, we present
an automated HPC scheduling agent named DRAS (Deep Reinforcement Agent for
Scheduling) by leveraging deep reinforcement learning. DRAS is built on a
novel, hierarchical neural network incorporating special HPC scheduling
features such as resource reservation and backfilling. A unique training
strategy is presented to enable DRAS to rapidly learn the target environment.
Once being provided a specific scheduling objective given by system manager,
DRAS automatically learns to improve its policy through interaction with the
scheduling environment and dynamically adjusts its policy as workload changes.
The experiments with different production workloads demonstrate that DRAS
outperforms the existing heuristic and optimization approaches by up to 45%."
http://arxiv.org/abs/2102.06237,Title: Knowledge Infused Policy Gradients for Adaptive Pandemic Control,"COVID-19 has impacted nations differently based on their policy
implementations. The effective policy requires taking into account public
information and adaptability to new knowledge. Epidemiological models built to
understand COVID-19 seldom provide the policymaker with the capability for
adaptive pandemic control (APC). Among the core challenges to be overcome
include (a) inability to handle a high degree of non-homogeneity in different
contributing features across the pandemic timeline, (b) lack of an approach
that enables adaptive incorporation of public health expert knowledge, and (c)
transparent models that enable understanding of the decision-making process in
suggesting policy. In this work, we take the early steps to address these
challenges using Knowledge Infused Policy Gradient (KIPG) methods. Prior work
on knowledge infusion does not handle soft and hard imposition of varying forms
of knowledge in disease information and guidelines to necessarily comply with.
Furthermore, the models do not attend to non-homogeneity in feature counts,
manifesting as partial observability in informing the policy. Additionally,
interpretable structures are extracted post-learning instead of learning an
interpretable model required for APC. To this end, we introduce a mathematical
framework for KIPG methods that can (a) induce relevant feature counts over
multi-relational features of the world, (b) handle latent non-homogeneous
counts as hidden variables that are linear combinations of kernelized
aggregates over the features, and (b) infuse knowledge as functional
constraints in a principled manner. The study establishes a theory for imposing
hard and soft constraints and simulates it through experiments. In comparison
with knowledge-intensive baselines, we show quick sample efficient adaptation
to new knowledge and interpretability in the learned policy, especially in a
pandemic context."
http://arxiv.org/abs/2102.06238,"Title: Regret, stability, and fairness in matching markets with bandit learners","We consider the two-sided matching market with bandit learners. In the
standard matching problem, users and providers are matched to ensure incentive
compatibility via the notion of stability. However, contrary to the core
assumption of the matching problem, users and providers do not know their true
preferences a priori and must learn them. To address this assumption, recent
works propose to blend the matching and multi-armed bandit problems. They
establish that it is possible to assign matchings that are stable (i.e.,
incentive-compatible) at every time step while also allowing agents to learn
enough so that the system converges to matchings that are stable under the
agents' true preferences. However, while some agents may incur low regret under
these matchings, others can incur high regret -- specifically, $\Omega(T)$
optimal regret where $T$ is the time horizon. In this work, we incorporate
costs and transfers in the two-sided matching market with bandit learners in
order to faithfully model competition between agents. We prove that, under our
framework, it is possible to simultaneously guarantee four desiderata: (1)
incentive compatibility, i.e., stability, (2) low regret, i.e., $O(\log(T))$
optimal regret, (3) fairness in the distribution of regret among agents, and
(4) high social welfare."
http://arxiv.org/abs/2102.06243,Title: Sample-Optimal PAC Learning of Halfspaces with Malicious Noise,"We study efficient PAC learning of homogeneous halfspaces in $\mathbb{R}^d$
in the presence of malicious noise of Valiant~(1985). This is a challenging
noise model and only until recently has near-optimal noise tolerance bound been
established under the mild condition that the unlabeled data distribution is
isotropic log-concave. However, it remains unsettled how to obtain the optimal
sample complexity simultaneously. In this work, we present a new analysis for
the algorithm of Awasthi et al.~(2017) and show that it essentially achieves
the near-optimal sample complexity bound of $\tilde{O}(d)$, improving the best
known result of $\tilde{O}(d^2)$. Our main ingredient is a novel incorporation
of a Matrix Chernoff-type inequality to bound the spectrum of an empirical
covariance matrix for well-behaved distributions, in conjunction with a careful
exploration of the localization schemes of Awasthi et al.~(2017). We further
extend the algorithm and analysis to the more general and stronger nasty noise
model of Bshouty~et~al. (2002), showing that it is still possible to achieve
near-optimal noise tolerance and sample complexity in polynomial time."
http://arxiv.org/abs/2102.06245,"Title: A Survey on Ransomware: Evolution, Taxonomy, and Defense Solutions","In recent years, ransomware has been one of the most notorious malware
targeting end users, governments, and business organizations. It has become a
very profitable business for cybercriminals with revenues of millions of
dollars, and a very serious threat to organizations with financial loss of
billions of dollars. Numerous studies were proposed to address the ransomware
threat, including surveys that cover certain aspects of ransomware research.
However, no study exists in the literature that gives the complete picture on
ransomware and ransomware defense research with respect to the diversity of
targeted platforms. Since ransomware is already prevalent in
PCs/workstations/desktops/laptops, is becoming more prevalent in mobile
devices, and has already hit IoT/CPS recently, and will likely grow further in
the IoT/CPS domain very soon, understanding ransomware and analyzing defense
mechanisms with respect to target platforms is becoming more imperative. In
order to fill this gap and motivate further research, in this paper, we present
a comprehensive survey on ransomware and ransomware defense research with
respect to PCs/workstations, mobile devices, and IoT/CPS platforms.
Specifically, covering 137 studies over the period of 1990-2020, we give a
detailed overview of ransomware evolution, comprehensively analyze the key
building blocks of ransomware, present a taxonomy of notable ransomware
families, and provide an extensive overview of ransomware defense research
(i.e., analysis, detection, and recovery) with respect to platforms of
PCs/workstations, mobile devices, and IoT/CPS. Moreover, we derive an extensive
list of open issues for future ransomware research. We believe this survey will
motivate further research by giving a complete picture on state-of-the-art
ransomware research."
http://arxiv.org/abs/2102.06246,Title: Why Don't Developers Detect Improper Input Validation?'; DROP TABLE  Papers; --,"Improper Input Validation (IIV) is a software vulnerability that occurs when
a system does not safely handle input data. Even though IIV is easy to detect
and fix, it still commonly happens in practice. In this paper, we study to what
extent developers can detect IIV and investigate underlying reasons. This
knowledge is essential to better understand how to support developers in
creating secure software systems. We conduct an online experiment with 146
participants, of which 105 report at least three years of professional software
development experience. Our results show that the existence of a visible attack
scenario facilitates the detection of IIV vulnerabilities and that a
significant portion of developers who did not find the vulnerability initially
could identify it when warned about its existence. Yet, a total of 60
participants could not detect the vulnerability even after the warning. Other
factors, such as the frequency with which the participants perform code
reviews, influence the detection of IIV. Data and materials:
https://doi.org/10.5281/zenodo.3996696"
http://arxiv.org/abs/2102.06247,Title: Continuum: Simple Management of Complex Continual Learning Scenarios,"Continual learning is a machine learning sub-field specialized in settings
with non-iid data. Hence, the training data distribution is not static and
drifts through time. Those drifts might cause interferences in the trained
model and knowledge learned on previous states of the data distribution might
be forgotten. Continual learning's challenge is to create algorithms able to
learn an ever-growing amount of knowledge while dealing with data distribution
drifts.
One implementation difficulty in these field is to create data loaders that
simulate non-iid scenarios. Indeed, data loaders are a key component for
continual algorithms. They should be carefully designed and reproducible. Small
errors in data loaders have a critical impact on algorithm results, e.g. with
bad preprocessing, wrong order of data or bad test set. Continuum is a simple
and efficient framework with numerous data loaders that avoid researcher to
spend time on designing data loader and eliminate time-consuming errors. Using
our proposed framework, it is possible to directly focus on the model design by
using the multiple scenarios and evaluation metrics implemented. Furthermore
the framework is easily extendable to add novel settings for specific needs."
http://arxiv.org/abs/2102.06249,Title: Securing RPL using Network Coding: The Chained Secure Mode (CSM),"As the de facto routing protocol for many Internet of Things (IoT) networks
nowadays, and to assure the confidentiality and integrity of its control
messages, the Routing Protocol for Low Power and Lossy Networks (RPL)
incorporates three modes of security: the Unsecured Mode (UM), Preinstalled
Secure Mode (PSM), and the Authenticated Secure Mode (ASM). While the PSM and
ASM are intended to protect against external routing attacks and some replay
attacks (through an optional replay protection mechanism), recent research
showed that RPL in PSM is still vulnerable to many routing attacks, both
internal and external. In this paper, we propose a novel secure mode for RPL,
the Chained Secure Mode (CSM), based on the concept of intraflow Network Coding
(NC). The CSM is designed to enhance RPL resilience and mitigation capability
against replay attacks while allowing the integration with external security
measures such as Intrusion Detection Systems (IDSs). The security and
performance of the proposed CSM were evaluated and compared against RPL in UM
and PSM (with and without the optional replay protection) under several routing
attacks: the Neighbor attack (NA), Wormhole (WH), and CloneID attack (CA),
using average packet delivery rate (PDR), End-to-End (E2E) latency, and power
consumption as metrics. It showed that CSM has better performance and more
enhanced security than both the UM and PSM with the replay protection, while
mitigating both the NA and WH attacks and significantly reducing the effect of
the CA in the investigated scenarios."
http://arxiv.org/abs/2102.06251,Title: Towards DeepSentinel: An extensible corpus of labelled Sentinel-1 and -2  imagery and a general-purpose sensor-fusion semantic embedding model,"Earth observation offers new insight into anthropogenic changes to nature,
and how these changes are effecting (and are effected by) the built environment
and the real economy. With the global availability of medium-resolution
(10-30m) synthetic aperture radar (SAR) Sentinel-1 and multispectral Sentinel-2
imagery, machine learning can be employed to offer these insights at scale,
unbiased to the reporting of companies and countries. In this paper, I
introduce DeepSentinel, a data pipeline and experimentation framework for
producing general-purpose semantic embeddings of paired Sentinel-1 and
Sentinel-2 imagery. I document the development of an extensible corpus of
labelled and unlabelled imagery for the purposes of sensor fusion research.
With this new dataset I develop a set of experiments applying popular
self-supervision methods and encoder architectures to a land cover
classification problem. Tile2vec spatial encoding with a self-attention enabled
ResNet model outperforms deeper ResNet variants as well as pretraining with
variational autoencoding and contrastive loss. All supporting and derived data
and code are made publicly available."
http://arxiv.org/abs/2102.06253,Title: Speculative Path Planning,"Parallelization of A* path planning is mostly limited by the number of
possible motions, which is far less than the level of parallelism that modern
processors support. In this paper, we go beyond the limitations of traditional
parallelism of A* and propose Speculative Path Planning to accelerate the
search when there are abundant idle resources. The key idea of our approach is
predicting future state expansions relying on patterns among expansions and
aggressively parallelize the computations of prospective states (i.e.
pre-evaluate the expensive collision checking operation of prospective nodes).
This method allows us to maintain the same search order as of vanilla A* and
safeguard any optimality guarantees. We evaluate our method on various
configurations and show that on a machine with 32 physical cores, our method
improves the performance around 11x and 10x on average over counterpart
single-threaded and multi-threaded implementations respectively. The code to
our paper can be found here:
https://github.com/bakhshalipour/speculative-path-planning."
http://arxiv.org/abs/2102.06254,Title: Fair Robust Assignment using Redundancy,"We study the consideration of fairness in redundant assignment for
multi-agent task allocation. It has recently been shown that redundant
assignment of agents to tasks provides robustness to uncertainty in task
performance. However, the question of how to fairly assign these redundant
resources across tasks remains unaddressed. In this paper, we present a novel
problem formulation for fair redundant task allocation, which we cast as the
optimization of worst-case task costs under a cardinality constraint. Solving
this problem optimally is NP-hard. Therefore, we exploit properties of
supermodularity to propose a polynomial-time, near-optimal solution. In
supermodular redundant assignment, the use of additional agents always improves
task costs. Therefore, we provide a solution set that is $\alpha$ times larger
than the cardinality constraint. This constraint relaxation enables our
approach to achieve a super-optimal cost by using a sub-optimal assignment
size.
We derive the sub-optimality bound on this cardinality relaxation, $\alpha$.
Additionally, we demonstrate that our algorithm performs near-optimally without
the cardinality relaxation. We show the algorithm in simulations of redundant
assignments of robots to goal nodes on transport networks with uncertain travel
times. Empirically, our algorithm outperforms benchmarks, scales to large
problems, and provides improvements in both fairness and average utility."
http://arxiv.org/abs/2102.06258,Title: On Graph Matching Using Generalized Seed Side-Information,"In this paper, matching pairs of stocahstically generated graphs in the
presence of generalized seed side-information is considered. The graph matching
problem emerges naturally in various applications such as social network
de-anonymization, image processing, DNA sequencing, and natural language
processing. A pair of randomly generated labeled Erdos-Renyi graphs with
pairwise correlated edges are considered. It is assumed that the matching
strategy has access to the labeling of the vertices in the first graph, as well
as a collection of shortlists -- called ambiguity sets -- of possible labels
for the vertices of the second graph. The objective is to leverage the
correlation among the edges of the graphs along with the side-information
provided in the form of ambiguity sets to recover the labels of the vertices in
the second graph. This scenario can be viewed as a generalization of the seeded
graph matching problem, where the ambiguity sets take a specific form such that
the exact labels for a subset of vertices in the second graph are known prior
to matching. A matching strategy is proposed which operates by evaluating the
joint typicality of the adjacency matrices of the graphs. Sufficient conditions
on the edge statistics as well as ambiguity set statistics are derived under
which the proposed matching strategy successfully recovers the labels of the
vertices in the second graph. Additionally, Fano-type arguments are used to
derive general necessary conditions for successful matching."
http://arxiv.org/abs/2102.06260,Title: Selecting Treatment Effects Models for Domain Adaptation Using Causal  Knowledge,"Selecting causal inference models for estimating individualized treatment
effects (ITE) from observational data presents a unique challenge since the
counterfactual outcomes are never observed. The problem is challenged further
in the unsupervised domain adaptation (UDA) setting where we only have access
to labeled samples in the source domain, but desire selecting a model that
achieves good performance on a target domain for which only unlabeled samples
are available. Existing techniques for UDA model selection are designed for the
predictive setting. These methods examine discriminative density ratios between
the input covariates in the source and target domain and do not factor in the
model's predictions in the target domain. Because of this, two models with
identical performance on the source domain would receive the same risk score by
existing methods, but in reality, have significantly different performance in
the test domain. We leverage the invariance of causal structures across domains
to propose a novel model selection metric specifically designed for ITE methods
under the UDA setting. In particular, we propose selecting models whose
predictions of interventions' effects satisfy known causal structures in the
target domain. Experimentally, our method selects ITE models that are more
robust to covariate shifts on several healthcare datasets, including estimating
the effect of ventilation in COVID-19 patients from different geographic
locations."
http://arxiv.org/abs/2102.06261,Title: Unsupervised Extractive Summarization using Pointwise Mutual Information,"Unsupervised approaches to extractive summarization usually rely on a notion
of sentence importance defined by the semantic similarity between a sentence
and the document. We propose new metrics of relevance and redundancy using
pointwise mutual information (PMI) between sentences, which can be easily
computed by a pre-trained language model. Intuitively, a relevant sentence
allows readers to infer the document content (high PMI with the document), and
a redundant sentence can be inferred from the summary (high PMI with the
summary). We then develop a greedy sentence selection algorithm to maximize
relevance and minimize redundancy of extracted sentences. We show that our
method outperforms similarity-based methods on datasets in a range of domains
including news, medical journal articles, and personal anecdotes."
http://arxiv.org/abs/2102.06265,Title: Hedging of Financial Derivative Contracts via Monte Carlo Tree Search,"The construction of approximate replication strategies for derivative
contracts in incomplete markets is a key problem of financial engineering.
Recently Reinforcement Learning algorithms for pricing and hedging under
realistic market conditions have attracted significant interest. While
financial research mostly focused on variations of $Q$-learning, in Artificial
Intelligence Monte Carlo Tree Search is the recognized state-of-the-art method
for various planning problems, such as the games of Hex, Chess, Go,... This
article introduces Monte Carlo Tree Search for the hedging of financial
derivatives in realistic markets and shows that there are good reasons, both on
the theoretical and practical side, to favor it over other Reinforcement
Learning methods."
http://arxiv.org/abs/2102.06267,Title: On Agnostic PAC Learning using $\mathcal{L}_2$-polynomial Regression and  Fourier-based Algorithms,"We develop a framework using Hilbert spaces as a proxy to analyze PAC
learning problems with structural properties. We consider a joint Hilbert space
incorporating the relation between the true label and the predictor under a
joint distribution $D$. We demonstrate that agnostic PAC learning with 0-1 loss
is equivalent to an optimization in the Hilbert space domain. With our model,
we revisit the PAC learning problem using methods based on least-squares such
as $\mathcal{L}_2$ polynomial regression and Linial's low-degree algorithm. We
study learning with respect to several hypothesis classes such as half-spaces
and polynomial-approximated classes (i.e., functions approximated by a
fixed-degree polynomial). We prove that (under some distributional assumptions)
such methods obtain generalization error up to $2opt$ with $opt$ being the
optimal error of the class. Hence, we show the tightest bound on generalization
error when $opt\leq 0.2$."
http://arxiv.org/abs/2102.06269,Title: kPAM 2.0: Feedback Control for Category-Level Robotic Manipulation,"In this paper, we explore generalizable, perception-to-action robotic
manipulation for precise, contact-rich tasks. In particular, we contribute a
framework for closed-loop robotic manipulation that automatically handles a
category of objects, despite potentially unseen object instances and
significant intra-category variations in shape, size and appearance. Previous
approaches typically build a feedback loop on top of a real-time 6-DOF pose
estimator. However, representing an object with a parameterized transformation
from a fixed geometric template does not capture large intra-category shape
variation. Hence we adopt the keypoint-based object representation proposed in
kPAM for category-level pick-and-place, and extend it to closed-loop
manipulation policies with contact-rich tasks. We first augment keypoints with
local orientation information. Using the oriented keypoints, we propose a novel
object-centric action representation in terms of regulating the linear/angular
velocity or force/torque of these oriented keypoints. This formulation is
surprisingly versatile -- we demonstrate that it can accomplish contact-rich
manipulation tasks that require precision and dexterity for a category of
objects with different shapes, sizes and appearances, such as peg-hole
insertion for pegs and holes with significant shape variation and tight
clearance. With the proposed object and action representation, our framework is
also agnostic to the robot grasp pose and initial object configuration, making
it flexible for integration and deployment."
http://arxiv.org/abs/2102.06271,Title: Straggler-Resilient Distributed Machine Learning with Dynamic Backup  Workers,"With the increasing demand for large-scale training of machine learning
models, consensus-based distributed optimization methods have recently been
advocated as alternatives to the popular parameter server framework. In this
paradigm, each worker maintains a local estimate of the optimal parameter
vector, and iteratively updates it by waiting and averaging all estimates
obtained from its neighbors, and then corrects it on the basis of its local
dataset. However, the synchronization phase can be time consuming due to the
need to wait for \textit{stragglers}, i.e., slower workers. An efficient way to
mitigate this effect is to let each worker wait only for updates from the
fastest neighbors before updating its local parameter. The remaining neighbors
are called \textit{backup workers.} To minimize the globally training time over
the network, we propose a fully distributed algorithm to dynamically determine
the number of backup workers for each worker. We show that our algorithm
achieves a linear speedup for convergence (i.e., convergence performance
increases linearly with respect to the number of workers). We conduct extensive
experiments on MNIST and CIFAR-10 to verify our theoretical results."
http://arxiv.org/abs/2102.06272,Title: A reproduction of Apple's bi-directional LSTM models for language  identification in short strings,"Language Identification is the task of identifying a document's language. For
applications like automatic spell checker selection, language identification
must use very short strings such as text message fragments. In this work, we
reproduce a language identification architecture that Apple briefly sketched in
a blog post. We confirm the bi-LSTM model's performance and find that it
outperforms current open-source language identifiers. We further find that its
language identification mistakes are due to confusion between related
languages."
http://arxiv.org/abs/2102.06274,Title: Speech-language Pre-training for End-to-end Spoken Language  Understanding,"End-to-end (E2E) spoken language understanding (SLU) can infer semantics
directly from speech signal without cascading an automatic speech recognizer
(ASR) with a natural language understanding (NLU) module. However, paired
utterance recordings and corresponding semantics may not always be available or
sufficient to train an E2E SLU model in a real production environment. In this
paper, we propose to unify a well-optimized E2E ASR encoder (speech) and a
pre-trained language model encoder (language) into a transformer decoder. The
unified speech-language pre-trained model (SLP) is continually enhanced on
limited labeled data from a target domain by using a conditional masked
language model (MLM) objective, and thus can effectively generate a sequence of
intent, slot type, and slot value for given input speech in the inference. The
experimental results on two public corpora show that our approach to E2E SLU is
superior to the conventional cascaded method. It also outperforms the present
state-of-the-art approaches to E2E SLU with much less paired data."
http://arxiv.org/abs/2102.06275,Title: Large Scale Distributed Collaborative Unlabeled Motion Planning with  Graph Policy Gradients,"In this paper, we present a learning method to solve the unlabelled motion
problem with motion constraints and space constraints in 2D space for a large
number of robots. To solve the problem of arbitrary dynamics and constraints we
propose formulating the problem as a multi-agent problem. We are able to
demonstrate the scalability of our methods for a large number of robots by
employing a graph neural network (GNN) to parameterize policies for the robots.
The GNN reduces the dimensionality of the problem by learning filters that
aggregate information among robots locally, similar to how a convolutional
neural network is able to learn local features in an image. Additionally, by
employing a GNN we are also able to overcome the computational overhead of
training policies for a large number of robots by first training graph filters
for a small number of robots followed by zero-shot policy transfer to a larger
number of robots. We demonstrate the effectiveness of our framework through
various simulations."
http://arxiv.org/abs/2102.06277,Title: K-Hairstyle: A Large-scale Korean hairstyle dataset for virtual hair  editing and hairstyle classification,"The hair and beauty industry is one of the fastest growing industries. This
led to the development of various applications, such as virtual hair dyeing or
hairstyle translations, to satisfy the need of the customers. Although there
are several public hair datasets available for these applications, they consist
of limited number of images with low resolution, which restrict their
performance on high-quality hair editing. Therefore, we introduce a novel
large-scale Korean hairstyle dataset, K-hairstyle, 256,679 with high-resolution
images. In addition, K-hairstyle contains various hair attributes annotated by
Korean expert hair stylists and hair segmentation masks. We validate the
effectiveness of our dataset by leveraging several applications, such as
hairstyle translation, and hair classification and hair retrieval. Furthermore,
we will release K-hairstyle soon."
http://arxiv.org/abs/2102.06278,Title: When and How Mixup Improves Calibration,"In many machine learning applications, it is important for the model to
provide confidence scores that accurately captures its prediction uncertainty.
Although modern learning methods have achieved great success in predictive
accuracy, generating calibrated confidence scores remains a major challenge.
Mixup, a popular yet simple data augmentation technique based on taking convex
combinations of pairs of training examples, has been empirically found to
significantly improve confidence calibration across diverse applications.
However, when and how Mixup helps calibration is still mysterious. In this
paper, we theoretically prove that Mixup improves calibration in
\textit{high-dimensional} settings by investigating two natural data models on
classification and regression. Interestingly, the calibration benefit of Mixup
increases as the model capacity increases. We support our theories with
experiments on common architectures and data sets. In addition, we study how
Mixup improves calibration in semi-supervised learning. While incorporating
unlabeled data can sometimes make the model less calibrated, adding Mixup
training mitigates this issue and provably improves calibration. Our analysis
provides new insights and a framework to understand Mixup and calibration."
http://arxiv.org/abs/2102.06279,Title: A Multi-View Approach To Audio-Visual Speaker Verification,"Although speaker verification has conventionally been an audio-only task,
some practical applications provide both audio and visual streams of input. In
these cases, the visual stream provides complementary information and can often
be leveraged in conjunction with the acoustics of speech to improve
verification performance. In this study, we explore audio-visual approaches to
speaker verification, starting with standard fusion techniques to learn joint
audio-visual (AV) embeddings, and then propose a novel approach to handle
cross-modal verification at test time. Specifically, we investigate unimodal
and concatenation based AV fusion and report the lowest AV equal error rate
(EER) of 0.7% on the VoxCeleb1 dataset using our best system. As these methods
lack the ability to do cross-modal verification, we introduce a multi-view
model which uses a shared classifier to map audio and video into the same
space. This new approach achieves 28% EER on VoxCeleb1 in the challenging
testing condition of cross-modal verification."
http://arxiv.org/abs/2102.06280,Title: Improving Fault Localization by Integrating Value and Predicate Based  Causal Inference Techniques,"Statistical fault localization (SFL) techniques use execution profiles and
success/failure information from software executions, in conjunction with
statistical inference, to automatically score program elements based on how
likely they are to be faulty. SFL techniques typically employ one type of
profile data: either coverage data, predicate outcomes, or variable values.
Most SFL techniques actually measure correlation, not causation, between
profile values and success/failure, and so they are subject to confounding bias
that distorts the scores they produce. This paper presents a new SFL technique,
named \emph{UniVal}, that uses causal inference techniques and machine learning
to integrate information about both predicate outcomes and variable values to
more accurately estimate the true failure-causing effect of program statements.
\emph{UniVal} was empirically compared to several coverage-based,
predicate-based, and value-based SFL techniques on 800 program versions with
real faults."
http://arxiv.org/abs/2102.06282,Title: No-Regret Algorithms for Time-Varying Bayesian Optimization,"In this paper, we consider the time-varying Bayesian optimization problem.
The unknown function at each time is assumed to lie in an RKHS (reproducing
kernel Hilbert space) with a bounded norm. We adopt the general variation
budget model to capture the time-varying environment, and the variation is
characterized by the change of the RKHS norm. We adapt the restart and sliding
window mechanism to introduce two GP-UCB type algorithms: R-GP-UCB and
SW-GP-UCB, respectively. We derive the first (frequentist) regret guarantee on
the dynamic regret for both algorithms. Our results not only recover previous
linear bandit results when a linear kernel is used, but complement the previous
regret analysis of time-varying Gaussian process bandit under a Bayesian-type
regularity assumption, i.e., each function is a sample from a Gaussian process."
http://arxiv.org/abs/2102.06283,Title: I Know What You Imported Last Summer: A study of security threats in  thePython ecosystem,"The popularity of Python has risen rapidly over the past 15 years. It is a
major language in some of the most exciting technologies today. This popularity
has led to a large ecosystem of third-party packages available via the pip
package registry which hosts more than 200,000 packages. These third-party
packages can be reused by simply importing the package after installing using
package managers like pip. The ease of reuse of third-party software comes with
security risks putting millions of users in danger. In this project, we study
the ecosystem to analyze this threat. The mature ecosystem of Python has
multiple weak spots that we highlight in our project. First, we demonstrate how
trivial it is to exploit the Python ecosystem. Then, we systematically analyze
dependencies amongst packages, maintainers, and publicly reported security
issues. Most attacks are possible only if users install malicious packages. We
thus try to analyze and evaluate different methods used by attackers to force
incorrect downloads. We quantify your ideas by estimating the potential threat
that can be caused by exploiting a popular Python package. We also discuss
methods used in the industry to defend against such attacks"
http://arxiv.org/abs/2102.06284,Title: What does LIME really see in images?,"The performance of modern algorithms on certain computer vision tasks such as
object recognition is now close to that of humans. This success was achieved at
the price of complicated architectures depending on millions of parameters and
it has become quite challenging to understand how particular predictions are
made. Interpretability methods propose to give us this understanding. In this
paper, we study LIME, perhaps one of the most popular. On the theoretical side,
we show that when the number of generated examples is large, LIME explanations
are concentrated around a limit explanation for which we give an explicit
expression. We further this study for elementary shape detectors and linear
models. As a consequence of this analysis, we uncover a connection between LIME
and integrated gradients, another explanation method. More precisely, the LIME
explanations are similar to the sum of integrated gradients over the
superpixels used in the preprocessing step of LIME."
http://arxiv.org/abs/2102.06285,Title: Does Culture Matter? Impact of Individualism and Uncertainty Avoidance  on App Reviews,"Mobile applications are often used by an international audience and therefore
receive a high daily amount of user reviews from various countries. Previous
work found evidence that app store reviews contain helpful information for
software evolution processes. However, the cultural diversity of the reviews
and its consequences on specific user feedback characteristics has only been
researched to a limited extent so far. In this paper, we examine the influence
of two cultural dimensions, Individualism and Uncertainty Avoidance on user
feedback in Apple app store reviews written in different languages. For this
purpose, we collected 647,141 reviews from eight countries and written in five
languages over a period of six months. We then used manual content analysis and
automated processing to examine a sample of 3,120 reviews. The results show
that there is a statistically significant influence of Individualism and
Uncertainty Avoidance on user feedback characteristics. The results of this
study will help researchers and practitioners to reduce algorithm bias caused
by less diversified training and test data and to raise awareness of the
importance of analyzing diversified user feedback."
http://arxiv.org/abs/2102.06288,Title: Embracing Domain Differences in Fake News: Cross-domain Fake News  Detection using Multi-modal Data,"With the rapid evolution of social media, fake news has become a significant
social problem, which cannot be addressed in a timely manner using manual
investigation. This has motivated numerous studies on automating fake news
detection. Most studies explore supervised training models with different
modalities (e.g., text, images, and propagation networks) of news records to
identify fake news. However, the performance of such techniques generally drops
if news records are coming from different domains (e.g., politics,
entertainment), especially for domains that are unseen or rarely-seen during
training. As motivation, we empirically show that news records from different
domains have significantly different word usage and propagation patterns.
Furthermore, due to the sheer volume of unlabelled news records, it is
challenging to select news records for manual labelling so that the
domain-coverage of the labelled dataset is maximized. Hence, this work: (1)
proposes a novel framework that jointly preserves domain-specific and
cross-domain knowledge in news records to detect fake news from different
domains; and (2) introduces an unsupervised technique to select a set of
unlabelled informative news records for manual labelling, which can be
ultimately used to train a fake news detection model that performs well for
many domains while minimizing the labelling cost. Our experiments show that the
integration of the proposed fake news model and the selective annotation
approach achieves state-of-the-art performance for cross-domain news datasets,
while yielding notable improvements for rarely-appearing domains in news
datasets."
http://arxiv.org/abs/2102.06289,"Title: Understanding the attitudes, knowledge sharing behaviors and task  performance of core developers: A longitudinal study","Context: Prior research has established that a few individuals generally
dominate project communication and source code changes during software
development, regardless of task assignments at project initiation. Objective:
While this phenomenon has been noted, prior research has not sought to
understand these dominant individuals. Previous work has found that core
communicators are the gatekeepers of their teams' knowledge, and the
performance of these members was correlated with their teams' success. Building
on this work, we have employed a longitudinal approach to study the way core
developers' attitudes, knowledge sharing behaviors and task performance change
over the course of their project. Method: We first used Social Network Analysis
(SNA) and standard statistical analysis techniques to identify and select
artifacts and central practitioners from ten different software development
teams. We then applied psycholinguistic analysis and directed content analysis
(CA) techniques to interpret the content of these practitioners' messages.
Finally, we inspected core developers' activities at various points in time
during systems' development. Results: Among our findings, we observe that core
developers' attitudes and knowledge sharing behaviors were linked to their
involvement in actual software development and the demands of their wider
project teams. However, core developers appeared to naturally possess high
levels of insightful characteristics. Conclusion: Project performance would
likely benefit from strategies aimed at surrounding core developers with other
competent communicators. Core developers should also be supported by a wider
team who are willing to ask questions and challenge their ideas. Finally, the
availability of adequate communication channels would help with maintaining
positive team climate especially in distributed developments.(Abridged)"
http://arxiv.org/abs/2102.06291,Title: Relating IS Developers' Attitudes to Engagement,"Increasing effort is being directed to understanding the personality profiles
of highly engaged information systems (IS) developers and the impact of such
profiles on development outcomes. However, there has been a lesser degree of
attention paid to studying attitudes at a fine-grained level, and relating such
attitudes to developers' in-process activities, in spite of the fact that
social motivation theory notes the importance of such a relationship in general
group work. We have therefore applied linguistic analysis, text mining and
visualization, and statistical analysis techniques to artefacts developed by
474 developers to study these issues. Our results indicate that our sample of
IS developers conveyed a range of attitudes while working to deliver systems
features, and those practitioners who communicated the most were also the most
engaged. Additionally, of eight linguistic dimensions considered, expressions
regarding work and achievement, as well as insightful attitudes, were most
closely related to developers' engagement. Accordingly, team diversity and the
provision of active support for outcome-driven developers may contribute
positively to maintaining team balance and performance."
http://arxiv.org/abs/2102.06292,Title: On Automatic Parsing of Log Records,"Software log analysis helps to maintain the health of software solutions and
ensure compliance and security. Existing software systems consist of
heterogeneous components emitting logs in various formats. A typical solution
is to unify the logs using manually built parsers, which is laborious.
Instead, we explore the possibility of automating the parsing task by
employing machine translation (MT). We create a tool that generates synthetic
Apache log records which we used to train recurrent-neural-network-based MT
models. Models' evaluation on real-world logs shows that the models can learn
Apache log format and parse individual log records. The median relative edit
distance between an actual real-world log record and the MT prediction is less
than or equal to 28%. Thus, we show that log parsing using an MT approach is
promising."
http://arxiv.org/abs/2102.06296,Title: Verifying High-Level Latency-Insensitive Designs with Formal Model  Checking,"Latency-insensitive design mitigates increasing interconnect delay and
enables productive component reuse in complex digital systems. This design
style has been adopted in high-level design flows because untimed functional
blocks connected through latency-insensitive interfaces provide a natural
communication abstraction. However, latency-insensitive design with high-level
languages also introduces a unique set of verification challenges that
jeopardize functional correctness. In particular, bugs due to invalid
consumption of inputs and deadlocks can be difficult to detect and debug with
dynamic simulation methods. To tackle these two classes of bugs, we propose
formal model checking methods to guarantee that a high-level
latency-insensitive design is unaffected by invalid input data and is free of
deadlock. We develop a well-structured verification wrapper for each property
to automatically construct the corresponding formal model for checking. Our
experiments demonstrate that the formal checks are effective in realistic bug
scenarios from high-level designs."
http://arxiv.org/abs/2102.06301,Title: ReRankMatch: Semi-Supervised Learning with Semantics-Oriented Similarity  Representation,"This paper proposes integrating semantics-oriented similarity representation
into RankingMatch, a recently proposed semi-supervised learning method. Our
method, dubbed ReRankMatch, aims to deal with the case in which labeled and
unlabeled data share non-overlapping categories. ReRankMatch encourages the
model to produce the similar image representations for the samples likely
belonging to the same class. We evaluate our method on various datasets such as
CIFAR-10, CIFAR-100, SVHN, STL-10, and Tiny ImageNet. We obtain promising
results (4.21% error rate on CIFAR-10 with 4000 labels, 22.32% error rate on
CIFAR-100 with 10000 labels, and 2.19% error rate on SVHN with 1000 labels)
when the amount of labeled data is sufficient to learn semantics-oriented
similarity representation."
http://arxiv.org/abs/2102.06304,Title: Stragglers Are Not Disaster: A Hybrid Federated Learning Algorithm with  Delayed Gradients,"Federated learning (FL) is a new machine learning framework which trains a
joint model across a large amount of decentralized computing devices. Existing
methods, e.g., Federated Averaging (FedAvg), are able to provide an
optimization guarantee by synchronously training the joint model, but usually
suffer from stragglers, i.e., IoT devices with low computing power or
communication bandwidth, especially on heterogeneous optimization problems. To
mitigate the influence of stragglers, this paper presents a novel FL algorithm,
namely Hybrid Federated Learning (HFL), to achieve a learning balance in
efficiency and effectiveness. It consists of two major components: synchronous
kernel and asynchronous updater. Unlike traditional synchronous FL methods, our
HFL introduces the asynchronous updater which actively pulls unsynchronized and
delayed local weights from stragglers. An adaptive approximation method,
Adaptive Delayed-SGD (AD-SGD), is proposed to merge the delayed local updates
into the joint model. The theoretical analysis of HFL shows that the
convergence rate of the proposed algorithm is $\mathcal{O}(\frac{1}{t+\tau})$
for both convex and non-convex optimization problems."
http://arxiv.org/abs/2102.06306,Title: Efficient Algorithms for Federated Saddle Point Optimization,"We consider strongly convex-concave minimax problems in the federated
setting, where the communication constraint is the main bottleneck. When
clients are arbitrarily heterogeneous, a simple Minibatch Mirror-prox achieves
the best performance. As the clients become more homogeneous, using multiple
local gradient updates at the clients significantly improves upon Minibatch
Mirror-prox by communicating less frequently. Our goal is to design an
algorithm that can harness the benefit of similarity in the clients while
recovering the Minibatch Mirror-prox performance under arbitrary heterogeneity
(up to log factors). We give the first federated minimax optimization algorithm
that achieves this goal. The main idea is to combine (i) SCAFFOLD (an algorithm
that performs variance reduction across clients for convex optimization) to
erase the worst-case dependency on heterogeneity and (ii) Catalyst (a framework
for acceleration based on modifying the objective) to accelerate convergence
without amplifying client drift. We prove that this algorithm achieves our
goal, and include experiments to validate the theory."
http://arxiv.org/abs/2102.06307,Title: Dancing along Battery: Enabling Transformer with Run-time  Reconfigurability on Mobile Devices,"A pruning-based AutoML framework for run-time reconfigurability, namely RT3,
is proposed in this work. This enables Transformer-based large Natural Language
Processing (NLP) models to be efficiently executed on resource-constrained
mobile devices and reconfigured (i.e., switching models for dynamic hardware
conditions) at run-time. Such reconfigurability is the key to save energy for
battery-powered mobile devices, which widely use dynamic voltage and frequency
scaling (DVFS) technique for hardware reconfiguration to prolong battery life.
In this work, we creatively explore a hybrid block-structured pruning (BP) and
pattern pruning (PP) for Transformer-based models and first attempt to combine
hardware and software reconfiguration to maximally save energy for
battery-powered mobile devices. Specifically, RT3 integrates two-level
optimizations: First, it utilizes an efficient BP as the first-step compression
for resource-constrained mobile devices; then, RT3 heuristically generates a
shrunken search space based on the first level optimization and searches
multiple pattern sets with diverse sparsity for PP via reinforcement learning
to support lightweight software reconfiguration, which corresponds to available
frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT3 can
switch the lightweight pattern sets within 45ms to guarantee the required
real-time constraint at different frequency levels. Results further show that
RT3 can prolong battery life over 4x improvement with less than 1% accuracy
loss for Transformer and 1.5% score decrease for DistilBERT."
http://arxiv.org/abs/2102.06311,Title: Personalized Visualization Recommendation,"Visualization recommendation work has focused solely on scoring
visualizations based on the underlying dataset and not the actual user and
their past visualization feedback. These systems recommend the same
visualizations for every user, despite that the underlying user interests,
intent, and visualization preferences are likely to be fundamentally different,
yet vitally important. In this work, we formally introduce the problem of
personalized visualization recommendation and present a generic learning
framework for solving it. In particular, we focus on recommending
visualizations personalized for each individual user based on their past
visualization interactions (e.g., viewed, clicked, manually created) along with
the data from those visualizations. More importantly, the framework can learn
from visualizations relevant to other users, even if the visualizations are
generated from completely different datasets. Experiments demonstrate the
effectiveness of the approach as it leads to higher quality visualization
recommendations tailored to the specific user intent and preferences. To
support research on this new problem, we release our user-centric visualization
corpus consisting of 17.4k users exploring 94k datasets with 2.3 million
attributes and 32k user-generated visualizations."
http://arxiv.org/abs/2102.06314,Title: Generating cryptographically-strong random lattice bases and recognizing  rotations of $\mathbb{Z}^n$,"Lattice-based cryptography relies on generating random bases which are
difficult to fully reduce. Given a lattice basis (such as the private basis for
a cryptosystem), all other bases are related by multiplication by matrices in
$GL(n,\mathbb{Z})$. How can one sample random elements from $GL(n,\mathbb{Z})$?
We consider various methods, finding some are stronger than others with respect
to the problem of recognizing rotations of the $\mathbb{Z}^n$ lattice. In
particular, the standard algorithm of multiplying unipotent generators together
(as implemented in Magma's RandomSLnZ command) generates instances of this last
problem which can be efficiently broken, even in dimensions nearing 1,500.
Similar weaknesses for this problem are found with the random basis generation
method in one of the NIST Post-Quantum Cryptography competition submissions
(DRS). Other algorithms are described which appear to be much stronger."
http://arxiv.org/abs/2102.06315,Title: A Visual Analysis Approach to Update Systematic Reviews,"Context: In order to preserve the value of Systematic Reviews (SRs), they
should be frequently updated considering new evidence that has been produced
since the completion of the previous version of the reviews. However, the
update of an SR is a time consuming, manual task. Thus, many SRs have not been
updated as they should be and, therefore, they are currently outdated.
Objective: The main contribution of this paper is to support the update of SRs.
Method: We propose USR-VTM, an approach based on Visual Text Mining (VTM)
techniques, to support selection of new evidence in the form of primary
studies. We then present a tool, named Revis, which supports our approach.
Finally, we evaluate our approach through a comparison of outcomes achieved
using USR-VTM versus the traditional (manual) approach. Results: Our results
show that USR-VTM increases the number of studies correctly included compared
to the traditional approach. Conclusions: USR-VTM effectively supports the
update of SRs."
http://arxiv.org/abs/2102.06317,Title: Physics-Informed Graphical Neural Network for Parameter & State  Estimations in Power Systems,"Parameter Estimation (PE) and State Estimation (SE) are the most wide-spread
tasks in the system engineering. They need to be done automatically, fast and
frequently, as measurements arrive. Deep Learning (DL) holds the promise of
tackling the challenge, however in so far, as PE and SE in power systems is
concerned, (a) DL did not win trust of the system operators because of the lack
of the physics of electricity based, interpretations and (b) DL remained
illusive in the operational regimes were data is scarce. To address this, we
present a hybrid scheme which embeds physics modeling of power systems into
Graphical Neural Networks (GNN), therefore empowering system operators with a
reliable and explainable real-time predictions which can then be used to
control the critical infrastructure. To enable progress towards trustworthy DL
for PE and SE, we build a physics-informed method, named Power-GNN, which
reconstructs physical, thus interpretable, parameters within Effective Power
Flow (EPF) models, such as admittances of effective power lines, and NN
parameters, representing implicitly unobserved elements of the system. In our
experiments, we test the Power-GNN on different realistic power networks,
including these with thousands of loads and hundreds of generators. We show
that the Power-GNN outperforms vanilla NN scheme unaware of the EPF physics."
http://arxiv.org/abs/2102.06318,Title: Projected Wasserstein gradient descent for high-dimensional Bayesian  inference,"We propose a projected Wasserstein gradient descent method (pWGD) for
high-dimensional Bayesian inference problems. The underlying density function
of a particle system of WGD is approximated by kernel density estimation (KDE),
which faces the long-standing curse of dimensionality. We overcome this
challenge by exploiting the intrinsic low-rank structure in the difference
between the posterior and prior distributions. The parameters are projected
into a low-dimensional subspace to alleviate the approximation error of KDE in
high dimensions. We formulate a projected Wasserstein gradient flow and analyze
its convergence property under mild assumptions. Several numerical experiments
illustrate the accuracy, convergence, and complexity scalability of pWGD with
respect to parameter dimension, sample size, and processor cores."
http://arxiv.org/abs/2102.06320,Title: An approximation method for electromagnetic wave models based on  fractional partial derivative,"The present article is devoting a numerical approach for solving a fractional
partial differential equation (FPDE) arising from electromagnetic waves in
dielectric media (EMWDM). The truncated Bernoulli and Hermite wavelets series
with unknown coefficients have been used to approximate the solution in both
the temporal and spatial variables. The basic idea for discretizing the FPDE is
wavelet approximation based on the Bernoulli and Hermite wavelets operational
matrices of integration and differentiation. The resulted system of a linear
algebraic equation has been solved by the collocation method. Moreover,
convergence and error analysis have been discussed. Finally, several numerical
experiments with different fractional-order derivatives have been provided and
compared with the exact analytical solutions to illustrate the accuracy and
efficiency of the method."
http://arxiv.org/abs/2102.06322,"Title: Same File, Different Changes: The Potential of Meta-Maintenance on  GitHub","Online collaboration platforms such as GitHub have provided software
developers with the ability to easily reuse and share code between
repositories. With clone-and-own and forking becoming prevalent, maintaining
these shared files is important, especially for keeping the most up-to-date
version of reused code. Different to related work, we propose the concept of
meta-maintenance -- i.e., tracking how the same files evolve in different
repositories with the aim to provide useful maintenance opportunities to those
files. We conduct an exploratory study by analyzing repositories from seven
different programming languages to explore the potential of meta-maintenance.
Our results indicate that a majority of active repositories on GitHub contains
at least one file which is also present in another repository, and that a
significant minority of these files are maintained differently in the different
repositories which contain them. We manually analyzed a representative sample
of shared files and their variants to understand which changes might be useful
for meta-maintenance. Our findings support the potential of meta-maintenance
and open up avenues for future work to capitalize on this potential."
http://arxiv.org/abs/2102.06326,"Title: A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers  Suffice Across Batch Sizes","Recently the LARS and LAMB optimizers have been proposed for training neural
networks faster using large batch sizes. LARS and LAMB add layer-wise
normalization to the update rules of Heavy-ball momentum and Adam,
respectively, and have become popular in prominent benchmarks and deep learning
libraries. However, without fair comparisons to standard optimizers, it remains
an open question whether LARS and LAMB have any benefit over traditional,
generic algorithms. In this work we demonstrate that standard optimization
algorithms such as Nesterov momentum and Adam can match or exceed the results
of LARS and LAMB at large batch sizes. Our results establish new, stronger
baselines for future comparisons at these batch sizes and shed light on the
difficulties of comparing optimizers for neural network training more
generally."
http://arxiv.org/abs/2102.06328,Title: Contrastive Unsupervised Learning for Speech Emotion Recognition,"Speech emotion recognition (SER) is a key technology to enable more natural
human-machine communication. However, SER has long suffered from a lack of
public large-scale labeled datasets. To circumvent this problem, we investigate
how unsupervised representation learning on unlabeled datasets can benefit SER.
We show that the contrastive predictive coding (CPC) method can learn salient
representations from unlabeled datasets, which improves emotion recognition
performance. In our experiments, this method achieved state-of-the-art
concordance correlation coefficient (CCC) performance for all emotion
primitives (activation, valence, and dominance) on IEMOCAP. Additionally, on
the MSP- Podcast dataset, our method obtained considerable performance
improvements compared to baselines."
http://arxiv.org/abs/2102.06329,Title: Min-Max-Plus Neural Networks,"We present a new model of neural networks called Min-Max-Plus Neural Networks
(MMP-NNs) based on operations in tropical arithmetic. In general, an MMP-NN is
composed of three types of alternately stacked layers, namely linear layers,
min-plus layers and max-plus layers. Specifically, the latter two types of
layers constitute the nonlinear part of the network which is trainable and more
sophisticated compared to the nonlinear part of conventional neural networks.
In addition, we show that with higher capability of nonlinearity expression,
MMP-NNs are universal approximators of continuous functions, even when the
number of multiplication operations is tremendously reduced (possibly to none
in certain extreme cases). Furthermore, we formulate the backpropagation
algorithm in the training process of MMP-NNs and introduce an algorithm of
normalization to improve the rate of convergence in training."
http://arxiv.org/abs/2102.06333,Title: DeepPseudo: Deep Pseudo-code Generation via Transformer and Code Feature  Extraction,"Pseudo-code written by natural language is helpful for novice developers'
program comprehension. However, writing such pseudo-code is time-consuming and
laborious. Motivated by the research advancements of sequence-to-sequence
learning and code semantic learning, we propose a novel deep pseudo-code
generation method DeepPseudo via Transformer and code feature extraction. In
particular, DeepPseudo utilizes both Transformer encoder and code feature
extractor to perform encoding for source code. Then it uses a pseudo-code
generator to perform decoding, which can generate the corresponding
pseudo-code. We choose corpus gathered from a web application framework Django,
which contains 18,805 pairs of Python statements and corresponding pseudo-code.
We first compare DeepPseudo with seven baselines from pseudo-code generation
and neural machine translation domains in terms of four performance measures.
Results show the competitiveness of DeepPseudo. Later, we analyze the
rationality of the component settings (i.e., the usage of code feature
extractor, the attention mechanism, and the positional encoding method) in
DeepPseudo. Finally, we perform a human study to verify the effectiveness of
DeepPseudo."
http://arxiv.org/abs/2102.06336,Title: SCOUT: Socially-COnsistent and UndersTandable Graph Attention Network  for Trajectory Prediction of Vehicles and VRUs,"Autonomous vehicles navigate in dynamically changing environments under a
wide variety of conditions, being continuously influenced by surrounding
objects. Modelling interactions among agents is essential for accurately
forecasting other agents' behaviour and achieving safe and comfortable motion
planning. In this work, we propose SCOUT, a novel Attention-based Graph Neural
Network that uses a flexible and generic representation of the scene as a graph
for modelling interactions, and predicts socially-consistent trajectories of
vehicles and Vulnerable Road Users (VRUs) under mixed traffic conditions. We
explore three different attention mechanisms and test our scheme with both
bird-eye-view and on-vehicle urban data, achieving superior performance than
existing state-of-the-art approaches on InD and ApolloScape Trajectory
benchmarks. Additionally, we evaluate our model's flexibility and
transferability by testing it under completely new scenarios on RounD dataset.
The importance and influence of each interaction in the final prediction is
explored by means of Integrated Gradients technique and the visualization of
the attention learned."
http://arxiv.org/abs/2102.06343,Title: A Decentralized Approach Towards Responsible AI in Social Ecosystems,"For AI technology to fulfill its full promises, we must design effective
mechanisms into the AI systems to support responsible AI behavior and curtail
potential irresponsible use, e.g. in areas of privacy protection, human
autonomy, robustness, and prevention of biases and discrimination in automated
decision making. In this paper, we present a framework that provides
computational facilities for parties in a social ecosystem to produce the
desired responsible AI behaviors. To achieve this goal, we analyze AI systems
at the architecture level and propose two decentralized cryptographic
mechanisms for an AI system architecture: (1) using Autonomous Identity to
empower human users, and (2) automating rules and adopting conventions within
social institutions. We then propose a decentralized approach and outline the
key concepts and mechanisms based on Decentralized Identifier (DID) and
Verifiable Credentials (VC) for a general-purpose computational infrastructure
to realize these mechanisms. We argue the case that a decentralized approach is
the most promising path towards Responsible AI from both the computer science
and social science perspectives."
http://arxiv.org/abs/2102.06344,Title: Distributed Source Coding with Encryption Using Correlated Keys,"We pose and investigate the distributed secure source coding based on the
common key cryptosystem. This cryptosystem includes the secrecy amplification
problem for distributed encrypted sources with correlated keys using
post-encryption-compression, which was posed investigated by Santoso and
Oohama. In this paper we propose another new security criterion which is
generally more strict compared to the commonly used security criterion which is
based on the upper-bound of mutual information between the plaintext and the
ciphertext. Under this criterion, we establish the necessary and sufficient
condition for the secure transmission of correlated sources."
http://arxiv.org/abs/2102.06345,Title: Software Estimations Risk in Pakistan Software Industry,"Software and IT industry in Pakistan have seen a dramatic growth and success
in past few years and is expected to get doubled by 2020, according to a
research. Software development life cycle comprises of multiple phases,
activities and techniques that can lead to successful projects, and software
evaluation is one of the vital and important parts of that. Software estimation
can alone be the reason of product success factor or the products failure
factor. To estimate the right cost, effort and resources is an art. But it is
also very important to include the risks that may arise in the in a software
project which can affect your estimates. In this paper, we highlight how the
risks in Pakistan Software Industry can affect the estimates and how to
mitigate them."
http://arxiv.org/abs/2102.06349,Title: Dynamic Precision Analog Computing for Neural Networks,"Analog electronic and optical computing exhibit tremendous advantages over
digital computing for accelerating deep learning when operations are executed
at low precision. In this work, we derive a relationship between analog
precision, which is limited by noise, and digital bit precision. We propose
extending analog computing architectures to support varying levels of precision
by repeating operations and averaging the result, decreasing the impact of
noise. Such architectures enable programmable tradeoffs between precision and
other desirable performance metrics such as energy efficiency or throughput. To
utilize dynamic precision, we propose a method for learning the precision of
each layer of a pre-trained model without retraining network weights. We
evaluate this method on analog architectures subject to a variety of noise
sources such as shot noise, thermal noise, and weight noise and find that
employing dynamic precision reduces energy consumption by up to 89% for
computer vision models such as Resnet50 and by 24% for natural language
processing models such as BERT. In one example, we apply dynamic precision to a
shot-noise limited homodyne optical neural network and simulate inference at an
optical energy consumption of 2.7 aJ/MAC for Resnet50 and 1.6 aJ/MAC for BERT
with <2% accuracy degradation."
http://arxiv.org/abs/2102.06350,Title: Confounding Tradeoffs for Neural Network Quantization,"Many neural network quantization techniques have been developed to decrease
the computational and memory footprint of deep learning. However, these methods
are evaluated subject to confounding tradeoffs that may affect inference
acceleration or resource complexity in exchange for higher accuracy. In this
work, we articulate a variety of tradeoffs whose impact is often overlooked and
empirically analyze their impact on uniform and mixed-precision post-training
quantization, finding that these confounding tradeoffs may have a larger impact
on quantized network accuracy than the actual quantization methods themselves.
Because these tradeoffs constrain the attainable hardware acceleration for
different use-cases, we encourage researchers to explicitly report these design
choices through the structure of ""quantization cards."" We expect quantization
cards to help researchers compare methods more effectively and engineers
determine the applicability of quantization techniques for their hardware."
http://arxiv.org/abs/2102.06352,Title: Multiplex Bipartite Network Embedding using Dual Hypergraph  Convolutional Networks,"A bipartite network is a graph structure where nodes are from two distinct
domains and only inter-domain interactions exist as edges. A large number of
network embedding methods exist to learn vectorial node representations from
general graphs with both homogeneous and heterogeneous node and edge types,
including some that can specifically model the distinct properties of bipartite
networks. However, these methods are inadequate to model multiplex bipartite
networks (e.g., in e-commerce), that have multiple types of interactions (e.g.,
click, inquiry, and buy) and node attributes. Most real-world multiplex
bipartite networks are also sparse and have imbalanced node distributions that
are challenging to model. In this paper, we develop an unsupervised Dual
HyperGraph Convolutional Network (DualHGCN) model that scalably transforms the
multiplex bipartite network into two sets of homogeneous hypergraphs and uses
spectral hypergraph convolutional operators, along with intra- and
inter-message passing strategies to promote information exchange within and
across domains, to learn effective node embedding. We benchmark DualHGCN using
four real-world datasets on link prediction and node classification tasks. Our
extensive experiments demonstrate that DualHGCN significantly outperforms
state-of-the-art methods, and is robust to varying sparsity levels and
imbalanced node distributions."
http://arxiv.org/abs/2102.06355,Title: Speculating Ineffective UI Exploration via Trace Analysis,"With the prosperity of mobile apps, quality assurance of mobile apps becomes
crucially important. Automated mobile User Interface (UI) testing had arisen as
a key technique for app quality assurance. However, despite years of efforts,
existing mobile UI testing techniques still cannot achieve high code coverage,
especially for industrial-quality apps. To substantially improve the efficacy
of mobile UI testing, we investigate state-of-the-art techniques and find a
fundamental limitation--each testing technique attempts to apply one predefined
strategy to explore the UI space of all mobile apps. However, we observe that
different UI design characteristics require customized UI exploration
strategies in practice. With this finding in mind, in this paper, we propose a
new direction for mobile UI testing--automatic customization of UI exploration
strategies for each app under test. As a first step in this direction, we
target ineffective exploration behavior, which refers to cases where UI testing
tools fail to make progress effectively. We present Vet as a general framework
for applying the idea of trace analysis on UI testing history to identify
ineffective exploration behavior for a given UI testing tool on a given app.
Vet embraces specialized algorithms for speculating subsequences in the trace
that manifest ineffective exploration behavior of UI space exploration. Vet
then enables enhancing the testing tool by guiding the exploration to avoid
ineffective exploration. We evaluate Vet by applying it to three
state-of-the-art Android UI testing tools. Vet locates ineffective exploration
behaviors that reveal various tool-app applicability issues hindering testing
efficacy. Vet automatically fixes the applicability issues and achieves up to
46.8% code coverage relative improvements on 11 of 15 industrial-quality apps
under evaluation."
http://arxiv.org/abs/2102.06356,Title: Neural Inverse Text Normalization,"While there have been several contributions exploring state of the art
techniques for text normalization, the problem of inverse text normalization
(ITN) remains relatively unexplored. The best known approaches leverage finite
state transducer (FST) based models which rely on manually curated rules and
are hence not scalable. We propose an efficient and robust neural solution for
ITN leveraging transformer based seq2seq models and FST-based text
normalization techniques for data preparation. We show that this can be easily
extended to other languages without the need for a linguistic expert to
manually curate them. We then present a hybrid framework for integrating Neural
ITN with an FST to overcome common recoverable errors in production
environments. Our empirical evaluations show that the proposed solution
minimizes incorrect perturbations (insertions, deletions and substitutions) to
ASR output and maintains high quality even on out of domain data. A transformer
based model infused with pretraining consistently achieves a lower WER across
several datasets and is able to outperform baselines on English, Spanish,
German and Italian datasets."
http://arxiv.org/abs/2102.06357,Title: Uncertainty-of-Information Scheduling: A Restless Multi-armed Bandit  Framework,"This paper proposes using the uncertainty of information (UoI), measured by
Shannon's entropy, as a metric for information freshness. We consider a system
in which a central monitor observes multiple binary Markov processes through a
communication channel. The UoI of a Markov process corresponds to the monitor's
uncertainty about its state. At each time step, only one Markov process can be
selected to update its state to the monitor; hence there is a tradeoff among
the UoIs of the processes that depend on the scheduling policy used to select
the process to be updated. The age of information (AoI) of a process
corresponds to the time since its last update. In general, the associated UoI
can be a non-increasing function, or even an oscillating function, of its AoI,
making the scheduling problem particularly challenging. This paper investigates
scheduling policies that aim to minimize the average sum-UoI of the processes
over the infinite time horizon. We formulate the problem as a restless
multi-armed bandit (RMAB) problem, and develop a Whittle index policy that is
near-optimal for the RMAB after proving its indexability. We further provide an
iterative algorithm to compute the Whittle index for the practical deployment
of the policy. Although this paper focuses on UoI scheduling, our results apply
to a general class of RMABs for which the UoI scheduling problem is a special
case. Specifically, this paper's Whittle index policy is valid for any RMAB in
which the bandits are binary Markov processes and the penalty is a concave
function of the belief state of the Markov process. Numerical results
demonstrate the excellent performance of the Whittle index policy for this
class of RMABs."
http://arxiv.org/abs/2102.06358,Title: The Symmetry between Bandits and Knapsacks: A Primal-Dual LP-based  Approach,"In this paper, we study the bandits with knapsacks (BwK) problem and develop
a primal-dual based algorithm that achieves a problem-dependent logarithmic
regret bound. The BwK problem extends the multi-arm bandit (MAB) problem to
model the resource consumption associated with playing each arm, and the
existing BwK literature has been mainly focused on deriving asymptotically
optimal distribution-free regret bounds. We first study the primal and dual
linear programs underlying the BwK problem. From this primal-dual perspective,
we discover symmetry between arms and knapsacks, and then propose a new notion
of sub-optimality measure for the BwK problem. The sub-optimality measure
highlights the important role of knapsacks in determining algorithm regret and
inspires the design of our two-phase algorithm. In the first phase, the
algorithm identifies the optimal arms and the binding knapsacks, and in the
second phase, it exhausts the binding knapsacks via playing the optimal arms
through an adaptive procedure. Our regret upper bound involves the proposed
sub-optimality measure and it has a logarithmic dependence on length of horizon
$T$ and a polynomial dependence on $m$ (the numbers of arms) and $d$ (the
number of knapsacks). To the best of our knowledge, this is the first
problem-dependent logarithmic regret bound for solving the general BwK problem."
http://arxiv.org/abs/2102.06360,Title: Multi-source Pseudo-label Learning of Semantic Segmentation for the  Scene Recognition of Agricultural Mobile Robots,"This paper describes a novel method of training a semantic segmentation model
for environment recognition of agricultural mobile robots by unsupervised
domain adaptation exploiting publicly available datasets of outdoor scenes that
are different from our target environments i.e., greenhouses. In conventional
semantic segmentation methods, the labels are given by manual annotation, which
is a tedious and time-consuming task. A method to work around the necessity of
the manual annotation is unsupervised domain adaptation (UDA) that transfer
knowledge from labeled source datasets to unlabeled target datasets. Most of
the UDA methods of semantic segmentation are validated by tasks of adaptation
from non-photorealistic synthetic images of urban scenes to real ones. However,
the effectiveness of the methods is not well studied in the case of adaptation
to other types of environments, such as greenhouses. In addition, it is not
always possible to prepare appropriate source datasets for such environments.
In this paper, we adopt an existing training method of UDA to a task of
training a model for greenhouse images. We propose to use multiple publicly
available datasets of outdoor images as source datasets, and also propose a
simple yet effective method of generating pseudo-labels by transferring
knowledge from the source datasets that have different appearance and a label
set from the target datasets. We demonstrate in experiments that by combining
our proposed method of pseudo-label generation with the existing training
method, the performance was improved by up to 14.3% of mIoU compared to the
best score of the single-source training."
http://arxiv.org/abs/2102.06361,Title: The Distributed Discrete Gaussian Mechanism for Federated Learning with  Secure Aggregation,"We consider training models on private data that is distributed across user
devices. To ensure privacy, we add on-device noise and use secure aggregation
so that only the noisy sum is revealed to the server. We present a
comprehensive end-to-end system, which appropriately discretizes the data and
adds discrete Gaussian noise before performing secure aggregation. We provide a
novel privacy analysis for sums of discrete Gaussians. We also analyze the
effect of rounding the input data and the modular summation arithmetic. Our
theoretical guarantees highlight the complex tension between communication,
privacy, and accuracy. Our extensive experimental results demonstrate that our
solution is essentially able to achieve a comparable accuracy to central
differential privacy with 16 bits of precision per value."
http://arxiv.org/abs/2102.06362,Title: The Software Heritage Filesystem (SwhFS): Integrating Source Code  Archival with Development,"We introduce the Software Heritage filesystem (SwhFS), a user-space
filesystem that integrates large-scale open source software archival with
development workflows. SwhFS provides a POSIX filesystem view of Software
Heritage, the largest public archive of software source code and version
control system (VCS) development history.Using SwhFS, developers can quickly
""checkout"" any of the 2 billion commits archived by Software Heritage, even
after they disappear from their previous known location and without incurring
the performance cost of repository cloning. SwhFS works across unrelated
repositories and different VCS technologies. Other source code artifacts
archived by Software Heritage-individual source code files and trees, releases,
and branches-can also be accessed using common programming tools and custom
scripts, as if they were locally available.A screencast of SwhFS is available
online at dx.doi.org/10.5281/zenodo.4531411."
http://arxiv.org/abs/2102.06363,Title: Multiversal views on language models,"The virtuosity of language models like GPT-3 opens a new world of possibility
for human-AI collaboration in writing. In this paper, we present a framework in
which generative language models are conceptualized as multiverse generators.
This framework also applies to human imagination and is core to how we read and
write fiction. We call for exploration into this commonality through new forms
of interfaces which allow humans to couple their imagination to AI to write,
explore, and understand non-linear fiction. We discuss the early insights we
have gained from actively pursuing this approach by developing and testing a
novel multiversal GPT-3-assisted writing interface."
http://arxiv.org/abs/2102.06364,Title: Complete Power Reallocation for MU-MIMO under Per-Antenna Power  Constraint,"This paper proposes a beamforming method under a per-antenna power constraint
(PAPC). Although many beamformer designs with the PAPC need to solve complex
optimization problems, the proposed complete power reallocation (CPR) method
can generate beamformers with excellent performance only with linear
operations. CPR is designed to have a simple structure, making it highly
flexible and practical. In this paper, three CPR variations considering
algorithm convergence speed, sum-rate maximization, and robustness to channel
uncertainty are developed. Simulation results verify that CPR and its
variations satisfy their design criteria, and, hence, CPR can be readily
utilized for various purposes."
http://arxiv.org/abs/2102.06365,Title: White-Box Performance-Influence Models: A Profiling and Learning  Approach,"Many modern software systems are highly configurable, allowing the user to
tune them for performance and more. Current performance modeling approaches aim
at finding performance-optimal configurations by building performance models in
a black-box manner. While these models provide accurate estimates, they cannot
pinpoint causes of observed performance behavior to specific code regions. This
does not only hinder system understanding, but it also complicates tracing the
influence of configuration options to individual methods.
We propose a white-box approach that models configuration-dependent
performance behavior at the method level. This allows us to predict the
influence of configuration decisions on individual methods, supporting system
understanding and performance debugging. The approach consists of two steps:
First, we use a coarse-grained profiler and learn performance-influence models
for all methods, potentially identifying some methods that are highly
configuration- and performance-sensitive, causing inaccurate predictions.
Second, we re-measure these methods with a fine-grained profiler and learn more
accurate models, at higher cost, though. By means of 9 real-world Java software
systems, we demonstrate that our approach can efficiently identify
configuration-relevant methods and learn accurate performance-influence models."
http://arxiv.org/abs/2102.06366,Title: Well-posedness theory for nonlinear scalar conservation laws on networks,"We consider nonlinear scalar conservation laws posed on a network. We
establish $L^1$ stability, and thus uniqueness, for weak solutions satisfying
the entropy condition. We apply standard finite volume methods and show
stability and convergence to the unique entropy solution, thus establishing
existence of a solution in the process. Both our existence and
stability/uniqueness theory is centred around families of stationary states for
the equation. In one important case -- for monotone fluxes with an upwind
difference scheme -- we show that the set of (discrete) stationary solutions is
indeed sufficiently large to suit our general theory. We demonstrate the
method's properties through several numerical experiments."
http://arxiv.org/abs/2102.06371,Title: SceneRec: Scene-Based Graph Neural Networks for Recommender Systems,"Collaborative filtering has been largely used to advance modern recommender
systems to predict user preference. A key component in collaborative filtering
is representation learning, which aims to project users and items into a low
dimensional space to capture collaborative signals. However, the scene
information, which has effectively guided many recommendation tasks, is rarely
considered in existing collaborative filtering methods. To bridge this gap, we
focus on scene-based collaborative recommendation and propose a novel
representation model SceneRec. SceneRec formally defines a scene as a set of
pre-defined item categories that occur simultaneously in real-life situations
and creatively designs an item-category-scene hierarchical structure to build a
scene-based graph. In the scene-based graph, we adopt graph neural networks to
learn scene-specific representation on each item node, which is further
aggregated with latent representation learned from collaborative interactions
to make recommendations. We perform extensive experiments on real-world
E-commerce datasets and the results demonstrate the effectiveness of the
proposed method."
http://arxiv.org/abs/2102.06377,Title: Rate-Splitting Multiple Access to Mitigate the Curse of Mobility in  (Massive) MIMO Networks,"Rate-Splitting Multiple Access (RSMA) is a flexible and robust multiple
access scheme for downlink multi-antenna wireless networks. RSMA relies on
multi-antenna Rate-Splitting (RS) at the transmitter and Successive
Interference Cancellation (SIC) at the receivers. In this work, we study the
performance of RSMA under the practical important setup of imperfect Channel
State Information at Transmitter (CSIT) originating from user mobility and
latency in the network. First, we derive a lower bound on the ergodic sum-rate
of RSMA for an arbitrary number of transmit antennas, number of users, user
speeds and transmit power. Then, we study the power allocation between common
and private streams and obtain a closed-form solution for the optimal power
allocation that maximizes the obtained lower bound. The proposed power
allocation greatly reduces precoder design complexity for RSMA. By Link-Level
Simulations (LLS), we demonstrate that RSMA with the proposed power allocation
is robust to degrading effects of user mobility and has significantly higher
performance compared to conventional multi-user (massive) Multiple-Input
Multiple-Output (MIMO) strategies. The work has important practical
significance as results demonstrate that, in contrast to conventional
multi-user (massive) MIMO whose performance collapse under mobility, RSMA can
maintain reliable multi-user connectivity in mobile deployments."
http://arxiv.org/abs/2102.06380,Title: A Too-Good-to-be-True Prior to Reduce Shortcut Reliance,"Despite their impressive performance in object recognition and other tasks
under standard testing conditions, deep convolutional neural networks (DCNNs)
often fail to generalize to out-of-distribution (o.o.d.) samples. One cause for
this shortcoming is that modern architectures tend to rely on ""shortcuts"" -
superficial features that correlate with categories without capturing deeper
invariants that hold across contexts. Real-world concepts often possess a
complex structure that can vary superficially across contexts, which can make
the most intuitive and promising solutions in one context not generalize to
others. One potential way to improve o.o.d. generalization is to assume simple
solutions are unlikely to be valid across contexts and downweight them, which
we refer to as the too-good-to-be-true prior. We implement this inductive bias
in a two-stage approach that uses predictions from a low-capacity network (LCN)
to inform the training of a high-capacity network (HCN). Since the shallow
architecture of the LCN can only learn surface relationships, which includes
shortcuts, we downweight training items for the HCN that the LCN can master,
thereby encouraging the HCN to rely on deeper invariant features that should
generalize broadly. Using a modified version of the CIFAR-10 dataset in which
we introduced shortcuts, we found that the two-stage LCN-HCN approach reduced
reliance on shortcuts and facilitated o.o.d. generalization."
http://arxiv.org/abs/2102.06384,Title: Densely Deformable Efficient Salient Object Detection Network,"Salient Object Detection (SOD) domain using RGB-D data has lately emerged
with some current models' adequately precise results. However, they have
restrained generalization abilities and intensive computational complexity. In
this paper, inspired by the best background/foreground separation abilities of
deformable convolutions, we employ them in our Densely Deformable Network
(DDNet) to achieve efficient SOD. The salient regions from densely deformable
convolutions are further refined using transposed convolutions to optimally
generate the saliency maps. Quantitative and qualitative evaluations using the
recent SOD dataset against 22 competing techniques show our method's efficiency
and effectiveness. We also offer evaluation using our own created
cross-dataset, surveillance-SOD (S-SOD), to check the trained models' validity
in terms of their applicability in diverse scenarios. The results indicate that
the current models have limited generalization potentials, demanding further
research in this direction. Our code and new dataset will be publicly available
at https://github.com/tanveer-hussain/EfficientSOD"
http://arxiv.org/abs/2102.06385,Title: Supervised training of spiking neural networks for robust deployment on  mixed-signal neuromorphic processors,"Mixed-signal analog/digital electronic circuits can emulate spiking neurons
and synapses with extremely high energy efficiency, following an approach known
as ""neuromorphic engineering"". However, analog circuits are sensitive to
variation in fabrication among transistors in a chip (""device mismatch""). In
the case of neuromorphic implementation of Spiking Neural Networks (SNNs),
mismatch is expressed as differences in effective parameters between
identically-configured neurons and synapses. Each fabricated chip therefore
provides a different distribution of parameters such as time constants or
synaptic weights. Without the expensive overhead in terms of area and power of
extra on-chip learning or calibration circuits, device mismatch and other noise
sources represent a critical challenge for the deployment of pre-trained neural
network chips. Here we present a supervised learning approach that addresses
this challenge by maximizing robustness to mismatch and other common sources of
noise.
The proposed method trains (SNNs) to perform temporal classification tasks by
mimicking a pre-trained dynamical system, using a local learning rule adapted
from non-linear control theory. We demonstrate the functionality of our model
on two tasks that require memory to perform successfully, and measure the
robustness of our approach to several forms of noise and variability present in
the network. We show that our approach is more robust than several common
alternative approaches for training SNNs.
Our method provides a viable way to robustly deploy pre-trained networks on
mixed-signal neuromorphic hardware, without requiring per-device training or
calibration."
http://arxiv.org/abs/2102.06386,Title: Robust Hybrid High-Order method on polytopal meshes with small faces,"We design a Hybrid High-Order (HHO) scheme for the Poisson problem that is
fully robust on polytopal meshes in the presence of small edges/faces. We state
general assumptions on the stabilisation terms involved in the scheme, under
which optimal error estimates (in discrete and continuous energy norms, as well
as $L^2$-norm) are established with multiplicative constants that do not depend
on the maximum number of faces in each element, or the relative size between an
element and its faces. We illustrate the error estimates through numerical
simulations in 2D and 3D on meshes designed by agglomeration techniques (such
meshes naturally have elements with a very large numbers of faces, and very
small faces)."
http://arxiv.org/abs/2102.06387,Title: Banana for scale: Gauging trends in academic interest by normalising  publication rates to common and innocuous keywords,"Many academics use yearly publication numbers to quantify academic interest
for their research topic. While such visualisations are ubiquitous in grant
applications, manuscript introductions, and review articles, they fail to
account for the rapid growth in scientific publications. As a result, any
search term will likely show an increase in supposed ""academic interest"". One
proposed solution is to normalise yearly publication rates by field size, but
this is arduous and difficult. Here, we propose an simpler index that
normalises keywords of interest by a ubiquitous and innocuous keyword, such as
""banana"". Alternatively, one could opt for field-specific keywords or
hierarchical structures (e.g. PubMed's Medical Subject Headings, MeSH) to
compute ""interest market share"". Using this approach, we uncovered plausible
trends in academic interest in examples from the medical literature. In
neuroimaging, we found that not the supplementary motor area (as was previously
claimed), but the prefrontal cortex is the most interesting part of the brain.
In cancer research, we found a contemporary preference for cancers with high
prevalence and clinical severity, and notable declines in interest for more
treatable or likely benign neoplasms. Finally, we found that interest in
respiratory viral infections spiked when strains showed potential for pandemic
involvement, with SARS-CoV-2 and the COVID-19 pandemic being the most extreme
example. In sum, the time is ripe for a quick and easy method to quantify
trends in academic interest for anecdotal purposes. We provide such a method,
along with software for researchers looking to implement it in their own
writing."
http://arxiv.org/abs/2102.06388,Title: Reaction or Speculation: Building Computational Support for Users in  Catching-Up Series Based on an Emerging Media Consumption Phenomenon,"A growing number of people are using catch-up TV services rather than
watching simultaneously with other audience members at the time of broadcast.
However, computational support for such catching-up users has not been well
explored. In particular, we are observing an emerging phenomenon in online
media consumption experiences in which speculation plays a vital role. As the
phenomenon of speculation implicitly assumes simultaneity in media consumption,
there is a gap for catching-up users, who cannot directly appreciate the
consumption experiences. This conversely suggests that there is potential for
computational support to enhance the consumption experiences of catching-up
users. Accordingly, we conducted a series of studies to pave the way for
developing computational support for catching-up users. First, we conducted
semi-structured interviews to understand how people are engaging with
speculation during media consumption. As a result, we discovered the
distinctive aspects of speculation-based consumption experiences in contrast to
social viewing experiences sharing immediate reactions that have been discussed
in previous studies. We then designed two prototypes for supporting catching-up
users based on our quantitative analysis of Twitter data in regard to reaction-
and speculation-based media consumption. Lastly, we evaluated the prototypes in
a user experiment and, based on its results, discussed ways to empower
catching-up users with computational supports in response to recent
transformations in media consumption."
http://arxiv.org/abs/2102.06390,Title: Emoji-Based Transfer Learning for Sentiment Tasks,"Sentiment tasks such as hate speech detection and sentiment analysis,
especially when performed on languages other than English, are often
low-resource. In this study, we exploit the emotional information encoded in
emojis to enhance the performance on a variety of sentiment tasks. This is done
using a transfer learning approach, where the parameters learned by an
emoji-based source task are transferred to a sentiment target task. We analyse
the efficacy of the transfer under three conditions, i.e. i) the emoji content
and ii) label distribution of the target task as well as iii) the difference
between monolingually and multilingually learned source tasks. We find i.a.
that the transfer is most beneficial if the target task is balanced with high
emoji content. Monolingually learned source tasks have the benefit of taking
into account the culturally specific use of emojis and gain up to F1 +0.280
over the baseline."
http://arxiv.org/abs/2102.06391,Title: A Subexponential Algorithm for ARRIVAL,"The ARRIVAL problem is to decide the fate of a train moving along the edges
of a directed graph, according to a simple (deterministic) pseudorandom walk.
The problem is in $NP \cap coNP$ but not known to be in $P$. The currently best
algorithms have runtime $2^{\Theta(n)}$ where $n$ is the number of vertices.
This is not much better than just performing the pseudorandom walk. We develop
a subexponential algorithm with runtime $2^{O(\sqrt{n}\log n)}$. We also give a
polynomial-time algorithm if the graph is almost acyclic. Both results are
derived from a new general approach to solve ARRIVAL instances."
http://arxiv.org/abs/2102.06392,Title: Bootstrapping Large-Scale Fine-Grained Contextual Advertising Classifier  from Wikipedia,"Contextual advertising provides advertisers with the opportunity to target
the context which is most relevant to their ads. However, its power cannot be
fully utilized unless we can target the page content using fine-grained
categories, e.g., ""coupe"" vs. ""hatchback"" instead of ""automotive"" vs. ""sport"".
The widely used advertising content taxonomy (IAB taxonomy) consists of 23
coarse-grained categories and 355 fine-grained categories. With the large
number of categories, it becomes very challenging either to collect training
documents to build a supervised classification model, or to compose
expert-written rules in a rule-based classification system. Besides, in
fine-grained classification, different categories often overlap or co-occur,
making it harder to classify accurately. In this work, we propose wiki2cat, a
method to tackle the problem of large-scaled fine-grained text classification
by tapping on Wikipedia category graph. The categories in IAB taxonomy are
first mapped to category nodes in the graph. Then the label is propagated
across the graph to obtain a list of labeled Wikipedia documents to induce text
classifiers. The method is ideal for large-scale classification problems since
it does not require any manually-labeled document or hand-curated rules or
keywords. The proposed method is benchmarked with various learning-based and
keyword-based baselines and yields competitive performance on both publicly
available datasets and a new dataset containing more than 300 fine-grained
categories."
http://arxiv.org/abs/2102.06393,Title: VARA-TTS: Non-Autoregressive Text-to-Speech Synthesis based on Very Deep  VAE with Residual Attention,"This paper proposes VARA-TTS, a non-autoregressive (non-AR) text-to-speech
(TTS) model using a very deep Variational Autoencoder (VDVAE) with Residual
Attention mechanism, which refines the textual-to-acoustic alignment
layer-wisely. Hierarchical latent variables with different temporal resolutions
from the VDVAE are used as queries for residual attention module. By leveraging
the coarse global alignment from previous attention layer as an extra input,
the following attention layer can produce a refined version of alignment. This
amortizes the burden of learning the textual-to-acoustic alignment among
multiple attention layers and outperforms the use of only a single attention
layer in robustness. An utterance-level speaking speed factor is computed by a
jointly-trained speaking speed predictor, which takes the mean-pooled latent
variables of the coarsest layer as input, to determine number of acoustic
frames at inference. Experimental results show that VARA-TTS achieves slightly
inferior speech quality to an AR counterpart Tacotron 2 but an
order-of-magnitude speed-up at inference; and outperforms an analogous non-AR
model, BVAE-TTS, in terms of speech quality."
http://arxiv.org/abs/2102.06395,Title: Towards Large Scale Automated Algorithm Design by Integrating Modular  Benchmarking Frameworks,"We present a first proof-of-concept use-case that demonstrates the efficiency
of interfacing the algorithm framework ParadisEO with the automated algorithm
configuration tool irace and the experimental platform IOHprofiler. By combing
these three tools, we obtain a powerful benchmarking environment that allows us
to systematically analyze large classes of algorithm spaces on complex
benchmark problems. Key advantages of our pipeline are fast evaluation times,
the possibility to generate rich data sets to support the analysis of the
algorithms, and a standardized interface that can be used to benchmark very
broad classes of sampling-based optimization heuristics.
In addition to enabling systematic algorithm configuration studies, our
approach paves a way for assessing the contribution of new ideas in interplay
with already existing operators -- a promising avenue for our research domain,
which at present may have a too strong focus on comparing entire algorithm
instances."
http://arxiv.org/abs/2102.06400,Title: Fast Fault Detection on a Quadrotor using Onboard Sensors and a Kalman  Filter Approach,"This paper presents a novel method for fast and robust detection of actuator
failures on quadrotors. The proposed algorithm has very little model
dependency. A Kalman filter estimator estimates a stochastic effectiveness
factor for every actuator, using only onboard RPM, gyro and accelerometer
measurements. Then, a hypothesis test identifies the failed actuator. This
algorithm is validated online in real-time, also as part of an active fault
tolerant control system. Loss of actuator effectiveness is induced by ejecting
the propellers from the motors. The robustness of this algorithm is further
investigated offline over a range of parameter settings by replaying real
flight data containing 26 propeller ejections. The detection delays are found
to be in the 30 to 130 ms range, without missed detections or false alarms
occurring."
http://arxiv.org/abs/2102.06401,Title: Broad-UNet: Multi-scale feature learning for nowcasting tasks,"Weather nowcasting consists of predicting meteorological components in the
short term at high spatial resolutions. Due to its influence in many human
activities, accurate nowcasting has recently gained plenty of attention. In
this paper, we treat the nowcasting problem as an image-to-image translation
problem using satellite imagery. We introduce Broad-UNet, a novel architecture
based on the core UNet model, to efficiently address this problem. In
particular, the proposed Broad-UNet is equipped with asymmetric parallel
convolutions as well as Atrous Spatial Pyramid Pooling (ASPP) module. In this
way, The the Broad-UNet model learns more complex patterns by combining
multi-scale features while using fewer parameters than the core UNet model. The
proposed model is applied on two different nowcasting tasks, i.e. precipitation
maps and cloud cover nowcasting. The obtained numerical results show that the
introduced Broad-UNet model performs more accurate predictions compared to the
other examined architectures."
http://arxiv.org/abs/2102.06405,"Title: Data Analytics and Machine Learning Methods, Techniques and Tool for  Model-Driven Engineering of Smart IoT Services","This doctoral dissertation proposes a novel approach to enhance the
development of smart services for the Internet of Things (IoT) and smart
Cyber-Physical Systems (CPS). The proposed approach offers abstraction and
automation to the software engineering processes, as well as the Data Analytics
(DA) and Machine Learning (ML) practices. This is realized in an integrated and
seamless manner. We implement and validate the proposed approach by extending
an open source modeling tool, called ThingML. ThingML is a domain-specific
language and modeling tool with code generation for the IoT/CPS domain. Neither
ThingML nor any other IoT/CPS modeling tool supports DA/ML at the modeling
level. Therefore, as the primary contribution of the doctoral dissertation, we
add the necessary syntax and semantics concerning DA/ML methods and techniques
to the modeling language of ThingML. Moreover, we support the APIs of several
ML libraries and frameworks for the automated generation of the source code of
the target software in Python and Java. Our approach enables
platform-independent, as well as platform-specific models. Further, we assist
in carrying out semiautomated DA/ML tasks by offering Automated ML (AutoML), in
the background (in expert mode), and through model-checking constraints and
hints at design-time. Finally, we consider three use case scenarios from the
domains of network security, smart energy systems and energy exchange markets."
http://arxiv.org/abs/2102.06406,Title: Annotation Cleaning for the MSR-Video to Text Dataset,"The video captioning task is to describe the video contents with natural
language by the machine. Many methods have been proposed for solving this task.
A large dataset called MSR Video to Text (MSR-VTT) is often used as the
benckmark dataset for testing the performance of the methods. However, we found
that the human annotations, i.e., the descriptions of video contents in the
dataset are quite noisy, e.g., there are many duplicate captions and many
captions contain grammatical problems. These problems may pose difficulties to
video captioning models for learning. We cleaned the MSR-VTT annotations by
removing these problems, then tested several typical video captioning models on
the cleaned dataset. Experimental results showed that data cleaning boosted the
performances of the models measured by popular quantitative metrics. We
recruited subjects to evaluate the results of a model trained on the original
and cleaned datasets. The human behavior experiment demonstrated that trained
on the cleaned dataset, the model generated captions that were more coherent
and more relevant to contents of the video clips. The cleaned dataset is
publicly available."
http://arxiv.org/abs/2102.06407,Title: Deep Sound Field Reconstruction in Real Rooms: Introducing the ISOBEL  Sound Field Dataset,"Knowledge of loudspeaker responses are useful in a number of applications,
where a sound system is located inside a room that alters the listening
experience depending on position within the room. Acquisition of sound fields
for sound sources located in reverberant rooms can be achieved through labor
intensive measurements of impulse response functions covering the room, or
alternatively by means of reconstruction methods which can potentially require
significantly fewer measurements. This paper extends evaluations of sound field
reconstruction at low frequencies by introducing a dataset with measurements
from four real rooms. The ISOBEL Sound Field dataset is publicly available, and
aims to bridge the gap between synthetic and real-world sound fields in
rectangular rooms. Moreover, the paper advances on a recent deep learning-based
method for sound field reconstruction using a very low number of microphones,
and proposes an approach for modeling both magnitude and phase response in a
U-Net-like neural network architecture. The complex-valued sound field
reconstruction demonstrates that the estimated room transfer functions are of
high enough accuracy to allow for personalized sound zones with contrast ratios
comparable to ideal room transfer functions using 15 microphones below 150 Hz."
http://arxiv.org/abs/2102.06408,Title: Two Elements of Pair Programming Skill,"Background: Pair programming (PP) can have many benefits in industry.
Researchers and practitioners recognize that successful and productive PP
involves some skill that might take time to learn and improve.
Question: What are the elements of pair programming skill?
Method: We perform qualitative analyses of industrial pair programming
sessions following the Grounded Theory Methodology. We look for patterns of
problematic behavior to conceptualize key elements of what 'good' and 'bad'
pairs do differently.
Results: Here, we report two elements of pair programming skill: Good pairs
(1) manage to maintain their Togetherness and (2) keep an eye on their
session's Expediency. We identify three problematic behavioral patterns that
affect one or both of these elements: Getting Lost in the Weeds, Losing the
Partner, and Drowning the Partner.
Conclusion: Pair programming skill is separate from general software
development skill. Years of PP experience are neither a prerequisite nor
sufficient for successful pair programming."
http://arxiv.org/abs/2102.06414,Title: Unified Compact Numerical Quadrature Formulas for Hadamard Finite Parts  of Singular Integrals of Periodic Functions,"We consider the numerical computation of finite-range singular integrals
$$I[f]=\intBar^b_a f(x)\,dx,\quad f(x)=\frac{g(x)}{(x-t)^m},\quad
m=1,2,\ldots,\quad
a<t<b,$$ that are defined in the sense of Hadamard Finite Part, assuming that
$g\in C^\infty[a,b]$ and $f(x)\in C^\infty(\mathbb{R}_t)$ is $T$-periodic with
$\mathbb{R}_t=\mathbb{R}\setminus\{t+ kT\}^\infty_{k=-\infty}$,
$T=b-a$. Using a generalization of the Euler--Maclaurin expansion developed
in [A. Sidi,
{Euler--Maclaurin} expansions for integrals with arbitrary algebraic endpoint
singularities. {\em Math. Comp.}, 81:2159--2173, 2012], we unify the treatment
of these integrals. For each $m$, we develop a number of numerical quadrature
formulas $\widehat{T}^{(s)}_{m,n}[f]$ of trapezoidal type for $I[f]$. For
example, three numerical quadrature formulas of trapezoidal type result from
this approach for the case $m=3$, and these are
\begin{align*}
\widehat{T}^{(0)}_{3,n}[f]&=h\sum^{n-1}_{j=1}f(t+jh)-\frac{\pi^2}{3}\,g'(t)\,h^{-1}
+\frac{1}{6}\,g'''(t)\,h, \quad h=\frac{T}{n},
\widehat{T}^{(1)}_{3,n}[f]&=h\sum^n_{j=1}f(t+jh-h/2)-\pi^2\,g'(t)\,h^{-1},\quad
h=\frac{T}{n},
\widehat{T}^{(2)}_{3,n}[f]&=2h\sum^n_{j=1}f(t+jh-h/2)-
\frac{h}{2}\sum^{2n}_{j=1}f(t+jh/2-h/4),\quad h=\frac{T}{n}.\end{align*}"
http://arxiv.org/abs/2102.06416,Title: Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph  Convolutional Neural Networks,"Most graph neural networks (GNN) perform poorly in graphs where neighbors
typically have different features/classes (heterophily) and when stacking
multiple layers (oversmoothing). These two seemingly unrelated problems have
been studied independently, but there is recent empirical evidence that solving
one problem may benefit the other. In this work, going beyond empirical
observations, we theoretically characterize the connections between heterophily
and oversmoothing, both of which lead to indistinguishable node
representations. By modeling the change in node representations during message
propagation, we theoretically analyze the factors (e.g., degree, heterophily
level) that make the representations of nodes from different classes
indistinguishable. Our analysis highlights that (1) nodes with high heterophily
and nodes with low heterophily and low degrees relative to their neighbors
(degree discrepancy) trigger the oversmoothing problem, and (2) allowing
""negative"" messages between neighbors can decouple the heterophily and
oversmoothing problems. Based on our insights, we design a model that addresses
the discrepancy in features and degrees between neighbors by incorporating
signed messages and learned degree corrections. Our experiments on 9 real
networks show that our model achieves state-of-the-art performance under
heterophily, and performs comparably to existing GNNs under low
heterophily(homophily). It also effectively addresses oversmoothing and even
benefits from multiple layers."
http://arxiv.org/abs/2102.06418,Title: A more accurate view of the Flat Wall Theorem,"We introduce a supporting combinatorial framework for the Flat Wall Theorem.
In particular, we suggest two variants of the theorem and we introduce a new,
more versatile, concept of wall homogeneity as well as the notion of regularity
in flat walls. All proposed concepts and results aim at facilitating the use of
the irrelevant vertex technique in future algorithmic applications."
http://arxiv.org/abs/2102.06422,Title: Content-Aware Speaker Embeddings for Speaker Diarisation,"Recent speaker diarisation systems often convert variable length speech
segments into fixed-length vector representations for speaker clustering, which
are known as speaker embeddings. In this paper, the content-aware speaker
embeddings (CASE) approach is proposed, which extends the input of the speaker
classifier to include not only acoustic features but also their corresponding
speech content, via phone, character, and word embeddings. Compared to
alternative methods that leverage similar information, such as multitask or
adversarial training, CASE factorises automatic speech recognition (ASR) from
speaker recognition to focus on modelling speaker characteristics and
correlations with the corresponding content units to derive more expressive
representations. CASE is evaluated for speaker re-clustering with a realistic
speaker diarisation setup using the AMI meeting transcription dataset, where
the content information is obtained by performing ASR based on an automatic
segmentation. Experimental results showed that CASE achieved a 17.8% relative
speaker error rate reduction over conventional methods."
http://arxiv.org/abs/2102.06423,Title: Exactness and Convergence Properties of Some Recent Numerical Quadrature  Formulas for Supersingular Integrals of Periodic Functions,"In a recent work, we developed three new compact numerical quadrature
formulas for finite-range periodic supersingular integrals $I[f]=\intBar^b_a
f(x)\,dx$, where $f(x)=g(x)/(x-t)^3,$ assuming that $g\in C^\infty[a,b]$ and
$f(x)$ is $T$-periodic, $T=b-a$. With $h=T/n$, these numerical quadrature
formulas read
\begin{align*}
\widehat{T}{}^{(0)}_n[f]&=h\sum^{n-1}_{j=1}f(t+jh)
-\frac{\pi^2}{3}\,g'(t)\,h^{-1}+\frac{1}{6}\,g'''(t)\,h,
\widehat{T}{}^{(1)}_n[f]&=h\sum^n_{j=1}f(t+jh-h/2)
-\pi^2\,g'(t)\,h^{-1},
\widehat{T}{}^{(2)}_n[f]&=2h\sum^n_{j=1}f(t+jh-h/2)-
\frac{h}{2}\sum^{2n}_{j=1}f(t+jh/2-h/4).
\end{align*}
We also showed that these formulas have spectral accuracy; that is,
$$\widehat{T}{}^{(s)}_n[f]-I[f]=O(n^{-\mu})\quad\text{as $n\to\infty$}\quad
\forall \mu>0.$$
In the present work, we continue our study of these formulas for the special
case in which
$f(x)=\frac{\cos\frac{\pi(x-t)}{T}}{\sin^3\frac{\pi(x-t)}{T}}\,u(x)$, where
$u(x)$ is in $C^\infty(\mathbb{R})$ and is $T$-periodic. Actually, we prove
that $\widehat{T}{}^{(s)}_n[f]$,
$s=0,1,2,$ are exact for a class of singular integrals involving $T$-periodic
trigonometric polynomials of degree at most $n-1$; that is, $$
\widehat{T}{}^{(s)}_n[f]=I[f]\quad\text{when\ \
$f(x)=\frac{\cos\frac{\pi(x-t)}{T}}{\sin^3\frac{\pi(x-t)}{T}}\,\sum^{n-1}_{m=-(n-1)}
c_m\exp(\mrm{i}2m\pi x/T)$.}$$ We also prove that, when $u(z)$ is analytic in a
strip $\big|\text{Im}\,z\big|<\sigma$ of the complex $z$-plane, the errors in
all three $\widehat{T}{}^{(s)}_n[f]$ are $O(e^{-2n\pi\sigma/T})$ as
$n\to\infty$, for all practical purposes."
http://arxiv.org/abs/2102.06427,Title: Transformer Language Models with LSTM-based Cross-utterance Information  Representation,"The effective incorporation of cross-utterance information has the potential
to improve language models (LMs) for automatic speech recognition (ASR). To
extract more powerful and robust cross-utterance representations for the
Transformer LM (TLM), this paper proposes the R-TLM which uses hidden states in
a long short-term memory (LSTM) LM. To encode the cross-utterance information,
the R-TLM incorporates an LSTM module together with a segment-wise recurrence
in some of the Transformer blocks. In addition to the LSTM module output, a
shortcut connection using a fusion layer that bypasses the LSTM module is also
investigated. The proposed system was evaluated on the AMI meeting corpus, the
Eval2000 and the RT03 telephone conversation evaluation sets. The best R-TLM
achieved 0.9%, 0.6%, and 0.8% absolute WER reductions over the single-utterance
TLM baseline, and 0.5%, 0.3%, 0.2% absolute WER reductions over a strong
cross-utterance TLM baseline on the AMI evaluation set, Eval2000 and RT03
respectively. Improvements on Eval2000 and RT03 were further supported by
significance tests. R-TLMs were found to have better LM scores on words where
recognition errors are more likely to occur. The R-TLM WER can be further
reduced by interpolation with an LSTM-LM."
http://arxiv.org/abs/2102.06429,Title: PVTSI$^{\boldmath(m)}$: A Novel Approach to Computation of Hadamard  Finite Parts of Nonperiodic Singular Integrals,"We consider the numerical computation of
$I[f]=\intBar^b_a f(x)\,dx$, the Hadamard Finite Part of the finite-range
singular integral $\int^b_a f(x)\,dx$, $f(x)=g(x)/(x-t)^{m}$ with $a<t<b$ and
$m\in\{1,2,\ldots\},$ assuming that (i)\,$g\in C^\infty(a,b)$ and (ii)\,$g(x)$
is allowed to have arbitrary integrable singularities at the endpoints $x=a$
and $x=b$.
We first prove that $\intBar^b_a f(x)\,dx$ is invariant under any suitable
variable transformation $x=\psi(\xi)$, $\psi:[\alpha,\beta]\rightarrow[a,b]$,
hence there holds $\intBar^\beta_\alpha F(\xi)\,d\xi=\intBar^b_a f(x)\,dx$,
where $F(\xi)=f(\psi(\xi))\,\psi'(\xi)$. Based on this result, we next choose
$\psi(\xi)$ such that the transformed integrand $F(\xi)$ is sufficiently
periodic with period $\T=\beta-\alpha$, and prove, with the help of some recent
extension/generalization of the Euler--Maclaurin expansion, that we can apply
to
$\intBar^\beta_\alpha F(\xi)\,d\xi$ the quadrature formulas derived for
periodic singular integrals developed in an earlier work of the author. We give
a whole family of numerical quadrature formulas for $\intBar^\beta_\alpha
F(\xi)\,d\xi$ for each $m$, which we denote $\widehat{T}^{(s)}_{m,n}[{\cal
F}]$, where ${\cal F}(\xi)$ is the $\T$-periodic extension of $F(\xi)$."
http://arxiv.org/abs/2102.06431,Title: Universal Adversarial Perturbations Through the Lens of Deep  Steganography: Towards A Fourier Perspective,"The booming interest in adversarial attacks stems from a misalignment between
human vision and a deep neural network (DNN), i.e. a human imperceptible
perturbation fools the DNN. Moreover, a single perturbation, often called
universal adversarial perturbation (UAP), can be generated to fool the DNN for
most images. A similar misalignment phenomenon has recently also been observed
in the deep steganography task, where a decoder network can retrieve a secret
image back from a slightly perturbed cover image. We attempt explaining the
success of both in a unified manner from the Fourier perspective. We perform
task-specific and joint analysis and reveal that (a) frequency is a key factor
that influences their performance based on the proposed entropy metric for
quantifying the frequency distribution; (b) their success can be attributed to
a DNN being highly sensitive to high-frequency content. We also perform feature
layer analysis for providing deep insight on model generalization and
robustness. Additionally, we propose two new variants of universal
perturbations: (1) Universal Secret Adversarial Perturbation (USAP) that
simultaneously achieves attack and hiding; (2) high-pass UAP (HP-UAP) that is
less visible to the human eye."
http://arxiv.org/abs/2102.06435,Title: Safety of Flow Decompositions in DAGs,"Network flows are one of the most studied combinatorial optimization problems
with innumerable applications. Any flow on a directed acyclic graph (DAG) $G$
having $n$ vertices and $m$ edges can be decomposed into a set of $O(m)$ paths,
with applications from network routing to assembly of biological sequences. In
some applications, the flow decomposition corresponds to some particular data
that need to be reconstructed from the flow, which require finding paths (or
subpaths) appearing in all possible flow decompositions, referred to as safe
paths.
Recently, Ma et al. [WABI 2020] addressed a related problem in a
probabilistic framework. Later, they gave a quadratic-time algorithm based on a
global criterion, for a generalized version (AND-Quant) of the corresponding
problem, i.e., reporting if a given flow path is safe. Our contributions are as
follows:
1- A simple characterization for the safety of a given path based on a local
criterion, which can be directly adapted to give an optimal linear time
verification algorithm.
2- A simple enumeration algorithm that reports all maximal safe paths on a
flow network in $O(mn)$ time. The algorithm reports all safe paths using a
compact representation of the solution (called ${\cal P}_c$), which is
$\Omega(mn)$ in the worst case, but merely $O(m+n)$ in the best case.
3- An improved enumeration algorithm where all safe paths ending at every
vertex are represented as funnels using $O(n^2+|{\cal P}_c|)$ space. These can
be computed and used to report all maximal safe paths, using time linear in the
total space required by funnels, with an extra logarithmic factor.
Overall we present a simple characterization for the problem leading to an
optimal verification algorithm and a simple enumeration algorithm. The
enumeration algorithm is improved using the funnel structures for safe paths,
which may be of independent interest."
http://arxiv.org/abs/2102.06439,Title: Leveraging Benchmarking Data for Informed One-Shot Dynamic Algorithm  Selection,"A key challenge in the application of evolutionary algorithms in practice is
the selection of an algorithm instance that best suits the problem at hand.
What complicates this decision further is that different algorithms may be best
suited for different stages of the optimization process. Dynamic algorithm
selection and configuration are therefore well-researched topics in
evolutionary computation. However, while hyper-heuristics and parameter control
studies typically assume a setting in which the algorithm needs to be chosen
while running the algorithms, without prior information, AutoML approaches such
as hyper-parameter tuning and automated algorithm configuration assume the
possibility of evaluating different configurations before making a final
recommendation. In practice, however, we are often in a middle-ground between
these two settings, where we need to decide on the algorithm instance before
the run (""oneshot"" setting), but where we have (possibly lots of) data
available on which we can base an informed decision.
We analyze in this work how such prior performance data can be used to infer
informed dynamic algorithm selection schemes for the solution of pseudo-Boolean
optimization problems. Our specific use-case considers a family of genetic
algorithms."
http://arxiv.org/abs/2102.06440,Title: Scalable Bayesian Inverse Reinforcement Learning,"Bayesian inference over the reward presents an ideal solution to the
ill-posed nature of the inverse reinforcement learning problem. Unfortunately
current methods generally do not scale well beyond the small tabular setting
due to the need for an inner-loop MDP solver, and even non-Bayesian methods
that do themselves scale often require extensive interaction with the
environment to perform well, being inappropriate for high stakes or costly
applications such as healthcare. In this paper we introduce our method,
Approximate Variational Reward Imitation Learning (AVRIL), that addresses both
of these issues by jointly learning an approximate posterior distribution over
the reward that scales to arbitrarily complicated state spaces alongside an
appropriate policy in a completely offline manner through a variational
approach to said latent reward. Applying our method to real medical data
alongside classic control simulations, we demonstrate Bayesian reward inference
in environments beyond the scope of current methods, as well as task
performance competitive with focused offline imitation learning algorithms."
http://arxiv.org/abs/2102.06442,Title: A space-time discretization of a nonlinear peridynamic model on a 2D  lamina,"Peridynamics is a nonlocal theory for dynamic fracture analysis consisting in
a second order in time partial integro-differential equation. In this paper, we
consider a nonlinear model of peridynamics in a two-dimensional spatial domain.
We implement a spectral method for the space discretization based on the
Fourier expansion of the solution while we consider the Newmark-$\beta$ method
for the time marching. This computational approach takes advantages from the
convolutional form of the peridynamic operator and from the use of the discrete
Fourier transform. We show a convergence result for the fully discrete
approximation and study the stability of the method applied to the linear
peridynamic model. Finally, we perform several numerical tests and comparisons
to validate our results and provide simulations implementing a volume
penalization technique to avoid the limitation of periodic boundary conditions
due to the spectral approach."
http://arxiv.org/abs/2102.06445,Title: Adaptive Sampling for Fast Constrained Maximization of Submodular  Function,"Several large-scale machine learning tasks, such as data summarization, can
be approached by maximizing functions that satisfy submodularity. These
optimization problems often involve complex side constraints, imposed by the
underlying application. In this paper, we develop an algorithm with
poly-logarithmic adaptivity for non-monotone submodular maximization under
general side constraints. The adaptive complexity of a problem is the minimal
number of sequential rounds required to achieve the objective.
Our algorithm is suitable to maximize a non-monotone submodular function
under a $p$-system side constraint, and it achieves a $(p +
O(\sqrt{p}))$-approximation for this problem, after only poly-logarithmic
adaptive rounds and polynomial queries to the valuation oracle function.
Furthermore, our algorithm achieves a $(p + O(1))$-approximation when the given
side constraint is a $p$-extendible system.
This algorithm yields an exponential speed-up, with respect to the
adaptivity, over any other known constant-factor approximation algorithm for
this problem. It also competes with previous known results in terms of the
query complexity. We perform various experiments on various real-world
applications. We find that, in comparison with commonly used heuristics, our
algorithm performs better on these instances."
http://arxiv.org/abs/2102.06448,Title: End-to-End Intelligent Framework for Rockfall Detection,"Rockfall detection is a crucial procedure in the field of geology, which
helps to reduce the associated risks. Currently, geologists identify rockfall
events almost manually utilizing point cloud and imagery data obtained from
different caption devices such as Terrestrial Laser Scanner or digital cameras.
Multi-temporal comparison of the point clouds obtained with these techniques
requires a tedious visual inspection to identify rockfall events which implies
inaccuracies that depend on several factors such as human expertise and the
sensibility of the sensors. This paper addresses this issue and provides an
intelligent framework for rockfall event detection for any individual working
in the intersection of the geology domain and decision support systems. The
development of such an analysis framework poses significant research challenges
and justifies intensive experimental analysis. In particular, we propose an
intelligent system that utilizes multiple machine learning algorithms to detect
rockfall clusters of point cloud data. Due to the extremely imbalanced nature
of the problem, a plethora of state-of-the-art resampling techniques
accompanied by multiple models and feature selection procedures are being
investigated. Various machine learning pipeline combinations have been
benchmarked and compared applying well-known metrics to be incorporated into
our system. Specifically, we developed statistical and machine learning
techniques and applied them to analyze point cloud data extracted from
Terrestrial Laser Scanner in two distinct case studies, involving different
geological contexts: the basaltic cliff of Castellfollit de la Roca and the
conglomerate Montserrat Massif, both located in Spain. Our experimental data
suggest that some of the above-mentioned machine learning pipelines can be
utilized to detect rockfall incidents on mountain walls, with experimentally
proven accuracy."
http://arxiv.org/abs/2102.06454,Title: Customizable Stochastic High Fidelity Model of the Sensors and Camera  onboard a Low SWaP Fixed Wing Autonomous Aircraft,"The navigation systems of autonomous aircraft rely on the readings provided
by a suite of onboard sensors to estimate the aircraft state. In the case of
fixed wing vehicles, the sensor suite is composed by triads of accelerometers,
gyroscopes, and magnetometers, a Global Navigation Satellite System (GNSS)
receiver, and an air data system (Pitot tube, air vanes, thermometer, and
barometer), and is often complemented by one or more digital cameras. An
accurate representation of the behavior and error sources of each of these
sensors, together with the images generated by the cameras, in indispensable
for flight simulation and the evaluation of novel inertial or visual navigation
algorithms, and more so in the case of low SWaP (size, weight, and power)
aircraft, in which the quality and price of the sensors is limited. This
article presents realistic and customizable models for each of these sensors,
which have been implemented as an open-source C ++ simulation. Provided with
the true variation of the aircraft state with time, the simulation provides a
time stamped series of the errors generated by all sensors, as well as
realistic images of the Earth surface that resemble those taken from a real
camera flying along the indicated state positions and attitudes."
http://arxiv.org/abs/2102.06455,Title: On Signings and the Well-Founded Semantics,"In this note, we use Kunen's notion of a signing to establish two theorems
about the well-founded semantics of logic programs, in the case where we are
interested in only (say) the positive literals of a predicate $p$ that are
consequences of the program. The first theorem identifies a class of programs
for which the well-founded and Fitting semantics coincide for the positive part
of $p$. The second theorem shows that if a program has a signing then computing
the positive part of $p$ under the well-founded semantics requires the
computation of only one part of each predicate. This theorem suggests an
analysis for query-answering under the well-founded semantics. In the process
of proving these results, we use an alternative formulation of the well-founded
semantics of logic programs, which might be of independent interest."
http://arxiv.org/abs/2102.06460,Title: Depthwise Separable Convolutions Allow for Fast and Memory-Efficient  Spectral Normalization,"An increasing number of models require the control of the spectral norm of
convolutional layers of a neural network. While there is an abundance of
methods for estimating and enforcing upper bounds on those during training,
they are typically costly in either memory or time. In this work, we introduce
a very simple method for spectral normalization of depthwise separable
convolutions, which introduces negligible computational and memory overhead. We
demonstrate the effectiveness of our method on image classification tasks using
standard architectures like MobileNetV2."
http://arxiv.org/abs/2102.06461,Title: Business Model Canvas Should Pay More Attention to the Software Startup  Team,"Business Model Canvas (BMC) is a tool widely used to describe startup
business models. Despite the various business aspects described, BMC pays a
little emphasis on team-related factors. The importance of team-related factors
in software development has been acknowledged widely in literature. While not
as extensively studied, the importance of teams in software startups is also
known in both literature and among practitioners. In this paper, we propose
potential changes to BMC to have the tool better reflect the importance of the
team, especially in a software startup environment. Based on a literature
review, we identify various components related to the team, which we then
further support with empirical data. We do so by means of a qualitative case
study of five startups."
http://arxiv.org/abs/2102.06462,Title: Amidst Uncertainty -- or Not? Decision-Making in Early-Stage Software  Startups,"It is commonly claimed that the initial stages of any startup business are
dominated by continuous, extended uncertainty, in an environment that has even
been described as chaotic. Consequently, decisions are made in uncertain
circumstances, so making the right decision is crucial to successful business.
However, little currently exists in the way of empirical studies into this
supposed uncertainty. In this paper, we study decision-making in early-stage
software startups by means of a single, in-depth case study. Based on our data,
we argue that software startups do not work in a chaotic environment, nor are
they characterized by unique uncertainty unlike that experienced by other
firms."
http://arxiv.org/abs/2102.06463,Title: Reducing Waiting Times at Charging Stations with Adaptive Electric  Vehicle Route Planning,"Electric vehicles are becoming more popular all over the world. With
increasing battery capacities and a growing fast-charging infrastructure, they
are becoming suitable for long distance travel. However, queues at charging
stations could lead to long waiting times, making efficient route planning even
more important. In general, optimal multi-objective route planning is extremely
computationally expensive. We propose an adaptive charging and routing
strategy, which considers driving, waiting, and charging time. For this, we
developed a multi-criterion shortest-path search algorithm using contraction
hierarchies. To further reduce the computational effort, we precompute
shortest-path trees between the known locations of the charging stations. We
propose a central charging station database (CSDB) that helps estimating
waiting times at charging stations ahead of time. This enables our adaptive
charging and routing strategy to reduce these waiting times. In an extensive
set of simulation experiments, we demonstrate the advantages of our concept,
which reduces average waiting times at charging stations by up to 97 %. Even if
only a subset of the cars uses the CSDB approach, we can substantially reduce
waiting times and thereby the total travel time of electric vehicles."
http://arxiv.org/abs/2102.06467,Title: Robust Data-Driven Discovery of Partial Differential Equations under  Uncertainties,"Robust physics (e.g., governing equations and laws) discovery is of great
interest for many engineering fields and explainable machine learning. A
critical challenge compared with general training is that the term and format
of governing equations is not known as a prior. In addition, significant
measurement noise and complex algorithm hyperparameter tuning usually reduces
the robustness of existing methods. A robust data-driven method is proposed in
this study for identifying the governing Partial Differential Equations (PDEs)
of a given system from noisy data. The proposed method is based on the concept
of Progressive Sparse Identification of PDEs (PSI-PDE or $\psi$-PDE). Special
focus is on the handling of data with huge uncertainties (e.g., 50$\%$ noise
level). Neural Network modeling and fast Fourier transform (FFT) are
implemented to reduce the influence of noise in sparse regression. Following
this, candidate terms from the prescribed library are progressively selected
and added to the learned PDEs, which automatically promotes parsimony with
respect to the number of terms in PDEs as well as their complexity. Next, the
significance of each learned terms is further evaluated and the coefficients of
PDE terms are optimized by minimizing the L2 residuals. Results of numerical
case studies indicate that the governing PDEs of many canonical dynamical
systems can be correctly identified using the proposed $\psi$-PDE method with
highly noisy data. One great benefit of proposed algorithm is that it avoids
complex algorithm modification and hyperparameter tuning in most existing
methods. Limitations of the proposed method and major findings are presented."
http://arxiv.org/abs/2102.06469,Title: When no news is bad news -- Detection of negative events from news media  content,"During the first wave of Covid-19 information decoupling could be observed in
the flow of news media content. The corollary of the content alignment within
and between news sources experienced by readers (i.e., all news transformed
into Corona-news), was that the novelty of news content went down as media
focused monotonically on the pandemic event. This all-important Covid-19 news
theme turned out to be quite persistent as the pandemic continued, resulting in
the, from a news media's perspective, paradoxical situation where the same news
was repeated over and over. This information phenomenon, where novelty
decreases and persistence increases, has previously been used to track change
in news media, but in this study we specifically test the claim that new
information decoupling behavior of media can be used to reliably detect change
in news media content originating in a negative event, using a Bayesian
approach to change point detection."
http://arxiv.org/abs/2102.06474,Title: Predicting and Attending to Damaging Collisions for Placing Everyday  Objects in Photo-Realistic Simulations,"Placing objects is a fundamental task for domestic service robots (DSRs).
Thus, inferring the collision-risk before a placing motion is crucial for
achieving the requested task. This problem is particularly challenging because
it is necessary to predict what happens if an object is placed in a cluttered
designated area. We show that a rule-based approach that uses plane detection,
to detect free areas, performs poorly. To address this, we develop PonNet,
which has multimodal attention branches and a self-attention mechanism to
predict damaging collisions, based on RGBD images. Our method can visualize the
risk of damaging collisions, which is convenient because it enables the user to
understand the risk. For this purpose, we build and publish an original dataset
that contains 12,000 photo-realistic images of specific placing areas, with
daily life objects, in home environments. The experimental results show that
our approach improves accuracy compared with the baseline methods."
http://arxiv.org/abs/2102.06476,Title: Interpretable Predictive Maintenance for Hard Drives,"Existing machine learning approaches for data-driven predictive maintenance
are usually black boxes that claim high predictive power yet cannot be
understood by humans. This limits the ability of humans to use these models to
derive insights and understanding of the underlying failure mechanisms, and
also limits the degree of confidence that can be placed in such a system to
perform well on future data. We consider the task of predicting hard drive
failure in a data center using recent algorithms for interpretable machine
learning. We demonstrate that these methods provide meaningful insights about
short- and long-term drive health, while also maintaining high predictive
performance. We also show that these analyses still deliver useful insights
even when limited historical data is available, enabling their use in
situations where data collection has only recently begun."
http://arxiv.org/abs/2102.06477,Title: A Non-Intrusive Machine Learning Solution for Malware Detection and Data  Theft Classification in Smartphones,"Smartphones contain information that is more sensitive and personal than
those found on computers and laptops. With an increase in the versatility of
smartphone functionality, more data has become vulnerable and exposed to
attackers. Successful mobile malware attacks could steal a user's location,
photos, or even banking information. Due to a lack of post-attack strategies
firms also risk going out of business due to data theft. Thus, there is a need
besides just detecting malware intrusion in smartphones but to also identify
the data that has been stolen to assess, aid in recovery and prevent future
attacks. In this paper, we propose an accessible, non-intrusive machine
learning solution to not only detect malware intrusion but also identify the
type of data stolen for any app under supervision. We do this with Android
usage data obtained by utilising publicly available data collection framework-
SherLock. We test the performance of our architecture for multiple users on
real-world data collected using the same framework. Our architecture exhibits
less than 9% inaccuracy in detecting malware and can classify with 83%
certainty on the type of data that is being stolen."
http://arxiv.org/abs/2102.06479,Title: Complete Bidirectional Typing for the Calculus of Inductive  Constructions,"This article presents a bidirectional type system for the Calculus of
Inductive Constructions (CIC). It introduces a novel judgement intermediate
between the usual inference and checking, dubbed constrained inference, to
handle the presence of computation in types. The key property is the
completeness of the system with respect to the usual undirected one, which has
been formally proven in Coq as a part of the MetaCoq project. Although it plays
a central role in an ongoing completeness proof for a realistic typing
algorithm, the interest of bidirectionality is much wider, as it clarifies
previous works in the area and gives strong insights and structure when trying
to prove properties on CIC or design variations and extensions."
http://arxiv.org/abs/2102.06480,Title: Bootstrapped Representation Learning on Graphs,"Current state-of-the-art self-supervised learning methods for graph neural
networks (GNNs) are based on contrastive learning. As such, they heavily depend
on the construction of augmentations and negative examples. For example, on the
standard PPI benchmark, increasing the number of negative pairs improves
performance, thereby requiring computation and memory cost quadratic in the
number of nodes to achieve peak performance. Inspired by BYOL, a recently
introduced method for self-supervised learning that does not require negative
pairs, we present Bootstrapped Graph Latents, BGRL, a self-supervised graph
representation method that gets rid of this potentially quadratic bottleneck.
BGRL outperforms or matches the previous unsupervised state-of-the-art results
on several established benchmark datasets. Moreover, it enables the effective
usage of graph attentional (GAT) encoders, allowing us to further improve the
state of the art. In particular on the PPI dataset, using GAT as an encoder we
achieve state-of-the-art 70.49% Micro-F1, using the linear evaluation protocol.
On all other datasets under consideration, our model is competitive with the
equivalent supervised GNN results, often exceeding them."
http://arxiv.org/abs/2102.06481,Title: How do climate change skeptics engage with opposing views? Understanding  mechanisms of social identity and cognitive dissonance in an online forum,"Does engagement with opposing views help break down ideological `echo
chambers'; or does it backfire and reinforce them? This question remains
critical as academics, policymakers and activists grapple with the question of
how to regulate political discussion on social media. In this study, we
contribute to the debate by examining the impact of opposing views within a
major climate change skeptic online community on Reddit. A large sample of
posts (N = 3000) was manually coded as either dissonant or consonant which
allowed the automated classification of the full dataset of more than 50,000
posts, with codes inferred from linked websites. We find that ideologically
dissonant submissions act as a stimulant to activity in the community: they
received more attention (comments) than consonant submissions, even though they
received lower scores through up-voting and down-voting. Users who engaged with
dissonant submissions were also more likely to return to the forum. Consistent
with identity theory, confrontation with opposing views triggered activity in
the forum, particularly among users that are highly engaged with the community.
In light of the findings, theory of social identity and echo chambers is
discussed and enhanced."
http://arxiv.org/abs/2102.06483,Title: VitrAI -- Applying Explainable AI in the Real World,"With recent progress in the field of Explainable Artificial Intelligence
(XAI) and increasing use in practice, the need for an evaluation of different
XAI methods and their explanation quality in practical usage scenarios arises.
For this purpose, we present VitrAI, which is a web-based service with the goal
of uniformly demonstrating four different XAI algorithms in the context of
three real life scenarios and evaluating their performance and
comprehensibility for humans. This work reveals practical obstacles when
adopting XAI methods and gives qualitative estimates on how well different
approaches perform in said scenarios."
http://arxiv.org/abs/2102.06485,Title: Leveraging Reinforcement Learning for evaluating Robustness of KNN  Search Algorithms,"The problem of finding K-nearest neighbors in the given dataset for a given
query point has been worked upon since several years. In very high dimensional
spaces the K-nearest neighbor search (KNNS) suffers in terms of complexity in
computation of high dimensional distances. With the issue of curse of
dimensionality, it gets quite tedious to reliably bank on the results of
variety approximate nearest neighbor search approaches. In this paper, we
survey some novel K-Nearest Neighbor Search approaches that tackles the problem
of Search from the perspectives of computations, the accuracy of approximated
results and leveraging parallelism to speed-up computations. We attempt to
derive a relationship between the true positive and false points for a given
KNNS approach. Finally, in order to evaluate the robustness of a KNNS approach
against adversarial points, we propose a generic Reinforcement Learning based
framework for the same."
http://arxiv.org/abs/2102.06486,Title: Mutually exciting point process graphs for modelling dynamic networks,"A new class of models for dynamic networks is proposed, called mutually
exciting point process graphs (MEG), motivated by a practical application in
computer network security. MEG is a scalable network-wide statistical model for
point processes with dyadic marks, which can be used for anomaly detection when
assessing the significance of previously unobserved connections. The model
combines mutually exciting point processes to estimate dependencies between
events and latent space models to infer relationships between the nodes. The
intensity functions for each network edge are parameterised exclusively by
node-specific parameters, which allows information to be shared across the
network. Fast inferential procedures using modern gradient ascent algorithms
are exploited. The model is tested on simulated graphs and real world computer
network datasets, demonstrating excellent performance."
http://arxiv.org/abs/2102.06489,Title: A Tale of Two Countries: A Longitudinal Cross-Country Study of Mobile  Users' Reactions to the COVID-19 Pandemic Through the Lens of App Popularity,"The ongoing COVID-19 pandemic has profoundly impacted people's life around
the world, including how they interact with mobile technologies. In this paper,
we seek to develop an understanding of how the dynamic trajectory of a pandemic
shapes mobile phone users' experiences. Through the lens of app popularity, we
approach this goal from a cross-country perspective. We compile a dataset
consisting of six-month daily snapshots of the most popular apps in the iOS App
Store in China and the US, where the pandemic has exhibited distinct
trajectories. Using this longitudinal dataset, our analysis provides detailed
patterns of app ranking during the pandemic at both category and individual app
levels. We reveal that app categories' rankings are correlated with the
pandemic, contingent upon country-specific development trajectories. Our work
offers rich insights into how the COVID-19, a typical global public health
crisis, has influence people's day-to-day interaction with the Internet and
mobile technologies."
http://arxiv.org/abs/2102.06491,Title: Improving Object Detection in Art Images Using Only Style Transfer,"Despite recent advances in object detection using deep learning neural
networks, these neural networks still struggle to identify objects in art
images such as paintings and drawings. This challenge is known as the cross
depiction problem and it stems in part from the tendency of neural networks to
prioritize identification of an object's texture over its shape. In this paper
we propose and evaluate a process for training neural networks to localize
objects - specifically people - in art images. We generate a large dataset for
training and validation by modifying the images in the COCO dataset using AdaIn
style transfer. This dataset is used to fine-tune a Faster R-CNN object
detection network, which is then tested on the existing People-Art testing
dataset. The result is a significant improvement on the state of the art and a
new way forward for creating datasets to train neural networks to process art
images."
http://arxiv.org/abs/2102.06492,Title: Algebraic cocompleteness and finitary functors,"A number of categories is presented that are algebraically complete and
cocomplete, i.e., every endofunctor has an initial algebra and a terminal
coalgebra. For all finitary (and, more generally, all precontinuous) set
functors the initial algebra and terminal coalgebra are proved to carry a
canonical partial order with the same ideal CPO-completion. And they also both
carry a canonical ultrametric with the same Cauchy completion."
http://arxiv.org/abs/2102.06495,Title: CrossStack: A 3-D Reconfigurable RRAM Crossbar Inference Engine,"Deep neural network inference accelerators are rapidly growing in importance
as we turn to massively parallelized processing beyond GPUs and ASICs. The
dominant operation in feedforward inference is the multiply-and-accumlate
process, where each column in a crossbar generates the current response of a
single neuron. As a result, memristor crossbar arrays parallelize inference and
image processing tasks very efficiently. In this brief, we present a 3-D active
memristor crossbar array `CrossStack', which adopts stacked pairs of
Al/TiO2/TiO2-x/Al devices with common middle electrodes. By designing
CMOS-memristor hybrid cells used in the layout of the array, CrossStack can
operate in one of two user-configurable modes as a reconfigurable inference
engine: 1) expansion mode and 2) deep-net mode. In expansion mode, the
resolution of the network is doubled by increasing the number of inputs for a
given chip area, reducing IR drop by 22%. In deep-net mode, inference speed
per-10-bit convolution is improved by 29\% by simultaneously using one
TiO2/TiO2-x layer for read processes, and the other for write processes. We
experimentally verify both modes on our $10\times10\times2$ array."
http://arxiv.org/abs/2102.06496,Title: Lazy Hermite Reduction and Creative Telescoping for Algebraic Functions,"We present criteria on the existence of telescopers for trivariate rational
functions in four mixed cases, in which discrete and continuous variables
appear simultaneously. We reduce the existence problem in the trivariate case
to the exactness testing problem, the separation problem and the existence
problem in the bivariate case. The existence criteria we present help us
determine the termination of Zeilberger's algorithm for the input functions
studied in this paper."
http://arxiv.org/abs/2102.06500,Title: Jacobian Determinant of Normalizing Flows,"Normalizing flows learn a diffeomorphic mapping between the target and base
distribution, while the Jacobian determinant of that mapping forms another
real-valued function. In this paper, we show that the Jacobian determinant
mapping is unique for the given distributions, hence the likelihood objective
of flows has a unique global optimum. In particular, the likelihood for a class
of flows is explicitly expressed by the eigenvalues of the auto-correlation
matrix of individual data point, and independent of the parameterization of
neural network, which provides a theoretical optimal value of likelihood
objective and relates to probabilistic PCA. Additionally, Jacobian determinant
is a measure of local volume change and is maximized when MLE is used for
optimization. To stabilize normalizing flows training, it is required to
maintain a balance between the expansiveness and contraction of volume, meaning
Lipschitz constraint on the diffeomorphic mapping and its inverse. With these
theoretical results, several principles of designing normalizing flow were
proposed. And numerical experiments on highdimensional datasets (such as
CelebA-HQ 1024x1024) were conducted to show the improved stability of training."
http://arxiv.org/abs/2102.06501,Title: Two Training Strategies for Improving Relation Extraction over Universal  Graph,"This paper explores how the Distantly Supervised Relation Extraction (DS-RE)
can benefit from the use of a Universal Graph (UG), the combination of a
Knowledge Graph (KG) and a large-scale text collection. A straightforward
extension of a current state-of-the-art neural model for DS-RE with a UG may
lead to degradation in performance. We first report that this degradation is
associated with the difficulty in learning a UG and then propose two training
strategies: (1) Path Type Adaptive Pretraining, which sequentially trains the
model with different types of UG paths so as to prevent the reliance on a
single type of UG path; and (2) Complexity Ranking Guided Attention mechanism,
which restricts the attention span according to the complexity of a UG path so
as to force the model to extract features not only from simple UG paths but
also from complex ones. Experimental results on both biomedical and NYT10
datasets prove the robustness of our methods and achieve a new state-of-the-art
result on the NYT10 dataset. The code and datasets used in this paper are
available at https://github.com/baodaiqin/UGDSRE."
http://arxiv.org/abs/2102.06502,Title: Computing Betweenness Centrality in Link Streams,"Betweeness centrality is one of the most important concepts in graph
analysis. It was recently extended to link streams, a graph generalization
where links arrive over time. However, its computation raises non-trivial
issues, due in particular to the fact that time is considered as continuous. We
provide here the first algorithms to compute this generalized betweenness
centrality, as well as several companion algorithms that have their own
interest. They work in polynomial time and space, we illustrate them on typical
examples, and we provide an implementation."
http://arxiv.org/abs/2102.06503,Title: A Little Pretraining Goes a Long Way: A Case Study on Dependency Parsing  Task for Low-resource Morphologically Rich Languages,"Neural dependency parsing has achieved remarkable performance for many
domains and languages. The bottleneck of massive labeled data limits the
effectiveness of these approaches for low resource languages. In this work, we
focus on dependency parsing for morphological rich languages (MRLs) in a
low-resource setting. Although morphological information is essential for the
dependency parsing task, the morphological disambiguation and lack of powerful
analyzers pose challenges to get this information for MRLs. To address these
challenges, we propose simple auxiliary tasks for pretraining. We perform
experiments on 10 MRLs in low-resource settings to measure the efficacy of our
proposed pretraining method and observe an average absolute gain of 2 points
(UAS) and 3.6 points (LAS). Code and data available at:
https://github.com/jivnesh/LCM"
http://arxiv.org/abs/2102.06504,Title: From System Level Synthesis to Robust Closed-loop Data-enabled  Predictive Control,"The Willem's fundamental lemma and the system level synthesis characterize
trajectories generated by a linear system. While the former method is valid for
deterministic LTI systems, the latter one is further effective for stochastic
linear time varying systems. In this paper, these two methods offers equivalent
characterization of stochastic LTI systems. Inspired by this observation, a
robust closed-loop data-enabled predictive control scheme is proposed for
stochastic LTI systems. A causal feedback structure is further derived, leading
to an computational cost similar to standard robust MPC with full state
measurements."
http://arxiv.org/abs/2102.06505,Title: Exploiting Spline Models for the Training of Fully Connected Layers in  Neural Network,"The fully connected (FC) layer, one of the most fundamental modules in
artificial neural networks (ANN), is often considered difficult and inefficient
to train due to issues including the risk of overfitting caused by its large
amount of parameters. Based on previous work studying ANN from linear spline
perspectives, we propose a spline-based approach that eases the difficulty of
training FC layers. Given some dataset, we first obtain a continuous piece-wise
linear (CPWL) fit through spline methods such as multivariate adaptive
regression spline (MARS). Next, we construct an ANN model from the linear
spline model and continue to train the ANN model on the dataset using gradient
descent optimization algorithms. Our experimental results and theoretical
analysis show that our approach reduces the computational cost, accelerates the
convergence of FC layers, and significantly increases the interpretability of
the resulting model (FC layers) compared with standard ANN training with random
parameter initialization followed by gradient descent optimizations."
http://arxiv.org/abs/2102.06507,Title: Online Graph Dictionary Learning,"Dictionary learning is a key tool for representation learning, that explains
the data as linear combination of few basic elements. Yet, this analysis is not
amenable in the context of graph learning, as graphs usually belong to
different metric spaces. We fill this gap by proposing a new online Graph
Dictionary Learning approach, which uses the Gromov Wasserstein divergence for
the data fitting term. In our work, graphs are encoded through their nodes'
pairwise relations and modeled as convex combination of graph atoms, i.e.
dictionary elements, estimated thanks to an online stochastic algorithm, which
operates on a dataset of unregistered graphs with potentially different number
of nodes. Our approach naturally extends to labeled graphs, and is completed by
a novel upper bound that can be used as a fast approximation of Gromov
Wasserstein in the embedding space. We provide numerical evidences showing the
interest of our approach for unsupervised embedding of graph datasets and for
online graph subspace estimation and tracking."
http://arxiv.org/abs/2102.06509,Title: Updatable Materialization of Approximate Constraints,"Modern big data applications integrate data from various sources. As a
result, these datasets may not satisfy perfect constraints, leading to sparse
schema information and non-optimal query performance. The existing approach of
PatchIndexes enable the definition of approximate constraints and improve query
performance by exploiting the materialized constraint information. As real
world data warehouse workloads are often not limited to read-only queries, we
enhance the PatchIndex structure towards an update-conscious design in this
paper. Therefore, we present a sharded bitmap as the underlying data structure
which offers efficient update operations, and describe approaches to maintain
approximate constraints under updates, avoiding index recomputations and full
table scans. In our evaluation, we prove that PatchIndexes significantly impact
query performance while achieving lightweight update support."
http://arxiv.org/abs/2102.06511,Title: Continuous Learning in Neural Machine Translation using Bilingual  Dictionaries,"While recent advances in deep learning led to significant improvements in
machine translation, neural machine translation is often still not able to
continuously adapt to the environment. For humans, as well as for machine
translation, bilingual dictionaries are a promising knowledge source to
continuously integrate new knowledge. However, their exploitation poses several
challenges: The system needs to be able to perform one-shot learning as well as
model the morphology of source and target language.
In this work, we proposed an evaluation framework to assess the ability of
neural machine translation to continuously learn new phrases. We integrate
one-shot learning methods for neural machine translation with different word
representations and show that it is important to address both in order to
successfully make use of bilingual dictionaries. By addressing both challenges
we are able to improve the ability to translate new, rare words and phrases
from 30% to up to 70%. The correct lemma is even generated by more than 90%."
http://arxiv.org/abs/2102.06513,Title: How Far Should We Look Back to Achieve Effective Real-Time Time-Series  Anomaly Detection?,"Anomaly detection is the process of identifying unexpected events or
ab-normalities in data, and it has been applied in many different areas such as
system monitoring, fraud detection, healthcare, intrusion detection, etc.
Providing real-time, lightweight, and proactive anomaly detection for time
series with neither human intervention nor domain knowledge could be highly
valuable since it reduces human effort and enables appropriate countermeasures
to be undertaken before a disastrous event occurs. To our knowledge, RePAD
(Real-time Proactive Anomaly Detection algorithm) is a generic approach with
all above-mentioned features. To achieve real-time and lightweight detection,
RePAD utilizes Long Short-Term Memory (LSTM) to detect whether or not each
upcoming data point is anomalous based on short-term historical data points.
However, it is unclear that how different amounts of historical data points
affect the performance of RePAD. Therefore, in this paper, we investigate the
impact of different amounts of historical data on RePAD by introducing a set of
performance metrics that cover novel detection accuracy measures, time
efficiency, readiness, and resource consumption, etc. Empirical experiments
based on real-world time series datasets are conducted to evaluate RePAD in
different scenarios, and the experimental results are presented and discussed."
http://arxiv.org/abs/2102.06514,Title: Querying collections of tree-structured records in the presence of  within-record referential constraints,"In this paper, we consider a tree-structured data model used in many
commercial databases like Dremel, F1, JSON. We define identity and referential
constraints within each tree-structured record. The query language is a variant
of SQL and flattening is used as an evaluation mechanism. We investigate
querying in the presence of these constraints, and point out the challenges
that arise from taking them into account during query evaluation."
http://arxiv.org/abs/2102.06515,Title: Analysis of Interpolation based Image In-painting Approaches,"Interpolation and internal painting are one of the basic approaches in image
internal painting, which is used to eliminate undesirable parts that occur in
digital images or to enhance faulty parts. This study was designed to compare
the interpolation algorithms used in image in-painting in the literature.
Errors and noise generated on the colour and grayscale formats of some of the
commonly used standard images in the literature were corrected by using Cubic,
Kriging, Radial based function and High dimensional model representation
approaches and the results were compared using standard image comparison
criteria, namely, PSNR (peak signal-to-noise ratio), SSIM (Structural
SIMilarity), Mean Square Error (MSE). According to the results obtained from
the study, the absolute superiority of the methods against each other was not
observed. However, Kriging and RBF interpolation give better results both for
numerical data and visual evaluation for image in-painting problems with large
area losses."
http://arxiv.org/abs/2102.06516,Title: Work-Optimal Parallel Minimum Cuts for Non-Sparse Graphs,"We present the first work-optimal polylogarithmic-depth parallel algorithm
for the minimum cut problem on non-sparse graphs. For $m\geq n^{1+\epsilon}$
for any constant $\epsilon>0$, our algorithm requires $O(m \log n)$ work and
$O(\log^3 n)$ depth and succeeds with high probability. Its work matches the
best $O(m \log n)$ runtime for sequential algorithms [MN STOC 2020, GMW SOSA
2021]. This improves the previous best work by Geissmann and Gianinazzi [SPAA
2018] by $O(\log^3 n)$ factor, while matching the depth of their algorithm. To
do this, we design a work-efficient approximation algorithm and parallelize the
recent sequential algorithms [MN STOC 2020; GMW SOSA 2021] that exploit a
connection between 2-respecting minimum cuts and 2-dimensional orthogonal range
searching."
http://arxiv.org/abs/2102.06518,"Title: User manual for bch, a program for the fast computation of the  Baker-Campbell-Hausdorff and similar series","This manual describes bch, an efficient program written in the C programming
language for the fast computation of the Baker-Campbell-Hausdorff (BCH) and
similar Lie series. The Lie series can be represented in the Lyndon basis, in
the classical Hall basis, or in the right-normed basis of E.S. Chibrikov. In
the Lyndon basis, which proves to be particularly efficient for this purpose,
the computation of 111013 coefficients for the BCH series up to terms of degree
20 takes less than half a second on an ordinary personal computer and requires
negligible 11MB of memory. Up to terms of degree 30, which is the maximum
degree the program can handle, the computation of 74248451 coefficients takes
55 hours but still requires only a modest 5.5GB of memory."
http://arxiv.org/abs/2102.06520,Title: Estimation and Applications of Quantiles in Deep Binary Classification,"Quantile regression, based on check loss, is a widely used inferential
paradigm in Econometrics and Statistics. The conditional quantiles provide a
robust alternative to classical conditional means, and also allow uncertainty
quantification of the predictions, while making very few distributional
assumptions. We consider the analogue of check loss in the binary
classification setting. We assume that the conditional quantiles are smooth
functions that can be learnt by Deep Neural Networks (DNNs). Subsequently, we
compute the Lipschitz constant of the proposed loss, and also show that its
curvature is bounded, under some regularity conditions. Consequently, recent
results on the error rates and DNN architecture complexity become directly
applicable.
We quantify the uncertainty of the class probabilities in terms of prediction
intervals, and develop individualized confidence scores that can be used to
decide whether a prediction is reliable or not at scoring time. By aggregating
the confidence scores at the dataset level, we provide two additional metrics,
model confidence, and retention rate, to complement the widely used classifier
summaries. We also the robustness of the proposed non-parametric binary
quantile classification framework are also studied, and we demonstrate how to
obtain several univariate summary statistics of the conditional distributions,
in particular conditional means, using smoothed conditional quantiles, allowing
the use of explanation techniques like Shapley to explain the mean predictions.
Finally, we demonstrate an efficient training regime for this loss based on
Stochastic Gradient Descent with Lipschitz Adaptive Learning Rates (LALR)."
http://arxiv.org/abs/2102.06521,Title: Improving Zero-shot Neural Machine Translation on Language-specific  Encoders-Decoders,"Recently, universal neural machine translation (NMT) with shared
encoder-decoder gained good performance on zero-shot translation. Unlike
universal NMT, jointly trained language-specific encoders-decoders aim to
achieve universal representation across non-shared modules, each of which is
for a language or language family. The non-shared architecture has the
advantage of mitigating internal language competition, especially when the
shared vocabulary and model parameters are restricted in their size. However,
the performance of using multiple encoders and decoders on zero-shot
translation still lags behind universal NMT. In this work, we study zero-shot
translation using language-specific encoders-decoders. We propose to generalize
the non-shared architecture and universal NMT by differentiating the
Transformer layers between language-specific and interlingua. By selectively
sharing parameters and applying cross-attentions, we explore maximizing the
representation universality and realizing the best alignment of
language-agnostic information. We also introduce a denoising auto-encoding
(DAE) objective to jointly train the model with the translation task in a
multi-task manner. Experiments on two public multilingual parallel datasets
show that our proposed model achieves a competitive or better results than
universal NMT and strong pivot baseline. Moreover, we experiment incrementally
adding new language to the trained model by only updating the new model
parameters. With this little effort, the zero-shot translation between this
newly added language and existing languages achieves a comparable result with
the model trained jointly from scratch on all languages."
http://arxiv.org/abs/2102.06522,Title: Fuzzing Symbolic Expressions,"Recent years have witnessed a wide array of results in software testing,
exploring different approaches and methodologies ranging from fuzzers to
symbolic engines, with a full spectrum of instances in between such as concolic
execution and hybrid fuzzing. A key ingredient of many of these tools is
Satisfiability Modulo Theories (SMT) solvers, which are used to reason over
symbolic expressions collected during the analysis. In this paper, we
investigate whether techniques borrowed from the fuzzing domain can be applied
to check whether symbolic formulas are satisfiable in the context of concolic
and hybrid fuzzing engines, providing a viable alternative to classic SMT
solving techniques. We devise a new approximate solver, FUZZY-SAT, and show
that it is both competitive with and complementary to state-of-the-art solvers
such as Z3 with respect to handling queries generated by hybrid fuzzers."
http://arxiv.org/abs/2102.06525,Title: Reviving Iterative Training with Mask Guidance for Interactive  Segmentation,"Recent works on click-based interactive segmentation have demonstrated
state-of-the-art results by using various inference-time optimization schemes.
These methods are considerably more computationally expensive compared to
feedforward approaches, as they require performing backward passes through a
network during inference and are hard to deploy on mobile frameworks that
usually support only forward passes. In this paper, we extensively evaluate
various design choices for interactive segmentation and discover that new
state-of-the-art results can be obtained without any additional optimization
schemes. Thus, we propose a simple feedforward model for click-based
interactive segmentation that employs the segmentation masks from previous
steps. It allows not only to segment an entirely new object, but also to start
with an external mask and correct it. When analyzing the performance of models
trained on different datasets, we observe that the choice of a training dataset
greatly impacts the quality of interactive segmentation. We find that the
models trained on a combination of COCO and LVIS with diverse and high-quality
annotations show performance superior to all existing models. The code and
trained models are available at
https://github.com/saic-vul/ritm_interactive_segmentation."
http://arxiv.org/abs/2102.06526,Title: On the Application of BAC-NOMA to 6G umMTC,"This letter studies the application of backscatter communications (BackCom)
assisted non-orthogonal multiple access (BAC-NOMA) to the envisioned
sixth-generation (6G) ultra-massive machine type communications (umMTC). In
particular, the proposed BAC-NOMA transmission scheme can realize simultaneous
energy and spectrum cooperation between uplink and downlink users, which is
important to support massive connectivity and stringent energy constraints in
umMTC. Furthermore, a resource allocation problem for maximizing the uplink
throughput and suppressing the interference between downlink and uplink
transmission is formulated as an optimization problem and the corresponding
optimal resource allocation policy is obtained. Computer simulations are
provided to demonstrate the superior performance of BAC-NOMA."
http://arxiv.org/abs/2102.06527,Title: A Computability Perspective on (Verified) Machine Learning,"There is a strong consensus that combining the versatility of machine
learning with the assurances given by formal verification is highly desirable.
It is much less clear what verified machine learning should mean exactly. We
consider this question from the (unexpected?) perspective of computable
analysis. This allows us to define the computational tasks underlying verified
ML in a model-agnostic way, and show that they are in principle computable."
http://arxiv.org/abs/2102.06528,Title: Disturbing Reinforcement Learning Agents with Corrupted Rewards,"Reinforcement Learning (RL) algorithms have led to recent successes in
solving complex games, such as Atari or Starcraft, and to a huge impact in
real-world applications, such as cybersecurity or autonomous driving. In the
side of the drawbacks, recent works have shown how the performance of RL
algorithms decreases under the influence of soft changes in the reward
function. However, little work has been done about how sensitive these
disturbances are depending on the aggressiveness of the attack and the learning
exploration strategy. In this paper, we propose to fill this gap in the
literature analyzing the effects of different attack strategies based on reward
perturbations, and studying the effect in the learner depending on its
exploration strategy. In order to explain all the behaviors, we choose a
sub-class of MDPs: episodic, stochastic goal-only-rewards MDPs, and in
particular, an intelligible grid domain as a benchmark. In this domain, we
demonstrate that smoothly crafting adversarial rewards are able to mislead the
learner, and that using low exploration probability values, the policy learned
is more robust to corrupt rewards. Finally, in the proposed learning scenario,
a counterintuitive result arises: attacking at each learning episode is the
lowest cost attack strategy."
http://arxiv.org/abs/2102.06529,Title: A taxonomy for quality in simulation-based development and testing of  automated driving systems,"Ensuring the safety and performance requirements of automated driving systems
is a major challenge for the automotive industry. One way to tackle this
problem is a simulation-based approach. However, to be able to rely on results
generated by this method, the simulation process needs to fulfill certain
quality criteria depending on the intended usage. Hence, quality should be
measured and determined at many different levels and areas of the testing and
developing landscape, providing information with varying degrees of
abstraction. Additionally, quality not only has to be assessed for a complete
automated driving system but also for the simulation models that approximate
the vehicles' components before they can be used for simulation. This taxonomy
provides a better understanding of the concept of quality during the
development and test process and introduces the possibility to systematically
evaluate whether development steps in this process need to be repeated or
further assessed."
http://arxiv.org/abs/2102.06532,Title: PAC-BUS: Meta-Learning Bounds via PAC-Bayes and Uniform Stability,"We are motivated by the problem of providing strong generalization guarantees
in the context of meta-learning. Existing generalization bounds are either
challenging to evaluate or provide vacuous guarantees in even relatively simple
settings. We derive a probably approximately correct (PAC) bound for
gradient-based meta-learning using two different generalization frameworks in
order to deal with the qualitatively different challenges of generalization at
the ""base"" and ""meta"" levels. We employ bounds for uniformly stable algorithms
at the base level and bounds from the PAC-Bayes framework at the meta level.
The result is a PAC-bound that is tighter when the base learner adapts quickly,
which is precisely the goal of meta-learning. We show that our bound provides a
tighter guarantee than other bounds on a toy non-convex problem on the unit
sphere and a text-based classification example. We also present a practical
regularization scheme motivated by the bound in settings where the bound is
loose and demonstrate improved performance over baseline techniques."
http://arxiv.org/abs/2102.06535,Title: VSync: Push-Button Verification and Optimization for Synchronization  Primitives on Weak Memory Models (Technical Report),"This technical report contains material accompanying our work with same title
published at ASPLOS'21. We start in Sec. 1 with a detailed presentation of the
core innovation of this work, Await Model Checking (AMC). The correctness
proofs of AMC can be found in Sec. 2. Next, we discuss three study cases in
Sec. 3, presenting bugs found and challenges encountered when applying VSync to
existing code bases. Finally, in Sec. 4 we describe the setup details of our
evaluation and report further experimental results."
http://arxiv.org/abs/2102.06536,Title: Outdoor inverse rendering from a single image using multiview  self-supervision,"In this paper we show how to perform scene-level inverse rendering to recover
shape, reflectance and lighting from a single, uncontrolled image using a fully
convolutional neural network. The network takes an RGB image as input,
regresses albedo, shadow and normal maps from which we infer least squares
optimal spherical harmonic lighting coefficients. Our network is trained using
large uncontrolled multiview and timelapse image collections without ground
truth. By incorporating a differentiable renderer, our network can learn from
self-supervision. Since the problem is ill-posed we introduce additional
supervision. Our key insight is to perform offline multiview stereo (MVS) on
images containing rich illumination variation. From the MVS pose and depth
maps, we can cross project between overlapping views such that Siamese training
can be used to ensure consistent estimation of photometric invariants. MVS
depth also provides direct coarse supervision for normal map estimation. We
believe this is the first attempt to use MVS supervision for learning inverse
rendering. In addition, we learn a statistical natural illumination prior. We
evaluate performance on inverse rendering, normal map estimation and intrinsic
image decomposition benchmarks."
http://arxiv.org/abs/2102.06538,Title: Neural Architecture Search as Program Transformation Exploration,"Improving the performance of deep neural networks (DNNs) is important to both
the compiler and neural architecture search (NAS) communities. Compilers apply
program transformations in order to exploit hardware parallelism and memory
hierarchy. However, legality concerns mean they fail to exploit the natural
robustness of neural networks. In contrast, NAS techniques mutate networks by
operations such as the grouping or bottlenecking of convolutions, exploiting
the resilience of DNNs. In this work, we express such neural architecture
operations as program transformations whose legality depends on a notion of
representational capacity. This allows them to be combined with existing
transformations into a unified optimization framework. This unification allows
us to express existing NAS operations as combinations of simpler
transformations. Crucially, it allows us to generate and explore new tensor
convolutions. We prototyped the combined framework in TVM and were able to find
optimizations across different DNNs, that significantly reduce inference time -
over 3$\times$ in the majority of cases.
Furthermore, our scheme dramatically reduces NAS search time. Code is
available
at~\href{https://github.com/jack-willturner/nas-as-program-transformation-exploration}{this
https url}."
http://arxiv.org/abs/2102.06539,Title: 3D-1D coupling on non conforming meshes via three-field optimization  based domain decomposition,"A new numerical approach is proposed for the simulation of coupled
three-dimensional and one-dimensional elliptic equations (3D-1D coupling)
arising from dimensionality reduction of 3D-3D problems with thin inclusions.
The method is based on a well posed mathematical formulation and results in a
numerical scheme with high robustness and flexibility in handling geometrical
complexities. This is achieved by means of a three-field approach to split the
1D problems from the bulk 3D problem, and then resorting to the minimization of
a properly designed functional to impose matching conditions at the interfaces.
Thanks to the structure of the functional, the method allows the use of
independent meshes for the various subdomains."
http://arxiv.org/abs/2102.06540,Title: Modeling Dynamic User Interests: A Neural Matrix Factorization Approach,"In recent years, there has been significant interest in understanding users'
online content consumption patterns. But, the unstructured, high-dimensional,
and dynamic nature of such data makes extracting valuable insights challenging.
Here we propose a model that combines the simplicity of matrix factorization
with the flexibility of neural networks to efficiently extract nonlinear
patterns from massive text data collections relevant to consumers' online
consumption patterns. Our model decomposes a user's content consumption journey
into nonlinear user and content factors that are used to model their dynamic
interests. This natural decomposition allows us to summarize each user's
content consumption journey with a dynamic probabilistic weighting over a set
of underlying content attributes. The model is fast to estimate, easy to
interpret and can harness external data sources as an empirical prior. These
advantages make our method well suited to the challenges posed by modern
datasets. We use our model to understand the dynamic news consumption interests
of Boston Globe readers over five years. Thorough qualitative studies,
including a crowdsourced evaluation, highlight our model's ability to
accurately identify nuanced and coherent consumption patterns. These results
are supported by our model's superior and robust predictive performance over
several competitive baseline methods."
http://arxiv.org/abs/2102.06543,Title: Semantically-Conditioned Negative Samples for Efficient Contrastive  Learning,"Negative sampling is a limiting factor w.r.t. the generalization of
metric-learned neural networks. We show that uniform negative sampling provides
little information about the class boundaries and thus propose three novel
techniques for efficient negative sampling: drawing negative samples from (1)
the top-$k$ most semantically similar classes, (2) the top-$k$ most
semantically similar samples and (3) interpolating between contrastive latent
representations to create pseudo negatives. Our experiments on CIFAR-10,
CIFAR-100 and Tiny-ImageNet-200 show that our proposed \textit{Semantically
Conditioned Negative Sampling} and Latent Mixup lead to consistent performance
improvements. In the standard supervised learning setting, on average we
increase test accuracy by 1.52\% percentage points on CIFAR-10 across various
network architectures. In the knowledge distillation setting, (1) the
performance of student networks increase by 4.56\% percentage points on
Tiny-ImageNet-200 and 3.29\% on CIFAR-100 over student networks trained with no
teacher and (2) 1.23\% and 1.72\% respectively over a \textit{hard-to-beat}
baseline (Hinton et al., 2015)."
http://arxiv.org/abs/2102.06548,Title: Cockpit: A Practical Debugging Tool for Training Deep Neural Networks,"When engineers train deep learning models, they are very much ""flying blind"".
Commonly used approaches for real-time training diagnostics, such as monitoring
the train/test loss, are limited. Assessing a network's training process solely
through these performance indicators is akin to debugging software without
access to internal states through a debugger. To address this, we present
Cockpit, a collection of instruments that enable a closer look into the inner
workings of a learning machine, and a more informative and meaningful status
report for practitioners. It facilitates the identification of learning phases
and failure modes, like ill-chosen hyperparameters. These instruments leverage
novel higher-order information about the gradient distribution and curvature,
which has only recently become efficiently accessible. We believe that such a
debugging tool, which we open-source for PyTorch, represents an important step
to improve troubleshooting the training process, reveal new insights, and help
develop novel methods and heuristics."
http://arxiv.org/abs/2102.06551,Title: Unleashing the Power of Contrastive Self-Supervised Visual Models via  Contrast-Regularized Fine-Tuning,"Contrastive self-supervised learning (CSL) leverages unlabeled data to train
models that provide instance-discriminative visual representations uniformly
scattered in the feature space. In deployment, the common practice is to
directly fine-tune models with the cross-entropy loss, which however may not be
an optimal strategy. Although cross-entropy tends to separate inter-class
features, the resulted models still have limited capability of reducing
intra-class feature scattering that inherits from pre-training, and thus may
suffer unsatisfactory performance on downstream tasks. In this paper, we
investigate whether applying contrastive learning to fine-tuning would bring
further benefits, and analytically find that optimizing the supervised
contrastive loss benefits both class-discriminative representation learning and
model optimization during fine-tuning. Inspired by these findings, we propose
Contrast-regularized tuning (Core-tuning), a novel approach for fine-tuning
contrastive self-supervised visual models. Instead of simply adding the
contrastive loss to the objective of fine-tuning, Core-tuning also generates
hard sample pairs for more effective contrastive learning through a novel
feature mixup strategy, as well as improves the generalizability of the model
by smoothing the decision boundary via mixed samples. Extensive experiments on
image classification and semantic segmentation verify the effectiveness of
Core-tuning."
http://arxiv.org/abs/2102.06553,Title: Intelligent Software Web Agents: A Gap Analysis,"Semantic web technologies have shown their effectiveness, especially when it
comes to knowledge representation, reasoning, and data integrations. However,
the original semantic web vision, whereby machine readable web data could be
automatically actioned upon by intelligent software web agents, has yet to be
realised. In order to better understand the existing technological challenges
and opportunities, in this paper we examine the status quo in terms of
intelligent software web agents, guided by research with respect to
requirements and architectural components, coming from that agents community.
We start by collating and summarising requirements and core architectural
components relating to intelligent software agent. Following on from this, we
use the identified requirements to both further elaborate on the semantic web
agent motivating use case scenario, and to summarise different perspectives on
the requirements when it comes to semantic web agent literature. Finally, we
propose a hybrid semantic web agent architecture, discuss the role played by
existing semantic web standards, and point to existing work in the broader
semantic web community any beyond that could help us to make the semantic web
agent vision a reality."
http://arxiv.org/abs/2102.06554,Title: Improved LP-based Approximation Algorithms for Facility Location with  Hard Capacities,"We present LP-based approximation algorithms for the capacitated facility
location problem (CFL), a long-standing problem with intriguing unsettled
complexity and literature dated back to the 90s. We present an elegant
iterative rounding scheme for the MFN relaxation that yields an approximation
guarantee of $\left(10+\sqrt{67}\right)/2 \approx 9.0927$, a significant
improvement upon the previous LP-based ratio due to An et al in~2014. For CFL
with cardinality facility cost (CFL-CFC), we present an LP-based
$4$-approximation algorithm, which surpasses the long-standing ratio of~$5$ due
to Levi et al that ages up for decades since 2004. Our result considerably
deepens the current understanding for the CFL problem and indicates that an
LP-based ratio strictly better than $5$ in polynomial time for the general
problem may still be possible to pursue."
http://arxiv.org/abs/2102.06555,Title: TerraWatt: Sustaining Sustainable Computing of Containers in Containers,"Each day the world inches closer to a climate catastrophe and a
sustainability revolution. To avoid the former and achieve the latter we must
transform our use of energy. Surprisingly, today's growing problem is that
there is too much wind and solar power generation at the wrong times and in the
wrong places.
We argue for the construction of TerraWatt: a geographically-distributed,
large-scale, zero-carbon compute infrastructure using renewable energy and
older hardware. Delivering zero-carbon compute for general cloud workloads is
challenging due to spatiotemporal power variability. We describe the systems
challenges in using intermittent renewable power at scale to fuel such an
older, decentralized compute infrastructure."
http://arxiv.org/abs/2102.06557,Title: Multi-access Coded Caching Scheme with Linear Sub-packetization using  PDAs,"We consider multi-access coded caching problem introduced by Hachem et.al.,
where each user has access to $L$ neighboring caches in a cyclic wrap-around
fashion. We focus on the deterministic schemes for a specific class of
multi-access coded caching problem based on the concept of PDA. We construct
new PDAs which specify the delivery scheme for the specific class of
multi-access coded caching problem discussed in this paper. For the proposed
scheme, the coding gain is larger than that of the state-of-the-art while the
sub-packetization level varies only linearly with the number of users. Hence,
we achieve a lower transmission rate with the least sub-packetization level
compared to the existing schemes."
http://arxiv.org/abs/2102.06558,Title: Optimizing Inference Performance of Transformers on CPUs,"The Transformer architecture revolutionized the field of natural language
processing (NLP). Transformers-based models (e.g., BERT) power many important
Web services, such as search, translation, question-answering, etc. While
enormous research attention is paid to the training of those models, relatively
little efforts are made to improve their inference performance. This paper
comes to address this gap by presenting an empirical analysis of scalability
and performance of inferencing a Transformer-based model on CPUs. Focusing on
the highly popular BERT model, we identify key components of the Transformer
architecture where the bulk of the computation happens, and propose three
optimizations to speed them up. The optimizations are evaluated using the
inference benchmark from HuggingFace, and are shown to achieve the speedup of
up to x2.36. The considered optimizations do not require any changes to the
implementation of the models nor affect their accuracy."
http://arxiv.org/abs/2102.06559,Title: MetaGrad: Adaptation using Multiple Learning Rates in Online Learning,"We provide a new adaptive method for online convex optimization, MetaGrad,
that is robust to general convex losses but achieves faster rates for a broad
class of special functions, including exp-concave and strongly convex
functions, but also various types of stochastic and non-stochastic functions
without any curvature. We prove this by drawing a connection to the Bernstein
condition, which is known to imply fast rates in offline statistical learning.
MetaGrad further adapts automatically to the size of the gradients. Its main
feature is that it simultaneously considers multiple learning rates, which are
weighted directly proportional to their empirical performance on the data using
a new meta-algorithm. We provide three versions of MetaGrad. The full matrix
version maintains a full covariance matrix and is applicable to learning tasks
for which we can afford update time quadratic in the dimension. The other two
versions provide speed-ups for high-dimensional learning tasks with an update
time that is linear in the dimension: one is based on sketching, the other on
running a separate copy of the basic algorithm per coordinate. We evaluate all
versions of MetaGrad on benchmark online classification and regression tasks,
on which they consistently outperform both online gradient descent and AdaGrad."
http://arxiv.org/abs/2102.06560,Title: Do-calculus enables causal reasoning with latent variable models,"Latent variable models (LVMs) are probabilistic models where some of the
variables are hidden during training. A broad class of LVMshave a directed
acyclic graphical structure. The directed structure suggests an intuitive
causal explanation of the data generating process. For example, a latent topic
model suggests that topics cause the occurrence of a token. Despite this
intuitive causal interpretation, a directed acyclic latent variable model
trained on data is generally insufficient for causal reasoning, as the required
model parameters may not be uniquely identified. In this manuscript we
demonstrate that an LVM can answer any causal query posed post-training,
provided that the query can be identified from the observed variables according
to the do-calculus rules. We show that causal reasoning can enhance a broad
class of LVM long established in the probabilistic modeling community, and
demonstrate its effectiveness on several case studies. These include a machine
learning model with multiple causes where there exists a set of latent
confounders and a mediator between the causes and the outcome variable, a study
where the identifiable causal query cannot be estimated using the front-door or
back-door criterion, a case study that captures unobserved crosstalk between
two biological signaling pathways, and a COVID-19 expert system that identifies
multiple causal queries."
http://arxiv.org/abs/2102.06563,Title: Deep Reinforcement Learning for Backup Strategies against Adversaries,"Many defensive measures in cyber security are still dominated by heuristics,
catalogs of standard procedures, and best practices. Considering the case of
data backup strategies, we aim towards mathematically modeling the underlying
threat models and decision problems. By formulating backup strategies in the
language of stochastic processes, we can translate the challenge of finding
optimal defenses into a reinforcement learning problem. This enables us to
train autonomous agents that learn to optimally support planning of defense
processes. In particular, we tackle the problem of finding an optimal backup
scheme in the following adversarial setting: Given $k$ backup devices, the goal
is to defend against an attacker who can infect data at one time but chooses to
destroy or encrypt it at a later time, potentially also corrupting multiple
backups made in between. In this setting, the usual round-robin scheme, which
always replaces the oldest backup, is no longer optimal with respect to
avoidable exposure. Thus, to find a defense strategy, we model the problem as a
hybrid discrete-continuous action space Markov decision process and
subsequently solve it using deep deterministic policy gradients. We show that
the proposed algorithm can find storage device update schemes which match or
exceed existing schemes with respect to various exposure metrics."
http://arxiv.org/abs/2102.06564,"Title: Discrete-Time Consensus Networks: Scalability, Grounding and  Countermeasures","We investigate the disruption of discrete-time consensus problems via
grounding. Loosely speaking, grounding a network occurs if the state of one
agent no longer responds to inputs from other agents and/or changes its
dynamics. Then, the agent becomes a leader or a so-called stubborn agent. The
disruption of the agent can be caused by internal faults, safety protocols or
due to an external malicious attack. In this paper we investigate how grounding
affects expander graph families that usually exhibit good scaling properties
with increasing network size. It is shown that the algebraic connectivity and
eigenratio of the network decrease due to the grounding causing the performance
and scalability of the network to deteriorate, even to the point of losing
consensusability. We then present possible countermeasures to such disruptions
and discuss their practicality and limitations. In particular, for a specific
countermeasure of deliberately grounding additional nodes, we investigate
extensively how to select additional nodes to ground and how many nodes we need
to ground to recover the consensus performance. Our findings are supported by a
wide range of numerical simulations."
http://arxiv.org/abs/2102.06565,Title: An Overview of Recommender Systems and Machine Learning in Feature  Modeling and Configuration,"Recommender systems support decisions in various domains ranging from simple
items such as books and movies to more complex items such as financial
services, telecommunication equipment, and software systems. In this context,
recommendations are determined, for example, on the basis of analyzing the
preferences of similar users. In contrast to simple items which can be
enumerated in an item catalog, complex items have to be represented on the
basis of variability models (e.g., feature models) since a complete enumeration
of all possible configurations is infeasible and would trigger significant
performance issues. In this paper, we give an overview of a potential new line
of research which is related to the application of recommender systems and
machine learning techniques in feature modeling and configuration. In this
context, we give examples of the application of recommender systems and machine
learning and discuss future research issues."
http://arxiv.org/abs/2102.06570,Title: ReLU Neural Networks for Exact Maximum Flow Computation,"Understanding the great empirical success of artificial neural networks (NNs)
from a theoretical point of view is currently one of the hottest research
topics in computer science. In this paper we study the expressive power of NNs
with rectified linear units from a combinatorial optimization perspective. In
particular, we show that, given a directed graph with $n$ nodes and $m$ arcs,
there exists an NN of polynomial size that computes a maximum flow from any
possible real-valued arc capacities as input. To prove this, we develop the
pseudo-code language Max-Affine Arithmetic Programs (MAAPs) and show
equivalence between MAAPs and NNs concerning natural complexity measures. We
then design a MAAP to exactly solve the Maximum Flow Problem, which translates
to an NN of size $\mathcal{O}(m^2 n^2)$."
http://arxiv.org/abs/2102.06571,Title: UAVs Path Deviation Attacks: Survey and Research Challenges,"Recently, Unmanned Aerial Vehicles (UAVs) are employed for a plethora of
civilian applications. Such flying vehicles can accomplish tasks under the
pilot's eyesight within the range of a remote controller, or autonomously
according to a certain pre-loaded path configuration. Different path deviation
attacks can be performed by malicious users against UAVs. We classify such
attacks and the relative defenses based on the UAV's flight mode, i.e., (i)
First Person View (FPV), (ii) civilian Global Navigation Satellite System based
(GNSS), and (iii) GNSS ""plus"" auxiliary technologies (GNSS+), and on the
multiplicity, i.e., (i) Single UAV, and (ii) Multiple UAVs. We found that very
little has been done to secure the FPV flight mode against path deviation. In
GNSS mode, spoofing is the most worrisome attack. The best defense against
spoofing seems to be redundancy, such as adding vision chips to single UAV or
using multiple arranged UAVs. No specific attacks and defenses have been found
in literature for GNSS+ or for UAVs moving in group without a pre-ordered
arrangement. These aspects require further investigation."
http://arxiv.org/abs/2102.06573,Title: Bayesian Quadrature on Riemannian Data Manifolds,"Riemannian manifolds provide a principled way to model nonlinear geometric
structure inherent in data. A Riemannian metric on said manifolds determines
geometry-aware shortest paths and provides the means to define statistical
models accordingly. However, these operations are typically computationally
demanding. To ease this computational burden, we advocate probabilistic
numerical methods for Riemannian statistics. In particular, we focus on
Bayesian quadrature (BQ) to numerically compute integrals over normal laws on
Riemannian manifolds learned from data. In this task, each function evaluation
relies on the solution of an expensive initial value problem. We show that by
leveraging both prior knowledge and an active exploration scheme, BQ
significantly reduces the number of required evaluations and thus outperforms
Monte Carlo methods on a wide range of integration problems. As a concrete
application, we highlight the merits of adopting Riemannian geometry with our
proposed framework on a nonlinear dataset from molecular dynamics."
http://arxiv.org/abs/2102.06575,Title: A Critical Look At The Identifiability of Causal Effects with Deep  Latent Variable Models,"Using deep latent variable models in causal inference has attracted
considerable interest recently, but an essential open question is their
identifiability. While they have yielded promising results and theory exists on
the identifiability of some simple model formulations, we also know that causal
effects cannot be identified in general with latent variables. We investigate
this gap between theory and empirical results with theoretical considerations
and extensive experiments under multiple synthetic and real-world data sets,
using the causal effect variational autoencoder (CEVAE) as a case study. While
CEVAE seems to work reliably under some simple scenarios, it does not identify
the correct causal effect with a misspecified latent variable or a complex data
distribution, as opposed to the original goals of the model. Our results show
that the question of identifiability cannot be disregarded, and we argue that
more attention should be paid to it in future work."
http://arxiv.org/abs/2102.06578,Title: Robust White Matter Hyperintensity Segmentation on Unseen Domain,"Typical machine learning frameworks heavily rely on an underlying assumption
that training and test data follow the same distribution. In medical imaging
which increasingly begun acquiring datasets from multiple sites or scanners,
this identical distribution assumption often fails to hold due to systematic
variability induced by site or scanner dependent factors. Therefore, we cannot
simply expect a model trained on a given dataset to consistently work well, or
generalize, on a dataset from another distribution. In this work, we address
this problem, investigating the application of machine learning models to
unseen medical imaging data. Specifically, we consider the challenging case of
Domain Generalization (DG) where we train a model without any knowledge about
the testing distribution. That is, we train on samples from a set of
distributions (sources) and test on samples from a new, unseen distribution
(target). We focus on the task of white matter hyperintensity (WMH) prediction
using the multi-site WMH Segmentation Challenge dataset and our local in-house
dataset. We identify how two mechanically distinct DG approaches, namely domain
adversarial learning and mix-up, have theoretical synergy. Then, we show
drastic improvements of WMH prediction on an unseen target domain."
http://arxiv.org/abs/2102.06580,Title: Barriers for recent methods in geodesic optimization,"We study a class of optimization problems including matrix scaling, matrix
balancing, multidimensional array scaling, operator scaling, and tensor scaling
that arise frequently in theory and in practice. Some of these problems, such
as matrix and array scaling, are convex in the Euclidean sense, but others such
as operator scaling and tensor scaling are \emph{geodesically convex} on a
different Riemannian manifold. Trust region methods, which include
box-constrained Newton's method, are known to produce high precision solutions
very quickly for matrix scaling and matrix balancing (Cohen et. al., FOCS 2017,
Allen-Zhu et. al. FOCS 2017), and result in polynomial time algorithms for some
geodesically convex problems like operator scaling (Garg et. al. STOC 2018,
B\""urgisser et. al. FOCS 2019). One is led to ask whether these guarantees also
hold for multidimensional array scaling and tensor scaling.
We show that this is not the case by exhibiting instances with exponential
\emph{diameter bound}: we construct polynomial-size instances of 3-dimensional
array scaling and 3-tensor scaling whose approximate solutions all have doubly
exponential condition number. Moreover, we study convex-geometric notions of
complexity known as margin and gap, which are used to bound the running times
of all existing optimization algorithms for such problems. We show that margin
and gap are exponentially small for several problems including array scaling,
tensor scaling and polynomial scaling. Our results suggest that it is
impossible to prove polynomial running time bounds for tensor scaling based on
diameter bounds alone. Therefore, our work motivates the search for analogues
of more sophisticated algorithms, such as interior point methods, for
geodesically convex optimization that do not rely on polynomial diameter
bounds."
http://arxiv.org/abs/2102.06583,Title: Responsibility and verification: Importance value in temporal logics,"We aim at measuring the influence of the nondeterministic choices of a part
of a system on its ability to satisfy a specification. For this purpose, we
apply the concept of Shapley values to verification as a means to evaluate how
important a part of a system is. The importance of a component is measured by
giving its control to an adversary, alone or along with other components, and
testing whether the system can still fulfill the specification. We study this
idea in the framework of model-checking with various classical types of
linear-time specification, and propose several ways to transpose it to
branching ones. We also provide tight complexity bounds in almost every case."
http://arxiv.org/abs/2102.06584,Title: Leveraging Artificial Intelligence to Analyze the COVID-19 Distribution  Pattern based on Socio-economic Determinants,"The spatialization of socioeconomic data can be used and integrated with
other sources of information to reveal valuable insights. Such data can be
utilized to infer different variations, such as the dynamics of city dwellers
and their spatial and temporal variability. This work focuses on such
applications to explore the underlying association between socioeconomic
characteristics of different geographical regions in Dublin, Ireland, and the
number of confirmed COVID cases in each area. Our aim is to implement a machine
learning approach to identify demographic characteristics and spatial patterns.
Spatial analysis was used to describe the pattern of interest in Electoral
Divisions (ED), which are the legally defined administrative areas in the
Republic of Ireland for which population statistics are published from the
census data. We used the most informative variables of the census data to model
the number of infected people in different regions at ED level."
http://arxiv.org/abs/2102.06585,Title: End-to-end Audio-visual Speech Recognition with Conformers,"In this work, we present a hybrid CTC/Attention model based on a ResNet-18
and Convolution-augmented transformer (Conformer), that can be trained in an
end-to-end manner. In particular, the audio and visual encoders learn to
extract features directly from raw pixels and audio waveforms, respectively,
which are then fed to conformers and then fusion takes place via a Multi-Layer
Perceptron (MLP). The model learns to recognise characters using a combination
of CTC and an attention mechanism. We show that end-to-end training, instead of
using pre-computed visual features which is common in the literature, the use
of a conformer, instead of a recurrent network, and the use of a
transformer-based language model, significantly improve the performance of our
model. We present results on the largest publicly available datasets for
sentence-level speech recognition, Lip Reading Sentences 2 (LRS2) and Lip
Reading Sentences 3 (LRS3), respectively. The results show that our proposed
models raise the state-of-the-art performance by a large margin in audio-only,
visual-only, and audio-visual experiments."
http://arxiv.org/abs/2102.06587,Title: Leveraging Artificial Intelligence to Analyze Citizens' Opinions on  Urban Green Space,"Continued population growth and urbanization is shifting research to consider
the quality of urban green space over the quantity of these parks, woods, and
wetlands. The quality of urban green space has been hitherto measured by expert
assessments, including in-situ observations, surveys, and remote sensing
analyses. Location data platforms, such as TripAdvisor, can provide people's
opinion on many destinations and experiences, including UGS. This paper
leverages Artificial Intelligence techniques for opinion mining and text
classification using such platform's reviews as a novel approach to urban green
space quality assessments. Natural Language Processing is used to analyze
contextual information given supervised scores of words by implementing
computational analysis. Such an application can support local authorities and
stakeholders in their understanding of and justification for future investments
in urban green space."
http://arxiv.org/abs/2102.06588,Title: Numerical investigation of Mach number consistent Roe solvers for the  Euler equations of gas dynamics,"While traditional approaches to prevent the carbuncle phenomenon in gas
dynamics simulations increase the viscosity on entropy and shear waves near
shocks, it was quite recently suggested to instead decrease the viscosity on
the acoustic waves for low Mach numbers. The goal is to achieve what, in this
paper, we call Mach number consistency: for all waves, the numerical viscosity
decreases with the same order of the Mach number when the Mach number tends to
zero. We take the simple approach that was used for the proof of concept
together with the simple model for the increased numerical viscosity on linear
waves and investigate the possibilities of combining both in an adaptive manner
while locally maintaining Mach number consistency."
http://arxiv.org/abs/2102.06589,Title: Qualifying Software Engineers Undergraduates in DevOps -- Challenges of  Introducing Technical and Non-technical Concepts in a Project-oriented Course,"The constant changes in the software industry, practices, and methodologies
impose challenges to teaching and learning current software engineering
concepts and skills. DevOps is particularly challenging because it covers
technical concepts, such as pipeline automation, and non-technical ones, such
as team roles and project management. The present study investigates a course
setup to introduce these concepts to software engineering undergraduates. We
designed the course by employing coding to associate DevOps concepts to Agile,
Lean, and Open source practices and tools. We present the main aspects of this
project-oriented DevOps course, with 240 students enrolled in it since its
first offering in 2016. We conducted an empirical study, with both a
quantitative and qualitative analysis, to evaluate this project-oriented course
setup. We collected the data from the projects repository and students
perceptions from a questionnaire. We mined 148 repositories (corresponding to
72 projects) and obtained 86 valid responses to the questionnaire. We also
mapped the concepts which are more challenging to students learn from
experience. The results evidence that first-hand experience facilitates the
comprehension of DevOps concepts and enriches classes discussions. We present a
set of lessons learned, which may help professors better design and conduct
project-oriented courses to cover DevOps concepts."
http://arxiv.org/abs/2102.06590,"Title: What helped, and what did not? An Evaluation of the Strategies to  Improve Continuous Integration","Continuous integration (CI) is a widely used practice in modern software
engineering. Unfortunately, it is also an expensive practice - Google and
Mozilla estimate their CI systems in millions of dollars. There are a number of
techniques and tools designed to or having the potential to save the cost of CI
or expand its benefit - reducing time to feedback. However, their benefits in
some dimensions may also result in drawbacks in others. They may also be
beneficial in other scenarios where they are not designed to help. In this
paper, we perform the first exhaustive comparison of techniques to improve CI,
evaluating 14 variants of 10 techniques using selection and prioritization
strategies on build and test granularity. We evaluate their strengths and
weaknesses with 10 different cost and time-tofeedback saving metrics on 100
real-world projects. We analyze the results of all techniques to understand the
design decisions that helped different dimensions of benefit. We also
synthesized those results to lay out a series of recommendations for the
development of future research techniques to advance this area."
http://arxiv.org/abs/2102.06591,Title: Online voluntary mentoring: Optimising the assignment of students and  mentors,"After the closure of the schools in Hungary from March 2020 due to the
pandemic, many students were left at home with no or not enough parental help
for studying, and in the meantime some people had more free time and
willingness to help others in need during the lockdown. In this paper we
describe the optimisation aspects of a joint NGO project for allocating
voluntary mentors to students using a web-based coordination mechanism. The
goal of the project has been to form optimal pairs and study groups by taking
into the preferences and the constraints of the participants. In this paper we
present the optimisation concept, and the integer programming techniques used
for solving the allocation problems. Furthermore, we conducted computation
simulations on real and generated data for evaluate the performance of this
dynamic matching scheme under different parameter settings."
http://arxiv.org/abs/2102.06593,Title: Proof complexity of positive branching programs,"We investigate the proof complexity of systems based on positive branching
programs, i.e. non-deterministic branching programs (NBPs) where, for any
0-transition between two nodes, there is also a 1-transition. Positive NBPs
compute monotone Boolean functions, just like negation-free circuits or
formulas, but constitute a positive version of (non-uniform) NL, rather than P
or NC1, respectively.
The proof complexity of NBPs was investigated in previous work by Buss, Das
and Knop, using extension variables to represent the dag-structure, over a
language of (non-deterministic) decision trees, yielding the system eLNDT. Our
system eLNDT+ is obtained by restricting their systems to a positive syntax,
similarly to how the 'monotone sequent calculus' MLK is obtained from the usual
sequent calculus LK by restricting to negation-free formulas.
Our main result is that eLNDT+ polynomially simulates eLNDT over positive
sequents. Our proof method is inspired by a similar result for MLK by Atserias,
Galesi and Pudl\'ak, that was recently improved to a bona fide polynomial
simulation via works of Je\v{r}\'abek and Buss, Kabanets, Kolokolova and
Kouck\'y. Along the way we formalise several properties of counting functions
within eLNDT+ by polynomial-size proofs and, as a case study, give explicit
polynomial-size poofs of the propositional pigeonhole principle."
http://arxiv.org/abs/2102.06599,Title: A model for traffic incident prediction using emergency braking data,"This article presents a model for traffic incident prediction. Specifically,
we address the fundamental problem of data scarcity in road traffic accident
prediction by training our model on emergency braking events instead of
accidents. Based on relevant risk factors for traffic accidents and
corresponding data categories, we evaluate different options for preprocessing
sparse data and different Machine Learning models. Furthermore, we present a
prototype implementing a traffic incident prediction model for Germany based on
emergency braking data from Mercedes-Benz vehicles as well as weather, traffic
and road data, respectively. After model evaluation and optimisation, we found
that a Random Forest model trained on artificially balanced (under-sampled)
data provided the highest classification accuracy of 85% on the original
imbalanced data. Finally, we present our conclusions and discuss further work;
from gathering more data over a longer period of time to build stronger
classification systems, to addition of internal factors such as the driver's
visual and cognitive attention."
http://arxiv.org/abs/2102.06601,Title: Adversarial Branch Architecture Search for Unsupervised Domain  Adaptation,"Unsupervised Domain Adaptation (UDA) is a key field in visual recognition, as
it enables robust performances across different visual domains. In the deep
learning era, the performance of UDA methods has been driven by better losses
and by improved network architectures, specifically the addition of auxiliary
domain-alignment branches to pre-trained backbones. However, all the neural
architectures proposed so far are hand-crafted, which might hinder further
progress.
The current copious offspring of Neural Architecture Search (NAS) only
alleviates hand-crafting so far, as it requires labels for model selection,
which are not available in UDA, and is usually applied to the whole
architecture, while using pre-trained models is a strict requirement for high
performance. No prior work has addressed these aspects in the context of NAS
for UDA.
Here we propose an Adversarial Branch Architecture Search (ABAS) for UDA, to
learn the auxiliary branch network from data without handcrafting. Our main
contribution include i. a novel data-driven ensemble approach for model
selection, to circumvent the lack of target labels, and ii. a pipeline to
automatically search for the best performing auxiliary branch.
To the best of our knowledge, ABAS is the first NAS method for UDA to comply
with a pre-trained backbone, a strict requirement for high performance. ABAS
outputs both the optimal auxiliary branch and its trained parameters. When
applied to two modern UDA techniques, DANN and ALDA, it improves performance on
three standard CV datasets (Office31, Office-Home and PACS). In all cases, ABAS
robustly finds the branch architectures which yield best performances. Code
will be released."
http://arxiv.org/abs/2102.06602,Title: Low precision logarithmic number systems: Beyond base-2,"Logarithmic number systems (LNS) are used to represent real numbers in many
applications using a constant base raised to a fixed-point exponent making its
distribution exponential. This greatly simplifies hardware multiply, divide and
square root. LNS with base-2 is most common, but in this paper we show that for
low-precision LNS the choice of base has a significant impact.
We make four main contributions. First, LNS is not closed under addition and
subtraction, so the result is approximate. We show that choosing a suitable
base can manipulate the distribution to reduce the average error. Second, we
show that low-precision LNS addition and subtraction can be implemented
efficiently in logic rather than commonly used ROM lookup tables, the
complexity of which can be reduced by an appropriate choice of base. A similar
effect is shown where the result of arithmetic has greater precision than the
input. Third, where input data from external sources is not expected to be in
LNS, we can reduce the conversion error by selecting a LNS base to match the
expected distribution of the input. Thus, there is no one base which gives the
global optimum, and base selection is a trade-off between different factors.
Fourth, we show that circuits realized in LNS require lower area and power
consumption for short word lengths."
http://arxiv.org/abs/2102.06603,Title: DeepGLEAM: an hybrid mechanistic and deep learning model for COVID-19  forecasting,"We introduce DeepGLEAM, a hybrid model for COVID-19 forecasting. DeepGLEAM
combines a mechanistic stochastic simulation model GLEAM with deep learning. It
uses deep learning to learn the correction terms from GLEAM, which leads to
improved performance. We further integrate various uncertainty quantification
methods to generate confidence intervals. We demonstrate DeepGLEAM on
real-world COVID-19 mortality forecasting tasks."
http://arxiv.org/abs/2102.06604,Title: Learning Depth via Leveraging Semantics: Self-supervised Monocular Depth  Estimation with Both Implicit and Explicit Semantic Guidance,"Self-supervised depth estimation has made a great success in learning depth
from unlabeled image sequences. While the mappings between image and pixel-wise
depth are well-studied in current methods, the correlation between image, depth
and scene semantics, however, is less considered. This hinders the network to
better understand the real geometry of the scene, since the contextual clues,
contribute not only the latent representations of scene depth, but also the
straight constraints for depth map. In this paper, we leverage the two benefits
by proposing the implicit and explicit semantic guidance for accurate
self-supervised depth estimation. We propose a Semantic-aware Spatial Feature
Alignment (SSFA) scheme to effectively align implicit semantic features with
depth features for scene-aware depth estimation. We also propose a
semantic-guided ranking loss to explicitly constrain the estimated depth maps
to be consistent with real scene contextual properties. Both semantic label
noise and prediction uncertainty is considered to yield reliable depth
supervisions. Extensive experimental results show that our method produces high
quality depth maps which are consistently superior either on complex scenes or
diverse semantic categories, and outperforms the state-of-the-art methods by a
significant margin."
http://arxiv.org/abs/2102.06605,Title: Destination similarity based on implicit user interest,"With the digitization of travel industry, it is more and more important to
understand users from their online behaviors. However, online travel industry
data are more challenging to analyze due to extra sparseness, dispersed user
history actions, fast change of user interest and lack of direct or indirect
feedbacks. In this work, a new similarity method is proposed to measure the
destination similarity in terms of implicit user interest. By comparing the
proposed method to several other widely used similarity measures in recommender
systems, the proposed method achieves a significant improvement on travel data.
Key words: Destination similarity, Travel industry, Recommender System,
Implicit user interest"
http://arxiv.org/abs/2102.06607,Title: Rethinking Eye-blink: Assessing Task Difficulty through Physiological  Representation of Spontaneous Blinking,"Continuous assessment of task difficulty and mental workload is essential in
improving the usability and accessibility of interactive systems. Eye tracking
data has often been investigated to achieve this ability, with reports on the
limited role of standard blink metrics. Here, we propose a new approach to the
analysis of eye-blink responses for automated estimation of task difficulty.
The core module is a time-frequency representation of eye-blink, which aims to
capture the richness of information reflected on blinking. In our first study,
we show that this method significantly improves the sensitivity to task
difficulty. We then demonstrate how to form a framework where the represented
patterns are analyzed with multi-dimensional Long Short-Term Memory recurrent
neural networks for their non-linear mapping onto difficulty-related
parameters. This framework outperformed other methods that used hand-engineered
features. This approach works with any built-in camera, without requiring
specialized devices. We conclude by discussing how Rethinking Eye-blink can
benefit real-world applications."
http://arxiv.org/abs/2102.06610,Title: Bias-Free Scalable Gaussian Processes via Randomized Truncations,"Scalable Gaussian Process methods are computationally attractive, yet
introduce modeling biases that require rigorous study. This paper analyzes two
common techniques: early truncated conjugate gradients (CG) and random Fourier
features (RFF). We find that both methods introduce a systematic bias on the
learned hyperparameters: CG tends to underfit while RFF tends to overfit. We
address these issues using randomized truncation estimators that eliminate bias
in exchange for increased variance. In the case of RFF, we show that the
bias-to-variance conversion is indeed a trade-off: the additional variance
proves detrimental to optimization. However, in the case of CG, our unbiased
learning procedure meaningfully outperforms its biased counterpart with minimal
additional computation."
http://arxiv.org/abs/2102.06613,Title: Efficient Conditional GAN Transfer with Knowledge Propagation across  Classes,"Generative adversarial networks (GANs) have shown impressive results in both
unconditional and conditional image generation. In recent literature, it is
shown that pre-trained GANs, on a different dataset, can be transferred to
improve the image generation from a small target data. The same, however, has
not been well-studied in the case of conditional GANs (cGANs), which provides
new opportunities for knowledge transfer compared to unconditional setup. In
particular, the new classes may borrow knowledge from the related old classes,
or share knowledge among themselves to improve the training. This motivates us
to study the problem of efficient conditional GAN transfer with knowledge
propagation across classes. To address this problem, we introduce a new GAN
transfer method to explicitly propagate the knowledge from the old classes to
the new classes. The key idea is to enforce the popularly used conditional
batch normalization (BN) to learn the class-specific information of the new
classes from that of the old classes, with implicit knowledge sharing among the
new ones. This allows for an efficient knowledge propagation from the old
classes to the new classes, with the BN parameters increasing linearly with the
number of new classes. The extensive evaluation demonstrates the clear
superiority of the proposed method over state-of-the-art competitors for
efficient conditional GAN transfer tasks. The code will be available at:
https://github.com/mshahbazi72/cGANTransfer"
http://arxiv.org/abs/2102.06614,Title: A Parameterised Quantum Circuit Approach to Point Set Matching,"Point set registration is one of the challenging tasks in areas such as
pattern recognition, computer vision and image processing. Efficient
performance of this task has been a hot topic of research due to its widespread
applications. We propose a parameterised quantum circuit learning approach to
point set matching problem. The proposed method benefits from a kernel-based
quantum generative model that: 1) is able to find all possible optimal matching
solution angles, 2) is potentially able to show quantum learning supremacy, and
3) benefits from kernel-embedding techniques and integral probability metrics
for the definition of a powerful loss function. Moreover, the theoretical
framework has been backed up by satisfactory preliminary and proof of concept
experimental results."
http://arxiv.org/abs/2102.06616,Title: Certified Defenses: Why Tighter Relaxations May Hurt Training?,"Certified defenses based on convex relaxations are an established technique
for training provably robust models. The key component is the choice of
relaxation, varying from simple intervals to tight polyhedra. Paradoxically,
however, it was empirically observed that training with tighter relaxations can
worsen certified robustness. While several methods were designed to partially
mitigate this issue, the underlying causes are poorly understood. In this work
we investigate the above phenomenon and show that tightness may not be the
determining factor for reduced certified robustness. Concretely, we identify
two key features of relaxations that impact training dynamics: continuity and
sensitivity. We then experimentally demonstrate that these two factors explain
the drop in certified robustness when using popular relaxations. Further, we
show, for the first time, that it is possible to successfully train with
tighter relaxations (i.e., triangle), a result supported by our two properties.
Overall, we believe the insights of this work can help drive the systematic
discovery of new effective certified defenses."
http://arxiv.org/abs/2102.06621,Title: Explaining Neural Scaling Laws,"The test loss of well-trained neural networks often follows precise power-law
scaling relations with either the size of the training dataset or the number of
parameters in the network. We propose a theory that explains and connects these
scaling laws. We identify variance-limited and resolution-limited scaling
behavior for both dataset and model size, for a total of four scaling regimes.
The variance-limited scaling follows simply from the existence of a
well-behaved infinite data or infinite width limit, while the
resolution-limited regime can be explained by positing that models are
effectively resolving a smooth data manifold. In the large width limit, this
can be equivalently obtained from the spectrum of certain kernels, and we
present evidence that large width and large dataset resolution-limited scaling
exponents are related by a duality. We exhibit all four scaling regimes in the
controlled setting of large random feature and pretrained models and test the
predictions empirically on a range of standard architectures and datasets. We
also observe several empirical relationships between datasets and scaling
exponents: super-classing image tasks does not change exponents, while changing
input distribution (via changing datasets or adding noise) has a strong effect.
We further explore the effect of architecture aspect ratio on scaling
exponents."
http://arxiv.org/abs/2102.06622,Title: Proximal and Federated Random Reshuffling,"Random Reshuffling (RR), also known as Stochastic Gradient Descent (SGD)
without replacement, is a popular and theoretically grounded method for
finite-sum minimization. We propose two new algorithms: Proximal and Federated
Random Reshuffing (ProxRR and FedRR). The first algorithm, ProxRR, solves
composite convex finite-sum minimization problems in which the objective is the
sum of a (potentially non-smooth) convex regularizer and an average of $n$
smooth objectives. We obtain the second algorithm, FedRR, as a special case of
ProxRR applied to a reformulation of distributed problems with either
homogeneous or heterogeneous data. We study the algorithms' convergence
properties with constant and decreasing stepsizes, and show that they have
considerable advantages over Proximal and Local SGD. In particular, our methods
have superior complexities and ProxRR evaluates the proximal operator once per
epoch only. When the proximal operator is expensive to compute, this small
difference makes ProxRR up to $n$ times faster than algorithms that evaluate
the proximal operator in every iteration. We give examples of practical
optimization tasks where the proximal operator is difficult to compute and
ProxRR has a clear advantage. Finally, we corroborate our results with
experiments on real data sets."
http://arxiv.org/abs/2102.06624,Title: Higher Order Generalization Error for First Order Discretization of  Langevin Diffusion,"We propose a novel approach to analyze generalization error for
discretizations of Langevin diffusion, such as the stochastic gradient Langevin
dynamics (SGLD). For an $\epsilon$ tolerance of expected generalization error,
it is known that a first order discretization can reach this target if we run
$\Omega(\epsilon^{-1} \log (\epsilon^{-1}) )$ iterations with
$\Omega(\epsilon^{-1})$ samples. In this article, we show that with additional
smoothness assumptions, even first order methods can achieve arbitrarily
runtime complexity. More precisely, for each $N>0$, we provide a sufficient
smoothness condition on the loss function such that a first order
discretization can reach $\epsilon$ expected generalization error given
$\Omega( \epsilon^{-1/N} \log (\epsilon^{-1}) )$ iterations with
$\Omega(\epsilon^{-1})$ samples."
http://arxiv.org/abs/2102.06626,Title: An Investigation of End-to-End Models for Robust Speech Recognition,"End-to-end models for robust automatic speech recognition (ASR) have not been
sufficiently well-explored in prior work. With end-to-end models, one could
choose to preprocess the input speech using speech enhancement techniques and
train the model using enhanced speech. Another alternative is to pass the noisy
speech as input and modify the model architecture to adapt to noisy speech. A
systematic comparison of these two approaches for end-to-end robust ASR has not
been attempted before. We address this gap and present a detailed comparison of
speech enhancement-based techniques and three different model-based adaptation
techniques covering data augmentation, multi-task learning, and adversarial
learning for robust ASR. While adversarial learning is the best-performing
technique on certain noise types, it comes at the cost of degrading clean
speech WER. On other relatively stationary noise types, a new speech
enhancement technique outperformed all the model-based adaptation techniques.
This suggests that knowledge of the underlying noise type can meaningfully
inform the choice of adaptation technique."
http://arxiv.org/abs/2102.06627,Title: A High Speed Integrated Quantum Random Number Generator with on-Chip  Real-Time Randomness Extraction,"The security of electronic devices has become a key requisite for the
rapidly-expanding pervasive and hyper-connected world. Robust security
protocols ensuring secure communication, device's resilience to attacks,
authentication control and users privacy need to be implemented. Random Number
Generators (RNGs) are the fundamental primitive in most secure protocols but,
often, also the weakest one. Establishing security in billions of devices
requires high quality random data generated at a sufficiently high throughput.
On the other hand, the RNG should exhibit a high integration level with on-chip
extraction to remove, in real time, potential imperfections. We present the
first integrated Quantum RNG (QRNG) in a standard CMOS technology node. The
QRNG is based on a parallel array of independent Single-Photon Avalanche Diodes
(SPADs), homogeneously illuminated by a DC-biased LED, and co-integrated logic
circuits for postprocessing. We describe the randomness generation process and
we prove the quantum origin of entropy. We show that co-integration of
combinational logic, even of high complexity, does not affect the quality of
randomness. Our CMOS QRNG can reach up to 400 Mbit/s throughput with low power
consumption. Thanks to the use of standard CMOS technology and a modular
architecture, our QRNG is suitable for a highly scalable solution."
http://arxiv.org/abs/2102.06632,Title: Echo State Networks for Reinforcement Learning,"Echo State Networks (ESNs) are a type of single-layer recurrent neural
network with randomly-chosen internal weights and a trainable output layer. We
prove under mild conditions that a sufficiently large Echo State Network (ESN)
can approximate the value function of a broad class of stochastic and
deterministic control problems. Such control problems are generally
non-Markovian. We describe how the ESN can form the basis for novel (and
computationally efficient) reinforcement learning algorithms in a non-Markovian
framework. We demonstrate this theory with two examples. In the first, we use
an ESN to solve a deterministic, partially observed, control problem which is a
simple game we call `Bee World'. In the second example, we consider a
stochastic control problem inspired by a market making problem in mathematical
finance. In both cases we can compare the dynamics of the algorithms with
analytic solutions to show that even after only a single reinforcement policy
iteration the algorithms perform with reasonable skill."
http://arxiv.org/abs/2102.06633,Title: Disentanglement for audio-visual emotion recognition using multitask  setup,"Deep learning models trained on audio-visual data have been successfully used
to achieve state-of-the-art performance for emotion recognition. In particular,
models trained with multitask learning have shown additional performance
improvements. However, such multitask models entangle information between the
tasks, encoding the mutual dependencies present in label distributions in the
real world data used for training. This work explores the disentanglement of
multimodal signal representations for the primary task of emotion recognition
and a secondary person identification task. In particular, we developed a
multitask framework to extract low-dimensional embeddings that aim to capture
emotion specific information, while containing minimal information related to
person identity. We evaluate three different techniques for disentanglement and
report results of up to 13% disentanglement while maintaining emotion
recognition performance."
http://arxiv.org/abs/2102.06634,Title: The Univalence Principle,"The Univalence Principle is the statement that equivalent mathematical
structures are indistinguishable. We prove a general version of this principle
that applies to all set-based, categorical, and higher-categorical structures
defined in a non-algebraic and space-based style, as well as models of
higher-order theories such as topological spaces. In particular, we formulate a
general definition of indiscernibility for objects of any such structure, and a
corresponding univalence condition that generalizes Rezk's completeness
condition for Segal spaces and ensures that all equivalences of structures are
levelwise equivalences.
Our work builds on Makkai's First-Order Logic with Dependent Sorts, but is
expressed in Voevodsky's Univalent Foundations (UF), extending previous work on
the Structure Identity Principle and univalent categories in UF. This enables
indistinguishability to be expressed simply as identification, and yields a
formal theory that is interpretable in classical homotopy theory, but also in
other higher topos models. It follows that Univalent Foundations is a fully
equivalence-invariant foundation for higher-categorical mathematics, as
intended by Voevodsky."
http://arxiv.org/abs/2102.06635,Title: Unsupervised Ground Metric Learning using Wasserstein Eigenvectors,"Optimal Transport (OT) defines geometrically meaningful ""Wasserstein""
distances, used in machine learning applications to compare probability
distributions. However, a key bottleneck is the design of a ""ground"" cost which
should be adapted to the task under study. In most cases, supervised metric
learning is not accessible, and one usually resorts to some ad-hoc approach.
Unsupervised metric learning is thus a fundamental problem to enable
data-driven applications of Optimal Transport. In this paper, we propose for
the first time a canonical answer by computing the ground cost as a positive
eigenvector of the function mapping a cost to the pairwise OT distances between
the inputs. This map is homogeneous and monotone, thus framing unsupervised
metric learning as a non-linear Perron-Frobenius problem. We provide criteria
to ensure the existence and uniqueness of this eigenvector. In addition, we
introduce a scalable computational method using entropic regularization, which
- in the large regularization limit - operates a principal component analysis
dimensionality reduction. We showcase this method on synthetic examples and
datasets. Finally, we apply it in the context of biology to the analysis of a
high-throughput single-cell RNA sequencing (scRNAseq) dataset, to improve cell
clustering and infer the relationships between genes in an unsupervised way."
http://arxiv.org/abs/2102.06638,Title: COVID-19 detection from scarce chest x-ray image data using deep  learning,"In the current COVID-19 pandemic situation, there is an urgent need to screen
infected patients quickly and accurately. Using deep learning models trained on
chest X-ray images can become an efficient method for screening COVID-19
patients in these situations. Deep learning approaches are already widely used
in the medical community. However, they require a large amount of data to be
accurate. The open-source community collectively has made efforts to collect
and annotate the data, but it is not enough to train an accurate deep learning
model. Few-shot learning is a sub-field of machine learning that aims to learn
the objective with less amount of data. In this work, we have experimented with
well-known solutions for data scarcity in deep learning to detect COVID-19.
These include data augmentation, transfer learning, and few-shot learning, and
unsupervised learning. We have also proposed a custom few-shot learning
approach to detect COVID-19 using siamese networks. Our experimental results
showcased that we can implement an efficient and accurate deep learning model
for COVID-19 detection by adopting the few-shot learning approaches even with
less amount of data. Using our proposed approach we were able to achieve 96.4%
accuracy an improvement from 83% using baseline models."
http://arxiv.org/abs/2102.06645,Title: Some Hoeffding- and Bernstein-type Concentration Inequalities,"We prove concentration inequalities for functions of independent random
variables {under} sub-gaussian and sub-exponential conditions. The utility of
the inequalities is demonstrated by an extension of the now classical method of
Rademacher complexities to Lipschitz function classes and unbounded
sub-exponential distribution."
http://arxiv.org/abs/2102.06648,Title: DEEPF0: End-To-End Fundamental Frequency Estimation for Music and Speech  Signals,"We propose a novel pitch estimation technique called DeepF0, which leverages
the available annotated data to directly learns from the raw audio in a
data-driven manner. F0 estimation is important in various speech processing and
music information retrieval applications. Existing deep learning models for
pitch estimations have relatively limited learning capabilities due to their
shallow receptive field. The proposed model addresses this issue by extending
the receptive field of a network by introducing the dilated convolutional
blocks into the network. The dilation factor increases the network receptive
field exponentially without increasing the parameters of the model
exponentially. To make the training process more efficient and faster, DeepF0
is augmented with residual blocks with residual connections. Our empirical
evaluation demonstrates that the proposed model outperforms the baselines in
terms of raw pitch accuracy and raw chroma accuracy even using 77.4% fewer
network parameters. We also show that our model can capture reasonably well
pitch estimation even under the various levels of accompaniment noise."
http://arxiv.org/abs/2102.06650,Title: Segmentation-Renormalized Deep Feature Modulation for Unpaired Image  Harmonization,"Deep networks are now ubiquitous in large-scale multi-center imaging studies.
However, the direct aggregation of images across sites is contraindicated for
downstream statistical and deep learning-based image analysis due to
inconsistent contrast, resolution, and noise. To this end, in the absence of
paired data, variations of Cycle-consistent Generative Adversarial Networks
have been used to harmonize image sets between a source and target domain.
Importantly, these methods are prone to instability, contrast inversion,
intractable manipulation of pathology, and steganographic mappings which limit
their reliable adoption in real-world medical imaging. In this work, based on
an underlying assumption that morphological shape is consistent across imaging
sites, we propose a segmentation-renormalized image translation framework to
reduce inter-scanner heterogeneity while preserving anatomical layout. We
replace the affine transformations used in the normalization layers within
generative networks with trainable scale and shift parameters conditioned on
jointly learned anatomical segmentation embeddings to modulate features at
every level of translation. We evaluate our methodologies against recent
baselines across several imaging modalities (T1w MRI, FLAIR MRI, and OCT) on
datasets with and without lesions. Segmentation-renormalization for translation
GANs yields superior image harmonization as quantified by Inception distances,
demonstrates improved downstream utility via post-hoc segmentation accuracy,
and improved robustness to translation perturbation and self-adversarial
attacks."
http://arxiv.org/abs/2102.06652,Title: Joint Dereverberation and Separation with Iterative Source Steering,"We propose a new algorithm for joint dereverberation and blind source
separation (DR-BSS). Our work builds upon the IRLMA-T framework that applies a
unified filter combining dereverberation and separation. One drawback of this
framework is that it requires several matrix inversions, an operation
inherently costly and with potential stability issues. We leverage the recently
introduced iterative source steering (ISS) updates to propose two algorithms
mitigating this issue. Albeit derived from first principles, the first
algorithm turns out to be a natural combination of weighted prediction error
(WPE) dereverberation and ISS-based BSS, applied alternatingly. In this case,
we manage to reduce the number of matrix inversion to only one per iteration
and source. The second algorithm updates the ILRMA-T matrix using only
sequential ISS updates requiring no matrix inversion at all. Its implementation
is straightforward and memory efficient. Numerical experiments demonstrate that
both methods achieve the same final performance as ILRMA-T in terms of several
relevant objective metrics. In the important case of two sources, the number of
iterations required is also similar."
http://arxiv.org/abs/2102.06655,Title: Uncertainty-Aware Semi-supervised Method using Large Unlabelled and  Limited Labeled COVID-19 Data,"The new coronavirus has caused more than 1 million deaths and continues to
spread rapidly. This virus targets the lungs, causing respiratory distress
which can be mild or severe. The X-ray or computed tomography (CT) images of
lungs can reveal whether the patient is infected with COVID-19 or not. Many
researchers are trying to improve COVID-19 detection using artificial
intelligence. In this paper, relying on Generative Adversarial Networks (GAN),
we propose a Semi-supervised Classification using Limited Labelled Data (SCLLD)
for automated COVID-19 detection. Our motivation is to develop learning method
which can cope with scenarios that preparing labelled data is time consuming or
expensive. We further improved the detection accuracy of the proposed method by
applying Sobel edge detection. The GAN discriminator output is a probability
value which is used for classification in this work. The proposed system is
trained using 10,000 CT scans collected from Omid hospital. Also, we validate
our system using the public dataset. The proposed method is compared with other
state of the art supervised methods such as Gaussian processes. To the best of
our knowledge, this is the first time a COVID-19 semi-supervised detection
method is presented. Our method is capable of learning from a mixture of
limited labelled and unlabelled data where supervised learners fail due to lack
of sufficient amount of labelled data. Our semi-supervised training method
significantly outperforms the supervised training of Convolutional Neural
Network (CNN) in case labelled training data is scarce. Our method has achieved
an accuracy of 99.60%, sensitivity of 99.39%, and specificity of 99.80% where
CNN (trained supervised) has achieved an accuracy of 69.87%, sensitivity of
94%, and specificity of 46.40%."
http://arxiv.org/abs/2102.06656,Title: Mind the beat: detecting audio onsets from EEG recordings of music  listening,"We propose a deep learning approach to predicting audio event onsets in
electroencephalogram (EEG) recorded from users as they listen to music. We use
a publicly available dataset containing ten contemporary songs and concurrently
recorded EEG. We generate a sequence of onset labels for the songs in our
dataset and trained neural networks (a fully connected network (FCN) and a
recurrent neural network (RNN)) to parse one second windows of input EEG to
predict one second windows of onsets in the audio. We compare our RNN network
to both the standard spectral-flux based novelty function and the FCN. We find
that our RNN was able to produce results that reflected its ability to
generalize better than the other methods.
Since there are no pre-existing works on this topic, the numbers presented in
this paper may serve as useful benchmarks for future approaches to this
research problem."
http://arxiv.org/abs/2102.06657,Title: Explaining predictive models using Shapley values and non-parametric  vine copulas,"The original development of Shapley values for prediction explanation relied
on the assumption that the features being described were independent. If the
features in reality are dependent this may lead to incorrect explanations.
Hence, there have recently been attempts of appropriately modelling/estimating
the dependence between the features. Although the proposed methods clearly
outperform the traditional approach assuming independence, they have their
weaknesses. In this paper we propose two new approaches for modelling the
dependence between the features.
Both approaches are based on vine copulas, which are flexible tools for
modelling multivariate non-Gaussian distributions able to characterise a wide
range of complex dependencies.
The performance of the proposed methods is evaluated on simulated data sets
and a real data set. The experiments demonstrate that the vine copula
approaches give more accurate approximations to the true Shapley values than
its competitors."
http://arxiv.org/abs/2102.06659,Title: Interview Hoarding,"Many centralized matching markets are preceded by interviews between the
participants. We study the impact on the final match of an increase to the
number of interviews one side of the market can participate in. Our motivation
is the match between residents and hospitals where, due to the COVID-19
pandemic, interviews for the 2020-21 season of the NRMP match have switched to
a virtual format. This has drastically reduced the cost to applicants of
accepting interview offers. However, the reduction in cost is not symmetric
since applicants, not programs, bore most of the costs of in-person interviews.
We show that if doctors are willing to accept more interviews but the hospitals
do not increase the number of interviews they offer, no doctor will be better
off and potentially many doctors will be harmed. This adverse consequence
results from a mechanism we describe as interview hoarding. We prove this
analytically and characterize optimal mitigation strategies for special cases.
We use simulations to extend the insights from our analytical results to more
general settings."
http://arxiv.org/abs/2102.06661,Title: Guided Variational Autoencoder for Speech Enhancement With a Supervised  Classifier,"Recently, variational autoencoders have been successfully used to learn a
probabilistic prior over speech signals, which is then used to perform speech
enhancement. However, variational autoencoders are trained on clean speech
only, which results in a limited ability of extracting the speech signal from
noisy speech compared to supervised approaches. In this paper, we propose to
guide the variational autoencoder with a supervised classifier separately
trained on noisy speech. The estimated label is a high-level categorical
variable describing the speech signal (e.g. speech activity) allowing for a
more informed latent distribution compared to the standard variational
autoencoder. We evaluate our method with different types of labels on real
recordings of different noisy environments. Provided that the label better
informs the latent distribution and that the classifier achieves good
performance, the proposed approach outperforms the standard variational
autoencoder and a conventional neural network-based supervised approach."
http://arxiv.org/abs/2102.06662,Title: Leveraging Global Parameters for Flow-based Neural Posterior Estimation,"Inferring the parameters of a stochastic model based on experimental
observations is central to the scientific method. A particularly challenging
setting is when the model is strongly indeterminate, i.e., when distinct sets
of parameters yield identical observations. This arises in many practical
situations, such as when inferring the distance and power of a radio source (is
the source close and weak or far and strong?) or when estimating the amplifier
gain and underlying brain activity of an electrophysiological experiment. In
this work, we present a method for cracking such indeterminacy by exploiting
additional information conveyed by an auxiliary set of observations sharing
global parameters. Our method extends recent developments in simulation-based
inference(SBI) based on normalizing flows to Bayesian hierarchical models. We
validate quantitatively our proposal on a motivating example amenable to
analytical solutions, and then apply it to invert a well known non-linear model
from computational neuroscience."
http://arxiv.org/abs/2102.06663,Title: Stability and Convergence of Stochastic Gradient Clipping: Beyond  Lipschitz Continuity and Smoothness,"Stochastic gradient algorithms are often unstable when applied to functions
that do not have Lipschitz-continuous and/or bounded gradients. Gradient
clipping is a simple and effective technique to stabilize the training process
for problems that are prone to the exploding gradient problem. Despite its
widespread popularity, the convergence properties of the gradient clipping
heuristic are poorly understood, especially for stochastic problems. This paper
establishes both qualitative and quantitative convergence results of the
clipped stochastic (sub)gradient method (SGD) for non-smooth convex functions
with rapidly growing subgradients. Our analyses show that clipping enhances the
stability of SGD and that the clipped SGD algorithm enjoys finite convergence
rates in many cases. We also study the convergence of a clipped method with
momentum, which includes clipped SGD as a special case, for weakly convex
problems under standard assumptions. With a novel Lyapunov analysis, we show
that the proposed method achieves the best-known rate for the considered class
of problems, demonstrating the effectiveness of clipped methods also in this
regime. Numerical results confirm our theoretical developments."
http://arxiv.org/abs/2102.06665,Title: Flying V and Reference Aircraft Evacuation Simulation and Comparison,"A preliminary comparison of evacuation times of the Flying V and the Airbus
A350-900 is presented in this study. A simple simulation tool based on the
technique of cellular automata was created to model the evacuation process for
different closed door configurations. Certification regulations state that the
time to evacuate a civil aircraft in case of an emergency with half of all exit
doors closed must be less than 90 seconds. The results of this study indicate
that the shorter V shaped cabin has advantages over the longer conventional
reference cabin for cases when passengers need to evacuate towards the front or
the back of the aircraft. Disadvantages occur when the passengers in the V
shaped cabin need to evacuate more towards one side (left or right wing) of the
aircraft. A more detailed simulation model to further investigate these cases
is currently created by the authors."
http://arxiv.org/abs/2102.06666,Title: Mediastinal lymph nodes segmentation using 3D convolutional neural  network ensembles and anatomical priors guiding,"As lung cancer evolves, the presence of enlarged and potentially malignant
lymph nodes must be assessed to properly estimate disease progression and
select the best treatment strategy. Following the clinical guidelines,
estimation of short-axis diameter and mediastinum station are paramount for
correct diagnosis. A method for accurate and automatic segmentation is hence
decisive for quantitatively describing lymph nodes. In this study, the use of
3D convolutional neural networks, either through slab-wise schemes or the
leveraging of downsampled entire volumes, is investigated. Furthermore, the
potential impact from simple ensemble strategies is considered. As lymph nodes
have similar attenuation values to nearby anatomical structures, we suggest
using the knowledge of other organs as prior information to guide the
segmentation task. To assess the segmentation and instance detection
performances, a 5-fold cross-validation strategy was followed over a dataset of
120 contrast-enhanced CT volumes. For the 1178 lymph nodes with a short-axis
diameter $\geq10$ mm, our best performing approach reached a patient-wise
recall of 92%, a false positive per patient ratio of 5, and a segmentation
overlap of 80.5%. The method performs similarly well across all stations.
Fusing a slab-wise and a full volume approach within an ensemble scheme
generated the best performances. The anatomical priors guiding strategy is
promising, yet a larger set than four organs appears needed to generate an
optimal benefit. A larger dataset is also mandatory, given the wide range of
expressions a lymph node can exhibit (i.e., shape, location, and attenuation),
and contrast uptake variations."
http://arxiv.org/abs/2102.06668,Title: Convex Synthesis of Accelerated Gradient Algorithms,"We present a convex solution for the design of generalized accelerated
gradient algorithms for strongly convex objective functions with Lipschitz
continuous gradients. We utilize integral quadratic constraints and the Youla
parameterization from robust control theory to formulate a solution of the
algorithm design problem as a convex semi-definite program. We establish
explicit formulas for the optimal convergence rates and extend the proposed
synthesis solution to extremum control problems."
http://arxiv.org/abs/2102.06671,Title: Robust and integrative Bayesian neural networks for likelihood-free  parameter inference,"State-of-the-art neural network-based methods for learning summary statistics
have delivered promising results for simulation-based likelihood-free parameter
inference. Existing approaches require density estimation as a post-processing
step building upon deterministic neural networks, and do not take network
prediction uncertainty into account. This work proposes a robust integrated
approach that learns summary statistics using Bayesian neural networks, and
directly estimates the posterior density using categorical distributions. An
adaptive sampling scheme selects simulation locations to efficiently and
iteratively refine the predictive posterior of the network conditioned on
observations. This allows for more efficient and robust convergence on
comparatively large prior spaces. We demonstrate our approach on benchmark
examples and compare against related methods."
http://arxiv.org/abs/2102.06673,Title: Sequential Neural Posterior and Likelihood Approximation,"We introduce the sequential neural posterior and likelihood approximation
(SNPLA) algorithm. SNPLA is a normalizing flows-based algorithm for inference
in implicit models. Thus, SNPLA is a simulation-based inference method that
only requires simulations from a generative model. Compared to similar methods,
the main advantage of SNPLA is that our method jointly learns both the
posterior and the likelihood. SNPLA completely avoid Markov chain Monte Carlo
sampling and correction-steps of the parameter proposal function that are
introduced in similar methods, but that can be numerically unstable or
restrictive. Over four experiments, we show that SNPLA performs competitively
when utilizing the same number of model simulations as used in other methods,
even though the inference problem for SNPLA is more complex due to the joint
learning of posterior and likelihood function."
http://arxiv.org/abs/2102.06674,Title: Comparison of Machine Learning Classifiers to Predict Patient Survival  and Genetics of GBM: Towards a Standardized Model for Clinical Implementation,"Radiomic models have been shown to outperform clinical data for outcome
prediction in glioblastoma (GBM). However, clinical implementation is limited
by lack of parameters standardization. We aimed to compare nine machine
learning classifiers, with different optimization parameters, to predict
overall survival (OS), isocitrate dehydrogenase (IDH) mutation,
O-6-methylguanine-DNA-methyltransferase (MGMT) promoter methylation, epidermal
growth factor receptor (EGFR) VII amplification and Ki-67 expression in GBM
patients, based on radiomic features from conventional and advanced MR. 156
adult patients with pathologic diagnosis of GBM were included. Three tumoral
regions were analyzed: contrast-enhancing tumor, necrosis and non-enhancing
tumor, selected by manual segmentation. Radiomic features were extracted with a
custom version of Pyradiomics, and selected through Boruta algorithm. A Grid
Search algorithm was applied when computing 4 times K-fold cross validation
(K=10) to get the highest mean and lowest spread of accuracy. Once optimal
parameters were identified, model performances were assessed in terms of Area
Under The Curve-Receiver Operating Characteristics (AUC-ROC). Metaheuristic and
ensemble classifiers showed the best performance across tasks. xGB obtained
maximum accuracy for OS (74.5%), AB for IDH mutation (88%), MGMT methylation
(71,7%), Ki-67 expression (86,6%), and EGFR amplification (81,6%). Best
performing features shed light on possible correlations between MR and tumor
histology."
http://arxiv.org/abs/2102.06679,Title: Hybrid quantum convolutional neural networks model for COVID-19  prediction using chest X-Ray images,"Despite the great efforts to find an effective way for COVID-19 prediction,
the virus nature and mutation represent a critical challenge to diagnose the
covered cases. However, developing a model to predict COVID-19 via Chest X-Ray
(CXR) images with accurate performance is necessary to help in early diagnosis.
In this paper, a hybrid quantum-classical convolutional Neural Networks (HQCNN)
model used the random quantum circuits (RQCs) as a base to detect COVID-19
patients with CXR images. A collection of 6952 CXR images, including 1161
COVID-19, 1575 normal, and 5216 pneumonia images, were used as a dataset in
this work. The proposed HQCNN model achieved higher performance with an
accuracy of 98.4\% and a sensitivity of 99.3\% on the first dataset cases.
Besides, it obtained an accuracy of 99\% and a sensitivity of 99.7\% on the
second dataset cases. Also, it achieved accuracy, and sensitivity of 88.6\%,
and 88.7\%, respectively, on the third multi-class dataset cases. Furthermore,
the HQCNN model outperforms various models in balanced accuracy, precision,
F1-measure, and AUC-ROC score. The experimental results are achieved by the
proposed model prove its ability in predicting positive COVID-19 cases."
http://arxiv.org/abs/2102.06681,Title: Tightening the Dependence on Horizon in the Sample Complexity of  Q-Learning,"Q-learning, which seeks to learn the optimal Q-function of a Markov decision
process (MDP) in a model-free fashion, lies at the heart of reinforcement
learning. When it comes to the synchronous setting (such that independent
samples for all state-action pairs are drawn from a generative model in each
iteration), substantial progress has been made recently towards understanding
the sample efficiency of Q-learning. To yield an entrywise
$\varepsilon$-accurate estimate of the optimal Q-function, state-of-the-art
theory requires at least an order of
$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^5\varepsilon^{2}}$ samples for a
$\gamma$-discounted infinite-horizon MDP with state space $\mathcal{S}$ and
action space $\mathcal{A}$. In this work, we sharpen the sample complexity of
synchronous Q-learning to an order of
$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^4\varepsilon^2}$ (up to some
logarithmic factor) for any $0<\varepsilon <1$, leading to an order-wise
improvement in terms of the effective horizon $\frac{1}{1-\gamma}$. Analogous
results are derived for finite-horizon MDPs as well. Our finding unveils the
effectiveness of vanilla Q-learning, which matches that of speedy Q-learning
without requiring extra computation and storage. A key ingredient of our
analysis lies in the establishment of novel error decompositions and
recursions, which might shed light on how to analyze finite-sample performance
of other Q-learning variants."
http://arxiv.org/abs/2102.06684,Title: Infinitely Deep Bayesian Neural Networks with Stochastic Differential  Equations,"We perform scalable approximate inference in a recently-proposed family of
continuous-depth Bayesian neural networks. In this model class, uncertainty
about separate weights in each layer produces dynamics that follow a stochastic
differential equation (SDE). We demonstrate gradient-based stochastic
variational inference in this infinite-parameter setting, producing
arbitrarily-flexible approximate posteriors. We also derive a novel gradient
estimator that approaches zero variance as the approximate posterior approaches
the true posterior. This approach further inherits the memory-efficient
training and tunable precision of neural ODEs."
http://arxiv.org/abs/2102.06685,Title: Bayesian Neural Network Priors Revisited,"Isotropic Gaussian priors are the de facto standard for modern Bayesian
neural network inference. However, such simplistic priors are unlikely to
either accurately reflect our true beliefs about the weight distributions, or
to give optimal performance. We study summary statistics of neural network
weights in different networks trained using SGD. We find that fully connected
networks (FCNNs) display heavy-tailed weight distributions, while convolutional
neural network (CNN) weights display strong spatial correlations. Building
these observations into the respective priors leads to improved performance on
a variety of image classification datasets. Moreover, we find that these priors
also mitigate the cold posterior effect in FCNNs, while in CNNs we see strong
improvements at all temperatures, and hence no reduction in the cold posterior
effect."
http://arxiv.org/abs/2102.06687,Title: Sparse Bayesian Causal Forests for Heterogeneous Treatment Effects  Estimation,"This paper develops a sparsity-inducing version of Bayesian Causal Forests, a
recently proposed nonparametric causal regression model that employs Bayesian
Additive Regression Trees and is specifically designed to estimate
heterogeneous treatment effects using observational data. The sparsity-inducing
component we introduce is motivated by empirical studies where the number of
pre-treatment covariates available is non-negligible, leading to different
degrees of sparsity underlying the surfaces of interest in the estimation of
individual treatment effects. The extended version presented in this work,
which we name Sparse Bayesian Causal Forest, is equipped with an additional
pair of priors allowing the model to adjust the weight of each covariate
through the corresponding number of splits in the tree ensemble. These priors
improve the model's adaptability to sparse settings and allow to perform fully
Bayesian variable selection in a framework for treatment effects estimation,
and thus to uncover the moderating factors driving heterogeneity. In addition,
the method allows prior knowledge about the relevant confounding pre-treatment
covariates and the relative magnitude of their impact on the outcome to be
incorporated in the model. We illustrate the performance of our method in
simulated studies, in comparison to Bayesian Causal Forest and other
state-of-the-art models, to demonstrate how it scales up with an increasing
number of covariates and how it handles strongly confounded scenarios. Finally,
we also provide an example of application using real-world data."
http://arxiv.org/abs/2102.06690,Title: Pareto Optimal Model Selection in Linear Bandits,"We study a model selection problem in the linear bandit setting, where the
learner must adapt to the dimension of the optimal hypothesis class on the fly
and balance exploration and exploitation. More specifically, we assume a
sequence of nested linear hypothesis classes with dimensions $d_1 < d_2 <
\dots$, and the goal is to automatically adapt to the smallest hypothesis class
that contains the true linear model. Although previous papers provide various
guarantees for this model selection problem, the analysis therein either works
in favorable cases when one can cheaply conduct statistical testing to locate
the right hypothesis class or is based on the idea of ""corralling"" multiple
base algorithms which often performs relatively poorly in practice. These works
also mainly focus on upper bounding the regret. In this paper, we first
establish a lower bound showing that, even with a fixed action set, adaptation
to the unknown intrinsic dimension $d_\star$ comes at a cost: there is no
algorithm that can achieve the regret bound $\widetilde{O}(\sqrt{d_\star T})$
simultaneously for all values of $d_\star$. We also bring new ideas, i.e.,
constructing virtual mixture-arms to effectively summarize useful information,
into the model selection problem in linear bandits. Under a mild assumption on
the action set, we design a Pareto optimal algorithm with guarantees matching
the rate in the lower bound. Experimental results confirm our theoretical
results and show advantages of our algorithm compared to prior work."
http://arxiv.org/abs/2102.06695,Title: Enhancing into the codec: Noise Robust Speech Coding with  Vector-Quantized Autoencoders,"Audio codecs based on discretized neural autoencoders have recently been
developed and shown to provide significantly higher compression levels for
comparable quality speech output. However, these models are tightly coupled
with speech content, and produce unintended outputs in noisy conditions. Based
on VQ-VAE autoencoders with WaveRNN decoders, we develop compressor-enhancer
encoders and accompanying decoders, and show that they operate well in noisy
conditions. We also observe that a compressor-enhancer model performs better on
clean speech inputs than a compressor model trained only on clean speech."
http://arxiv.org/abs/2102.06696,Title: A Generative Model for Hallucinating Diverse Versions of Super  Resolution Images,"Traditionally, the main focus of image super-resolution techniques is on
recovering the most likely high-quality images from low-quality images, using a
one-to-one low- to high-resolution mapping. Proceeding that way, we ignore the
fact that there are generally many valid versions of high-resolution images
that map to a given low-resolution image. We are tackling in this work the
problem of obtaining different high-resolution versions from the same
low-resolution image using Generative Adversarial Models. Our learning approach
makes use of high frequencies available in the training high-resolution images
for preserving and exploring in an unsupervised manner the structural
information available within these images. Experimental results on the CelebA
dataset confirm the effectiveness of the proposed method, which allows the
generation of both realistic and diverse high-resolution images from
low-resolution images."
http://arxiv.org/abs/2102.06697,Title: A fast and scalable computational framework for goal-oriented linear  Bayesian optimal experimental design: Application to optimal sensor placement,"Optimal experimental design (OED) is a principled framework for maximizing
information gained from limited data in inverse problems. Unfortunately,
conventional methods for OED are prohibitive when applied to expensive models
with high-dimensional parameters, as we target here. We develop a fast and
scalable computational framework for goal-oriented OED of large-scale Bayesian
linear inverse problems that finds sensor locations to maximize the expected
information gain (EIG) for a predicted quantity of interest. By employing
low-rank approximations of appropriate operators, an online-offline
decomposition, and a new swapping greedy algorithm, we are able to maximize EIG
at a cost measured in model solutions that is independent of the problem
dimensions. We demonstrate the efficiency, accuracy, and both data- and
parameter-dimension independence of the proposed algorithm for a contaminant
transport inverse problem with infinite-dimensional parameter field."
http://arxiv.org/abs/2102.06700,Title: Potential Singularity Formation of 3D Axisymmetric Navier-Stokes  Equations with Degenerate Variable Diffusion Coefficients,"In this paper, we present strong numerical evidences that the $3$D
axisymmetric Navier-Stokes equations with degenerate variable diffusion
coefficients and smooth initial data of finite energy develop a potential
finite time locally self-similar singularity at the origin. An important
feature of this potential singularity is that the solution develops a two-scale
traveling wave that travels towards the origin. The two-scale feature is
characterized by the property that the center of the traveling wave approaches
to the origin at a slower rate than the rate of the collapse of the
singularity. The driving mechanism for this potential singularity is due to two
antisymmetric vortex dipoles that generate a strong shearing layer in both the
radial and axial velocity fields, which transport the solution first towards
$z=0$ and then towards the symmetry axis $r=0$. The initial condition is
designed in such a way that it generates a positive feedback loop that enforces
a strong nonlinear alignment of vortex stretching, leading to a stable locally
self-similar blowup at the origin. We perform careful resolution study and
asymptotic scaling analysis to provide further support of the potential finite
time locally self-similar blowup."
http://arxiv.org/abs/2102.06701,Title: Bayesian Uncertainty Estimation of Learned Variational MRI  Reconstruction,"Recent deep learning approaches focus on improving quantitative scores of
dedicated benchmarks, and therefore only reduce the observation-related
(aleatoric) uncertainty. However, the model-immanent (epistemic) uncertainty is
less frequently systematically analyzed. In this work, we introduce a Bayesian
variational framework to quantify the epistemic uncertainty. To this end, we
solve the linear inverse problem of undersampled MRI reconstruction in a
variational setting. The associated energy functional is composed of a data
fidelity term and the total deep variation (TDV) as a learned parametric
regularizer. To estimate the epistemic uncertainty we draw the parameters of
the TDV regularizer from a multivariate Gaussian distribution, whose mean and
covariance matrix are learned in a stochastic optimal control problem. In
several numerical experiments, we demonstrate that our approach yields
competitive results for undersampled MRI reconstruction. Moreover, we can
accurately quantify the pixelwise epistemic uncertainty, which can serve
radiologists as an additional resource to visualize reconstruction reliability."
http://arxiv.org/abs/2102.06702,Title: Numerical analysis of a model of two phase compressible fluid flow,"We consider a model of a binary mixture of two immiscible compressible
fluids. We propose a numerical scheme and discuss its basic properties:
Stability, consistency, convergence. The convergence is established via the
method of generalized weak solutions combined with the weak-strong uniqueness
principle."
http://arxiv.org/abs/2102.06704,Title: Material absorption-based carrier generation model for modeling  optoelectronic devices,"The generation rate of photocarriers in optoelectronic materials is commonly
calculated using the Poynting vector in the frequency domain. In time-domain
approaches where the nonlinear coupling between electromagnetic (EM) waves and
photocarriers can be accounted for, the Poynting vector model is no longer
applicable. One main reason is that the photocurrent radiates low-frequency EM
waves out of the spectrum of the source, e.g., terahertz (THz) waves are
generated in THz photoconductive antennas. These frequency components do not
contribute to the photocarrier generation since the corresponding photon energy
is smaller than the optoelectronic material's bandgap energy. However, the
instantaneous Poynting vector does not distinguish the power flux of different
frequency components. This work proposes a material absorption-based model
capable of calculating the carrier generation rate accurately in the time
domain. Using the Lorentz dispersion model with poles reside in the optical
frequency region, the instantaneous optical absorption, which corresponds to
the power dissipation in the polarization, is calculated and used to calculate
the generation rate. The Lorentz model is formulated with an auxiliary
differential equation method that updates the polarization current density,
from which the absorbed optical power corresponding to each Lorentz pole is
directly calculated in the time domain. Examples show that the proposed model
is more accurate than the Poynting vector-based model and is stable even when
the generated low-frequency component is strong."
