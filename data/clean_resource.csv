link,title,description
http://arxiv.org/abs/2102.06219, Silentium! Run-Analyse-Eradicate the Noise out of the DB/OS Stack,When multiple tenants compete for resources  database performance tends tosuffer. Yet there are scenarios where guaranteed sub-millisecond latencies arecrucial  such as in real-time data processing  IoT devices  or when operatingin safety-critical environments. In this paper  we study how to make querylatencies deterministic in the face of noise (whether caused by other tenantsor unrelated operating system tasks). We perform controlled experiments with anin-memory database engine in a multi-tenant setting  where we successivelyeradicate noisy interference from within the system software stack  to thepoint where the engine runs close to bare-metal on the underlying hardware.We show that we can achieve query latencies comparable to the database enginerunning as the sole tenant  but without noticeably impacting the workload ofcompeting tenants. We discuss these results in the context of ongoing effortsto build custom operating systems for database workloads  and point out thatfor certain use cases  the margin for improvement is rather narrow. In fact for scenarios like ours  existing operating systems might just be good enough provided that they are expertly configured. We then critically discuss thesefindings in the light of a broader family of database systems (e.g.  includingdisk-based)  and how to extend the approach of this paper accordingly.
http://arxiv.org/abs/2102.06228, Learning Gaussian-Bernoulli RBMs using Difference of Convex Functions  Optimization,The Gaussian-Bernoulli restricted Boltzmann machine (GB-RBM) is a usefulgenerative model that captures meaningful features from the given n -dimensional continuous data. The difficulties associated with learningGB-RBM are reported extensively in earlier studies. They indicate that thetraining of the GB-RBM using the current standard algorithms  namely contrastive divergence (CD) and persistent contrastive divergence (PCD)  needsa carefully chosen small learning rate to avoid divergence which  in turn results in slow learning. In this work  we alleviate such difficulties byshowing that the negative log-likelihood for a GB-RBM can be expressed as adifference of convex functions if we keep the variance of the conditionaldistribution of visible units (given hidden unit states) and the biases of thevisible units  constant. Using this  we propose a stochastic {\em difference ofconvex functions} (DC) programming (S-DCP) algorithm for learning the GB-RBM.We present extensive empirical studies on several benchmark datasets tovalidate the performance of this S-DCP algorithm. It is seen that S-DCP isbetter than the CD and PCD algorithms in terms of speed of learning and thequality of the generative model learnt.
http://arxiv.org/abs/2102.06229, To Reuse or Not To Reuse? A Framework and System for Evaluating  Summarized Knowledge,As the amount of information online continues to grow  a correspondinglyimportant opportunity is for individuals to reuse knowledge which has beensummarized by others rather than starting from scratch. However  appropriatereuse requires judging the relevance  trustworthiness  and thoroughness ofothers' knowledge in relation to an individual's goals and context. In thiswork  we explore augmenting judgements of the appropriateness of reusingknowledge in the domain of programming  specifically of reusing artifacts thatresult from other developers' searching and decision making. Through ananalysis of prior research on sensemaking and trust  along with new interviewswith developers  we synthesized a framework for reuse judgements. Theinterviews also validated that developers express a desire for help withjudging whether to reuse an existing decision. From this framework  wedeveloped a set of techniques for capturing the initial decision maker'sbehavior and visualizing signals calculated based on the behavior  tofacilitate subsequent consumers' reuse decisions  instantiated in a prototypesystem called Strata. Results of a user study suggest that the systemsignificantly improves the accuracy  depth  and speed of reusing decisions.These results have implications for systems involving user-generated content inwhich other users need to evaluate the relevance and trustworthiness of thatcontent.
http://arxiv.org/abs/2102.06231, Optimization Issues in KL-Constrained Approximate Policy Iteration,Many reinforcement learning algorithms can be seen as versions of approximatepolicy iteration (API). While standard API often performs poorly  it has beenshown that learning can be stabilized by regularizing each policy update by theKL-divergence to the previous policy. Popular practical algorithms such asTRPO  MPO  and VMPO replace regularization by a constraint on KL-divergence ofconsecutive policies  arguing that this is easier to implement and tune. Inthis work  we study this implementation choice in more detail. We compare theuse of KL divergence as a constraint vs. as a regularizer  and point outseveral optimization issues with the widely-used constrained approach. We showthat the constrained algorithm is not guaranteed to converge even on simpleproblem instances where the constrained problem can be solved exactly  and infact incurs linear expected regret. With approximate implementation usingsoftmax policies  we show that regularization can improve the optimizationlandscape of the original objective. We demonstrate these issues empirically onseveral bandit and RL environments.
http://arxiv.org/abs/2102.06234, Robotic Tool Tracking under Partially Visible Kinematic Chain: A Unified  Approach,Anytime a robot manipulator is controlled via visual feedback  thetransformation between the robot and camera frame must be known. However  inthe case where cameras can only capture a portion of the robot manipulator inorder to better perceive the environment being interacted with  there isgreater sensitivity to errors in calibration of the base-to-camera transform. Asecondary source of uncertainty during robotic control are inaccuracies injoint angle measurements which can be caused by biases in positioning andcomplex transmission effects such as backlash and cable stretch. In this work we bring together these two sets of unknown parameters into a unified problemformulation when the kinematic chain is partially visible in the camera view.We prove that these parameters are non-identifiable implying that explicitestimation of them is infeasible. To overcome this  we derive a smaller set ofparameters we call Lumped Error since it lumps together the errors ofcalibration and joint angle measurements. A particle filter method is presentedand tested in simulation and on two real world robots to estimate the LumpedError and show the efficiency of this parameter reduction.
http://arxiv.org/abs/2102.06235, Deep Reinforcement Agent for Scheduling in HPC,Cluster scheduler is crucial in high-performance computing (HPC). Itdetermines when and which user jobs should be allocated to available systemresources. Existing cluster scheduling heuristics are developed by humanexperts based on their experience with specific HPC systems and workloads.However  the increasing complexity of computing systems and the highly dynamicnature of application workloads have placed tremendous burden on manuallydesigned and tuned scheduling heuristics. More aggressive optimization andautomation are needed for cluster scheduling in HPC. In this work  we presentan automated HPC scheduling agent named DRAS (Deep Reinforcement Agent forScheduling) by leveraging deep reinforcement learning. DRAS is built on anovel  hierarchical neural network incorporating special HPC schedulingfeatures such as resource reservation and backfilling. A unique trainingstrategy is presented to enable DRAS to rapidly learn the target environment.Once being provided a specific scheduling objective given by system manager DRAS automatically learns to improve its policy through interaction with thescheduling environment and dynamically adjusts its policy as workload changes.The experiments with different production workloads demonstrate that DRASoutperforms the existing heuristic and optimization approaches by up to 45%.
http://arxiv.org/abs/2102.06237, Knowledge Infused Policy Gradients for Adaptive Pandemic Control,COVID-19 has impacted nations differently based on their policyimplementations. The effective policy requires taking into account publicinformation and adaptability to new knowledge. Epidemiological models built tounderstand COVID-19 seldom provide the policymaker with the capability foradaptive pandemic control (APC). Among the core challenges to be overcomeinclude (a) inability to handle a high degree of non-homogeneity in differentcontributing features across the pandemic timeline  (b) lack of an approachthat enables adaptive incorporation of public health expert knowledge  and (c)transparent models that enable understanding of the decision-making process insuggesting policy. In this work  we take the early steps to address thesechallenges using Knowledge Infused Policy Gradient (KIPG) methods. Prior workon knowledge infusion does not handle soft and hard imposition of varying formsof knowledge in disease information and guidelines to necessarily comply with.Furthermore  the models do not attend to non-homogeneity in feature counts manifesting as partial observability in informing the policy. Additionally interpretable structures are extracted post-learning instead of learning aninterpretable model required for APC. To this end  we introduce a mathematicalframework for KIPG methods that can (a) induce relevant feature counts overmulti-relational features of the world  (b) handle latent non-homogeneouscounts as hidden variables that are linear combinations of kernelizedaggregates over the features  and (b) infuse knowledge as functionalconstraints in a principled manner. The study establishes a theory for imposinghard and soft constraints and simulates it through experiments. In comparisonwith knowledge-intensive baselines  we show quick sample efficient adaptationto new knowledge and interpretability in the learned policy  especially in apandemic context.
http://arxiv.org/abs/2102.06238, Regret  stability  and fairness in matching markets with bandit learners,We consider the two-sided matching market with bandit learners. In thestandard matching problem  users and providers are matched to ensure incentivecompatibility via the notion of stability. However  contrary to the coreassumption of the matching problem  users and providers do not know their truepreferences a priori and must learn them. To address this assumption  recentworks propose to blend the matching and multi-armed bandit problems. Theyestablish that it is possible to assign matchings that are stable (i.e. incentive-compatible) at every time step while also allowing agents to learnenough so that the system converges to matchings that are stable under theagents' true preferences. However  while some agents may incur low regret underthese matchings  others can incur high regret -- specifically   \Omega(T) optimal regret where  T  is the time horizon. In this work  we incorporatecosts and transfers in the two-sided matching market with bandit learners inorder to faithfully model competition between agents. We prove that  under ourframework  it is possible to simultaneously guarantee four desiderata: (1)incentive compatibility  i.e.  stability  (2) low regret  i.e.   O(\log(T)) optimal regret  (3) fairness in the distribution of regret among agents  and(4) high social welfare.
http://arxiv.org/abs/2102.06243, Sample-Optimal PAC Learning of Halfspaces with Malicious Noise,We study efficient PAC learning of homogeneous halfspaces in  \mathbb{R}^d in the presence of malicious noise of Valiant~(1985). This is a challengingnoise model and only until recently has near-optimal noise tolerance bound beenestablished under the mild condition that the unlabeled data distribution isisotropic log-concave. However  it remains unsettled how to obtain the optimalsample complexity simultaneously. In this work  we present a new analysis forthe algorithm of Awasthi et al.~(2017) and show that it essentially achievesthe near-optimal sample complexity bound of   ilde{O}(d)   improving the bestknown result of   ilde{O}(d^2) . Our main ingredient is a novel incorporationof a Matrix Chernoff-type inequality to bound the spectrum of an empiricalcovariance matrix for well-behaved distributions  in conjunction with a carefulexploration of the localization schemes of Awasthi et al.~(2017). We furtherextend the algorithm and analysis to the more general and stronger nasty noisemodel of Bshouty~et~al. (2002)  showing that it is still possible to achievenear-optimal noise tolerance and sample complexity in polynomial time.
http://arxiv.org/abs/2102.06245, A Survey on Ransomware: Evolution  Taxonomy  and Defense Solutions,In recent years  ransomware has been one of the most notorious malwaretargeting end users  governments  and business organizations. It has become avery profitable business for cybercriminals with revenues of millions ofdollars  and a very serious threat to organizations with financial loss ofbillions of dollars. Numerous studies were proposed to address the ransomwarethreat  including surveys that cover certain aspects of ransomware research.However  no study exists in the literature that gives the complete picture onransomware and ransomware defense research with respect to the diversity oftargeted platforms. Since ransomware is already prevalent inPCs/workstations/desktops/laptops  is becoming more prevalent in mobiledevices  and has already hit IoT/CPS recently  and will likely grow further inthe IoT/CPS domain very soon  understanding ransomware and analyzing defensemechanisms with respect to target platforms is becoming more imperative. Inorder to fill this gap and motivate further research  in this paper  we presenta comprehensive survey on ransomware and ransomware defense research withrespect to PCs/workstations  mobile devices  and IoT/CPS platforms.Specifically  covering 137 studies over the period of 1990-2020  we give adetailed overview of ransomware evolution  comprehensively analyze the keybuilding blocks of ransomware  present a taxonomy of notable ransomwarefamilies  and provide an extensive overview of ransomware defense research(i.e.  analysis  detection  and recovery) with respect to platforms ofPCs/workstations  mobile devices  and IoT/CPS. Moreover  we derive an extensivelist of open issues for future ransomware research. We believe this survey willmotivate further research by giving a complete picture on state-of-the-artransomware research.
http://arxiv.org/abs/2102.06246, Why Don't Developers Detect Improper Input Validation?'; DROP TABLE  Papers; --,Improper Input Validation (IIV) is a software vulnerability that occurs whena system does not safely handle input data. Even though IIV is easy to detectand fix  it still commonly happens in practice. In this paper  we study to whatextent developers can detect IIV and investigate underlying reasons. Thisknowledge is essential to better understand how to support developers increating secure software systems. We conduct an online experiment with 146participants  of which 105 report at least three years of professional softwaredevelopment experience. Our results show that the existence of a visible attackscenario facilitates the detection of IIV vulnerabilities and that asignificant portion of developers who did not find the vulnerability initiallycould identify it when warned about its existence. Yet  a total of 60participants could not detect the vulnerability even after the warning. Otherfactors  such as the frequency with which the participants perform codereviews  influence the detection of IIV. Data and materials:https://doi.org/10.5281/zenodo.3996696
http://arxiv.org/abs/2102.06247, Continuum: Simple Management of Complex Continual Learning Scenarios,Continual learning is a machine learning sub-field specialized in settingswith non-iid data. Hence  the training data distribution is not static anddrifts through time. Those drifts might cause interferences in the trainedmodel and knowledge learned on previous states of the data distribution mightbe forgotten. Continual learning's challenge is to create algorithms able tolearn an ever-growing amount of knowledge while dealing with data distributiondrifts.One implementation difficulty in these field is to create data loaders thatsimulate non-iid scenarios. Indeed  data loaders are a key component forcontinual algorithms. They should be carefully designed and reproducible. Smallerrors in data loaders have a critical impact on algorithm results  e.g. withbad preprocessing  wrong order of data or bad test set. Continuum is a simpleand efficient framework with numerous data loaders that avoid researcher tospend time on designing data loader and eliminate time-consuming errors. Usingour proposed framework  it is possible to directly focus on the model design byusing the multiple scenarios and evaluation metrics implemented. Furthermorethe framework is easily extendable to add novel settings for specific needs.
http://arxiv.org/abs/2102.06249, Securing RPL using Network Coding: The Chained Secure Mode (CSM),As the de facto routing protocol for many Internet of Things (IoT) networksnowadays  and to assure the confidentiality and integrity of its controlmessages  the Routing Protocol for Low Power and Lossy Networks (RPL)incorporates three modes of security: the Unsecured Mode (UM)  PreinstalledSecure Mode (PSM)  and the Authenticated Secure Mode (ASM). While the PSM andASM are intended to protect against external routing attacks and some replayattacks (through an optional replay protection mechanism)  recent researchshowed that RPL in PSM is still vulnerable to many routing attacks  bothinternal and external. In this paper  we propose a novel secure mode for RPL the Chained Secure Mode (CSM)  based on the concept of intraflow Network Coding(NC). The CSM is designed to enhance RPL resilience and mitigation capabilityagainst replay attacks while allowing the integration with external securitymeasures such as Intrusion Detection Systems (IDSs). The security andperformance of the proposed CSM were evaluated and compared against RPL in UMand PSM (with and without the optional replay protection) under several routingattacks: the Neighbor attack (NA)  Wormhole (WH)  and CloneID attack (CA) using average packet delivery rate (PDR)  End-to-End (E2E) latency  and powerconsumption as metrics. It showed that CSM has better performance and moreenhanced security than both the UM and PSM with the replay protection  whilemitigating both the NA and WH attacks and significantly reducing the effect ofthe CA in the investigated scenarios.
http://arxiv.org/abs/2102.06251, Towards DeepSentinel: An extensible corpus of labelled Sentinel-1 and -2  imagery and a general-purpose sensor-fusion semantic embedding model,Earth observation offers new insight into anthropogenic changes to nature and how these changes are effecting (and are effected by) the built environmentand the real economy. With the global availability of medium-resolution(10-30m) synthetic aperture radar (SAR) Sentinel-1 and multispectral Sentinel-2imagery  machine learning can be employed to offer these insights at scale unbiased to the reporting of companies and countries. In this paper  Iintroduce DeepSentinel  a data pipeline and experimentation framework forproducing general-purpose semantic embeddings of paired Sentinel-1 andSentinel-2 imagery. I document the development of an extensible corpus oflabelled and unlabelled imagery for the purposes of sensor fusion research.With this new dataset I develop a set of experiments applying popularself-supervision methods and encoder architectures to a land coverclassification problem. Tile2vec spatial encoding with a self-attention enabledResNet model outperforms deeper ResNet variants as well as pretraining withvariational autoencoding and contrastive loss. All supporting and derived dataand code are made publicly available.
http://arxiv.org/abs/2102.06253, Speculative Path Planning,Parallelization of A* path planning is mostly limited by the number ofpossible motions  which is far less than the level of parallelism that modernprocessors support. In this paper  we go beyond the limitations of traditionalparallelism of A* and propose Speculative Path Planning to accelerate thesearch when there are abundant idle resources. The key idea of our approach ispredicting future state expansions relying on patterns among expansions andaggressively parallelize the computations of prospective states (i.e.pre-evaluate the expensive collision checking operation of prospective nodes).This method allows us to maintain the same search order as of vanilla A* andsafeguard any optimality guarantees. We evaluate our method on variousconfigurations and show that on a machine with 32 physical cores  our methodimproves the performance around 11x and 10x on average over counterpartsingle-threaded and multi-threaded implementations respectively. The code toour paper can be found here:https://github.com/bakhshalipour/speculative-path-planning.
http://arxiv.org/abs/2102.06254, Fair Robust Assignment using Redundancy,We study the consideration of fairness in redundant assignment formulti-agent task allocation. It has recently been shown that redundantassignment of agents to tasks provides robustness to uncertainty in taskperformance. However  the question of how to fairly assign these redundantresources across tasks remains unaddressed. In this paper  we present a novelproblem formulation for fair redundant task allocation  which we cast as theoptimization of worst-case task costs under a cardinality constraint. Solvingthis problem optimally is NP-hard. Therefore  we exploit properties ofsupermodularity to propose a polynomial-time  near-optimal solution. Insupermodular redundant assignment  the use of additional agents always improvestask costs. Therefore  we provide a solution set that is  \alpha  times largerthan the cardinality constraint. This constraint relaxation enables ourapproach to achieve a super-optimal cost by using a sub-optimal assignmentsize.We derive the sub-optimality bound on this cardinality relaxation   \alpha .Additionally  we demonstrate that our algorithm performs near-optimally withoutthe cardinality relaxation. We show the algorithm in simulations of redundantassignments of robots to goal nodes on transport networks with uncertain traveltimes. Empirically  our algorithm outperforms benchmarks  scales to largeproblems  and provides improvements in both fairness and average utility.
http://arxiv.org/abs/2102.06258, On Graph Matching Using Generalized Seed Side-Information,In this paper  matching pairs of stocahstically generated graphs in thepresence of generalized seed side-information is considered. The graph matchingproblem emerges naturally in various applications such as social networkde-anonymization  image processing  DNA sequencing  and natural languageprocessing. A pair of randomly generated labeled Erdos-Renyi graphs withpairwise correlated edges are considered. It is assumed that the matchingstrategy has access to the labeling of the vertices in the first graph  as wellas a collection of shortlists -- called ambiguity sets -- of possible labelsfor the vertices of the second graph. The objective is to leverage thecorrelation among the edges of the graphs along with the side-informationprovided in the form of ambiguity sets to recover the labels of the vertices inthe second graph. This scenario can be viewed as a generalization of the seededgraph matching problem  where the ambiguity sets take a specific form such thatthe exact labels for a subset of vertices in the second graph are known priorto matching. A matching strategy is proposed which operates by evaluating thejoint typicality of the adjacency matrices of the graphs. Sufficient conditionson the edge statistics as well as ambiguity set statistics are derived underwhich the proposed matching strategy successfully recovers the labels of thevertices in the second graph. Additionally  Fano-type arguments are used toderive general necessary conditions for successful matching.
http://arxiv.org/abs/2102.06260, Selecting Treatment Effects Models for Domain Adaptation Using Causal  Knowledge,Selecting causal inference models for estimating individualized treatmenteffects (ITE) from observational data presents a unique challenge since thecounterfactual outcomes are never observed. The problem is challenged furtherin the unsupervised domain adaptation (UDA) setting where we only have accessto labeled samples in the source domain  but desire selecting a model thatachieves good performance on a target domain for which only unlabeled samplesare available. Existing techniques for UDA model selection are designed for thepredictive setting. These methods examine discriminative density ratios betweenthe input covariates in the source and target domain and do not factor in themodel's predictions in the target domain. Because of this  two models withidentical performance on the source domain would receive the same risk score byexisting methods  but in reality  have significantly different performance inthe test domain. We leverage the invariance of causal structures across domainsto propose a novel model selection metric specifically designed for ITE methodsunder the UDA setting. In particular  we propose selecting models whosepredictions of interventions' effects satisfy known causal structures in thetarget domain. Experimentally  our method selects ITE models that are morerobust to covariate shifts on several healthcare datasets  including estimatingthe effect of ventilation in COVID-19 patients from different geographiclocations.
http://arxiv.org/abs/2102.06261, Unsupervised Extractive Summarization using Pointwise Mutual Information,Unsupervised approaches to extractive summarization usually rely on a notionof sentence importance defined by the semantic similarity between a sentenceand the document. We propose new metrics of relevance and redundancy usingpointwise mutual information (PMI) between sentences  which can be easilycomputed by a pre-trained language model. Intuitively  a relevant sentenceallows readers to infer the document content (high PMI with the document)  anda redundant sentence can be inferred from the summary (high PMI with thesummary). We then develop a greedy sentence selection algorithm to maximizerelevance and minimize redundancy of extracted sentences. We show that ourmethod outperforms similarity-based methods on datasets in a range of domainsincluding news  medical journal articles  and personal anecdotes.
http://arxiv.org/abs/2102.06265, Hedging of Financial Derivative Contracts via Monte Carlo Tree Search,The construction of approximate replication strategies for derivativecontracts in incomplete markets is a key problem of financial engineering.Recently Reinforcement Learning algorithms for pricing and hedging underrealistic market conditions have attracted significant interest. Whilefinancial research mostly focused on variations of  Q -learning  in ArtificialIntelligence Monte Carlo Tree Search is the recognized state-of-the-art methodfor various planning problems  such as the games of Hex  Chess  Go ... Thisarticle introduces Monte Carlo Tree Search for the hedging of financialderivatives in realistic markets and shows that there are good reasons  both onthe theoretical and practical side  to favor it over other ReinforcementLearning methods.
http://arxiv.org/abs/2102.06267, On Agnostic PAC Learning using  \mathcal{L}_2 -polynomial Regression and  Fourier-based Algorithms,We develop a framework using Hilbert spaces as a proxy to analyze PAClearning problems with structural properties. We consider a joint Hilbert spaceincorporating the relation between the true label and the predictor under ajoint distribution  D . We demonstrate that agnostic PAC learning with 0-1 lossis equivalent to an optimization in the Hilbert space domain. With our model we revisit the PAC learning problem using methods based on least-squares suchas  \mathcal{L}_2  polynomial regression and Linial's low-degree algorithm. Westudy learning with respect to several hypothesis classes such as half-spacesand polynomial-approximated classes (i.e.  functions approximated by afixed-degree polynomial). We prove that (under some distributional assumptions)such methods obtain generalization error up to  2opt  with  opt  being theoptimal error of the class. Hence  we show the tightest bound on generalizationerror when  opt\leq 0.2 .
http://arxiv.org/abs/2102.06269, kPAM 2.0: Feedback Control for Category-Level Robotic Manipulation,In this paper  we explore generalizable  perception-to-action roboticmanipulation for precise  contact-rich tasks. In particular  we contribute aframework for closed-loop robotic manipulation that automatically handles acategory of objects  despite potentially unseen object instances andsignificant intra-category variations in shape  size and appearance. Previousapproaches typically build a feedback loop on top of a real-time 6-DOF poseestimator. However  representing an object with a parameterized transformationfrom a fixed geometric template does not capture large intra-category shapevariation. Hence we adopt the keypoint-based object representation proposed inkPAM for category-level pick-and-place  and extend it to closed-loopmanipulation policies with contact-rich tasks. We first augment keypoints withlocal orientation information. Using the oriented keypoints  we propose a novelobject-centric action representation in terms of regulating the linear/angularvelocity or force/torque of these oriented keypoints. This formulation issurprisingly versatile -- we demonstrate that it can accomplish contact-richmanipulation tasks that require precision and dexterity for a category ofobjects with different shapes  sizes and appearances  such as peg-holeinsertion for pegs and holes with significant shape variation and tightclearance. With the proposed object and action representation  our framework isalso agnostic to the robot grasp pose and initial object configuration  makingit flexible for integration and deployment.
http://arxiv.org/abs/2102.06271, Straggler-Resilient Distributed Machine Learning with Dynamic Backup  Workers,With the increasing demand for large-scale training of machine learningmodels  consensus-based distributed optimization methods have recently beenadvocated as alternatives to the popular parameter server framework. In thisparadigm  each worker maintains a local estimate of the optimal parametervector  and iteratively updates it by waiting and averaging all estimatesobtained from its neighbors  and then corrects it on the basis of its localdataset. However  the synchronization phase can be time consuming due to theneed to wait for  extit{stragglers}  i.e.  slower workers. An efficient way tomitigate this effect is to let each worker wait only for updates from thefastest neighbors before updating its local parameter. The remaining neighborsare called  extit{backup workers.} To minimize the globally training time overthe network  we propose a fully distributed algorithm to dynamically determinethe number of backup workers for each worker. We show that our algorithmachieves a linear speedup for convergence (i.e.  convergence performanceincreases linearly with respect to the number of workers). We conduct extensiveexperiments on MNIST and CIFAR-10 to verify our theoretical results.
http://arxiv.org/abs/2102.06272, A reproduction of Apple's bi-directional LSTM models for language  identification in short strings,Language Identification is the task of identifying a document's language. Forapplications like automatic spell checker selection  language identificationmust use very short strings such as text message fragments. In this work  wereproduce a language identification architecture that Apple briefly sketched ina blog post. We confirm the bi-LSTM model's performance and find that itoutperforms current open-source language identifiers. We further find that itslanguage identification mistakes are due to confusion between relatedlanguages.
http://arxiv.org/abs/2102.06274, Speech-language Pre-training for End-to-end Spoken Language  Understanding,End-to-end (E2E) spoken language understanding (SLU) can infer semanticsdirectly from speech signal without cascading an automatic speech recognizer(ASR) with a natural language understanding (NLU) module. However  pairedutterance recordings and corresponding semantics may not always be available orsufficient to train an E2E SLU model in a real production environment. In thispaper  we propose to unify a well-optimized E2E ASR encoder (speech) and apre-trained language model encoder (language) into a transformer decoder. Theunified speech-language pre-trained model (SLP) is continually enhanced onlimited labeled data from a target domain by using a conditional maskedlanguage model (MLM) objective  and thus can effectively generate a sequence ofintent  slot type  and slot value for given input speech in the inference. Theexperimental results on two public corpora show that our approach to E2E SLU issuperior to the conventional cascaded method. It also outperforms the presentstate-of-the-art approaches to E2E SLU with much less paired data.
http://arxiv.org/abs/2102.06275, Large Scale Distributed Collaborative Unlabeled Motion Planning with  Graph Policy Gradients,In this paper  we present a learning method to solve the unlabelled motionproblem with motion constraints and space constraints in 2D space for a largenumber of robots. To solve the problem of arbitrary dynamics and constraints wepropose formulating the problem as a multi-agent problem. We are able todemonstrate the scalability of our methods for a large number of robots byemploying a graph neural network (GNN) to parameterize policies for the robots.The GNN reduces the dimensionality of the problem by learning filters thataggregate information among robots locally  similar to how a convolutionalneural network is able to learn local features in an image. Additionally  byemploying a GNN we are also able to overcome the computational overhead oftraining policies for a large number of robots by first training graph filtersfor a small number of robots followed by zero-shot policy transfer to a largernumber of robots. We demonstrate the effectiveness of our framework throughvarious simulations.
http://arxiv.org/abs/2102.06277, K-Hairstyle: A Large-scale Korean hairstyle dataset for virtual hair  editing and hairstyle classification,The hair and beauty industry is one of the fastest growing industries. Thisled to the development of various applications  such as virtual hair dyeing orhairstyle translations  to satisfy the need of the customers. Although thereare several public hair datasets available for these applications  they consistof limited number of images with low resolution  which restrict theirperformance on high-quality hair editing. Therefore  we introduce a novellarge-scale Korean hairstyle dataset  K-hairstyle  256 679 with high-resolutionimages. In addition  K-hairstyle contains various hair attributes annotated byKorean expert hair stylists and hair segmentation masks. We validate theeffectiveness of our dataset by leveraging several applications  such ashairstyle translation  and hair classification and hair retrieval. Furthermore we will release K-hairstyle soon.
http://arxiv.org/abs/2102.06278, When and How Mixup Improves Calibration,In many machine learning applications  it is important for the model toprovide confidence scores that accurately captures its prediction uncertainty.Although modern learning methods have achieved great success in predictiveaccuracy  generating calibrated confidence scores remains a major challenge.Mixup  a popular yet simple data augmentation technique based on taking convexcombinations of pairs of training examples  has been empirically found tosignificantly improve confidence calibration across diverse applications.However  when and how Mixup helps calibration is still mysterious. In thispaper  we theoretically prove that Mixup improves calibration in extit{high-dimensional} settings by investigating two natural data models onclassification and regression. Interestingly  the calibration benefit of Mixupincreases as the model capacity increases. We support our theories withexperiments on common architectures and data sets. In addition  we study howMixup improves calibration in semi-supervised learning. While incorporatingunlabeled data can sometimes make the model less calibrated  adding Mixuptraining mitigates this issue and provably improves calibration. Our analysisprovides new insights and a framework to understand Mixup and calibration.
http://arxiv.org/abs/2102.06279, A Multi-View Approach To Audio-Visual Speaker Verification,Although speaker verification has conventionally been an audio-only task some practical applications provide both audio and visual streams of input. Inthese cases  the visual stream provides complementary information and can oftenbe leveraged in conjunction with the acoustics of speech to improveverification performance. In this study  we explore audio-visual approaches tospeaker verification  starting with standard fusion techniques to learn jointaudio-visual (AV) embeddings  and then propose a novel approach to handlecross-modal verification at test time. Specifically  we investigate unimodaland concatenation based AV fusion and report the lowest AV equal error rate(EER) of 0.7% on the VoxCeleb1 dataset using our best system. As these methodslack the ability to do cross-modal verification  we introduce a multi-viewmodel which uses a shared classifier to map audio and video into the samespace. This new approach achieves 28% EER on VoxCeleb1 in the challengingtesting condition of cross-modal verification.
http://arxiv.org/abs/2102.06280, Improving Fault Localization by Integrating Value and Predicate Based  Causal Inference Techniques,Statistical fault localization (SFL) techniques use execution profiles andsuccess/failure information from software executions  in conjunction withstatistical inference  to automatically score program elements based on howlikely they are to be faulty. SFL techniques typically employ one type ofprofile data: either coverage data  predicate outcomes  or variable values.Most SFL techniques actually measure correlation  not causation  betweenprofile values and success/failure  and so they are subject to confounding biasthat distorts the scores they produce. This paper presents a new SFL technique named \emph{UniVal}  that uses causal inference techniques and machine learningto integrate information about both predicate outcomes and variable values tomore accurately estimate the true failure-causing effect of program statements.\emph{UniVal} was empirically compared to several coverage-based predicate-based  and value-based SFL techniques on 800 program versions withreal faults.
http://arxiv.org/abs/2102.06282, No-Regret Algorithms for Time-Varying Bayesian Optimization,In this paper  we consider the time-varying Bayesian optimization problem.The unknown function at each time is assumed to lie in an RKHS (reproducingkernel Hilbert space) with a bounded norm. We adopt the general variationbudget model to capture the time-varying environment  and the variation ischaracterized by the change of the RKHS norm. We adapt the restart and slidingwindow mechanism to introduce two GP-UCB type algorithms: R-GP-UCB andSW-GP-UCB  respectively. We derive the first (frequentist) regret guarantee onthe dynamic regret for both algorithms. Our results not only recover previouslinear bandit results when a linear kernel is used  but complement the previousregret analysis of time-varying Gaussian process bandit under a Bayesian-typeregularity assumption  i.e.  each function is a sample from a Gaussian process.
http://arxiv.org/abs/2102.06283, I Know What You Imported Last Summer: A study of security threats in  thePython ecosystem,The popularity of Python has risen rapidly over the past 15 years. It is amajor language in some of the most exciting technologies today. This popularityhas led to a large ecosystem of third-party packages available via the pippackage registry which hosts more than 200 000 packages. These third-partypackages can be reused by simply importing the package after installing usingpackage managers like pip. The ease of reuse of third-party software comes withsecurity risks putting millions of users in danger. In this project  we studythe ecosystem to analyze this threat. The mature ecosystem of Python hasmultiple weak spots that we highlight in our project. First  we demonstrate howtrivial it is to exploit the Python ecosystem. Then  we systematically analyzedependencies amongst packages  maintainers  and publicly reported securityissues. Most attacks are possible only if users install malicious packages. Wethus try to analyze and evaluate different methods used by attackers to forceincorrect downloads. We quantify your ideas by estimating the potential threatthat can be caused by exploiting a popular Python package. We also discussmethods used in the industry to defend against such attacks
http://arxiv.org/abs/2102.06284, What does LIME really see in images?,The performance of modern algorithms on certain computer vision tasks such asobject recognition is now close to that of humans. This success was achieved atthe price of complicated architectures depending on millions of parameters andit has become quite challenging to understand how particular predictions aremade. Interpretability methods propose to give us this understanding. In thispaper  we study LIME  perhaps one of the most popular. On the theoretical side we show that when the number of generated examples is large  LIME explanationsare concentrated around a limit explanation for which we give an explicitexpression. We further this study for elementary shape detectors and linearmodels. As a consequence of this analysis  we uncover a connection between LIMEand integrated gradients  another explanation method. More precisely  the LIMEexplanations are similar to the sum of integrated gradients over thesuperpixels used in the preprocessing step of LIME.
http://arxiv.org/abs/2102.06285, Does Culture Matter? Impact of Individualism and Uncertainty Avoidance  on App Reviews,Mobile applications are often used by an international audience and thereforereceive a high daily amount of user reviews from various countries. Previouswork found evidence that app store reviews contain helpful information forsoftware evolution processes. However  the cultural diversity of the reviewsand its consequences on specific user feedback characteristics has only beenresearched to a limited extent so far. In this paper  we examine the influenceof two cultural dimensions  Individualism and Uncertainty Avoidance on userfeedback in Apple app store reviews written in different languages. For thispurpose  we collected 647 141 reviews from eight countries and written in fivelanguages over a period of six months. We then used manual content analysis andautomated processing to examine a sample of 3 120 reviews. The results showthat there is a statistically significant influence of Individualism andUncertainty Avoidance on user feedback characteristics. The results of thisstudy will help researchers and practitioners to reduce algorithm bias causedby less diversified training and test data and to raise awareness of theimportance of analyzing diversified user feedback.
http://arxiv.org/abs/2102.06288, Embracing Domain Differences in Fake News: Cross-domain Fake News  Detection using Multi-modal Data,With the rapid evolution of social media  fake news has become a significantsocial problem  which cannot be addressed in a timely manner using manualinvestigation. This has motivated numerous studies on automating fake newsdetection. Most studies explore supervised training models with differentmodalities (e.g.  text  images  and propagation networks) of news records toidentify fake news. However  the performance of such techniques generally dropsif news records are coming from different domains (e.g.  politics entertainment)  especially for domains that are unseen or rarely-seen duringtraining. As motivation  we empirically show that news records from differentdomains have significantly different word usage and propagation patterns.Furthermore  due to the sheer volume of unlabelled news records  it ischallenging to select news records for manual labelling so that thedomain-coverage of the labelled dataset is maximized. Hence  this work: (1)proposes a novel framework that jointly preserves domain-specific andcross-domain knowledge in news records to detect fake news from differentdomains; and (2) introduces an unsupervised technique to select a set ofunlabelled informative news records for manual labelling  which can beultimately used to train a fake news detection model that performs well formany domains while minimizing the labelling cost. Our experiments show that theintegration of the proposed fake news model and the selective annotationapproach achieves state-of-the-art performance for cross-domain news datasets while yielding notable improvements for rarely-appearing domains in newsdatasets.
http://arxiv.org/abs/2102.06289, Understanding the attitudes  knowledge sharing behaviors and task  performance of core developers: A longitudinal study,Context: Prior research has established that a few individuals generallydominate project communication and source code changes during softwaredevelopment  regardless of task assignments at project initiation. Objective:While this phenomenon has been noted  prior research has not sought tounderstand these dominant individuals. Previous work has found that corecommunicators are the gatekeepers of their teams' knowledge  and theperformance of these members was correlated with their teams' success. Buildingon this work  we have employed a longitudinal approach to study the way coredevelopers' attitudes  knowledge sharing behaviors and task performance changeover the course of their project. Method: We first used Social Network Analysis(SNA) and standard statistical analysis techniques to identify and selectartifacts and central practitioners from ten different software developmentteams. We then applied psycholinguistic analysis and directed content analysis(CA) techniques to interpret the content of these practitioners' messages.Finally  we inspected core developers' activities at various points in timeduring systems' development. Results: Among our findings  we observe that coredevelopers' attitudes and knowledge sharing behaviors were linked to theirinvolvement in actual software development and the demands of their widerproject teams. However  core developers appeared to naturally possess highlevels of insightful characteristics. Conclusion: Project performance wouldlikely benefit from strategies aimed at surrounding core developers with othercompetent communicators. Core developers should also be supported by a widerteam who are willing to ask questions and challenge their ideas. Finally  theavailability of adequate communication channels would help with maintainingpositive team climate especially in distributed developments.(Abridged)
http://arxiv.org/abs/2102.06291, Relating IS Developers' Attitudes to Engagement,Increasing effort is being directed to understanding the personality profilesof highly engaged information systems (IS) developers and the impact of suchprofiles on development outcomes. However  there has been a lesser degree ofattention paid to studying attitudes at a fine-grained level  and relating suchattitudes to developers' in-process activities  in spite of the fact thatsocial motivation theory notes the importance of such a relationship in generalgroup work. We have therefore applied linguistic analysis  text mining andvisualization  and statistical analysis techniques to artefacts developed by474 developers to study these issues. Our results indicate that our sample ofIS developers conveyed a range of attitudes while working to deliver systemsfeatures  and those practitioners who communicated the most were also the mostengaged. Additionally  of eight linguistic dimensions considered  expressionsregarding work and achievement  as well as insightful attitudes  were mostclosely related to developers' engagement. Accordingly  team diversity and theprovision of active support for outcome-driven developers may contributepositively to maintaining team balance and performance.
http://arxiv.org/abs/2102.06292, On Automatic Parsing of Log Records,Software log analysis helps to maintain the health of software solutions andensure compliance and security. Existing software systems consist ofheterogeneous components emitting logs in various formats. A typical solutionis to unify the logs using manually built parsers  which is laborious.Instead  we explore the possibility of automating the parsing task byemploying machine translation (MT). We create a tool that generates syntheticApache log records which we used to train recurrent-neural-network-based MTmodels. Models' evaluation on real-world logs shows that the models can learnApache log format and parse individual log records. The median relative editdistance between an actual real-world log record and the MT prediction is lessthan or equal to 28%. Thus  we show that log parsing using an MT approach ispromising.
http://arxiv.org/abs/2102.06296, Verifying High-Level Latency-Insensitive Designs with Formal Model  Checking,Latency-insensitive design mitigates increasing interconnect delay andenables productive component reuse in complex digital systems. This designstyle has been adopted in high-level design flows because untimed functionalblocks connected through latency-insensitive interfaces provide a naturalcommunication abstraction. However  latency-insensitive design with high-levellanguages also introduces a unique set of verification challenges thatjeopardize functional correctness. In particular  bugs due to invalidconsumption of inputs and deadlocks can be difficult to detect and debug withdynamic simulation methods. To tackle these two classes of bugs  we proposeformal model checking methods to guarantee that a high-levellatency-insensitive design is unaffected by invalid input data and is free ofdeadlock. We develop a well-structured verification wrapper for each propertyto automatically construct the corresponding formal model for checking. Ourexperiments demonstrate that the formal checks are effective in realistic bugscenarios from high-level designs.
http://arxiv.org/abs/2102.06301, ReRankMatch: Semi-Supervised Learning with Semantics-Oriented Similarity  Representation,This paper proposes integrating semantics-oriented similarity representationinto RankingMatch  a recently proposed semi-supervised learning method. Ourmethod  dubbed ReRankMatch  aims to deal with the case in which labeled andunlabeled data share non-overlapping categories. ReRankMatch encourages themodel to produce the similar image representations for the samples likelybelonging to the same class. We evaluate our method on various datasets such asCIFAR-10  CIFAR-100  SVHN  STL-10  and Tiny ImageNet. We obtain promisingresults (4.21% error rate on CIFAR-10 with 4000 labels  22.32% error rate onCIFAR-100 with 10000 labels  and 2.19% error rate on SVHN with 1000 labels)when the amount of labeled data is sufficient to learn semantics-orientedsimilarity representation.
http://arxiv.org/abs/2102.06304, Stragglers Are Not Disaster: A Hybrid Federated Learning Algorithm with  Delayed Gradients,Federated learning (FL) is a new machine learning framework which trains ajoint model across a large amount of decentralized computing devices. Existingmethods  e.g.  Federated Averaging (FedAvg)  are able to provide anoptimization guarantee by synchronously training the joint model  but usuallysuffer from stragglers  i.e.  IoT devices with low computing power orcommunication bandwidth  especially on heterogeneous optimization problems. Tomitigate the influence of stragglers  this paper presents a novel FL algorithm namely Hybrid Federated Learning (HFL)  to achieve a learning balance inefficiency and effectiveness. It consists of two major components: synchronouskernel and asynchronous updater. Unlike traditional synchronous FL methods  ourHFL introduces the asynchronous updater which actively pulls unsynchronized anddelayed local weights from stragglers. An adaptive approximation method Adaptive Delayed-SGD (AD-SGD)  is proposed to merge the delayed local updatesinto the joint model. The theoretical analysis of HFL shows that theconvergence rate of the proposed algorithm is  \mathcal{O}(\frac{1}{t+ au}) for both convex and non-convex optimization problems.
http://arxiv.org/abs/2102.06306, Efficient Algorithms for Federated Saddle Point Optimization,We consider strongly convex-concave minimax problems in the federatedsetting  where the communication constraint is the main bottleneck. Whenclients are arbitrarily heterogeneous  a simple Minibatch Mirror-prox achievesthe best performance. As the clients become more homogeneous  using multiplelocal gradient updates at the clients significantly improves upon MinibatchMirror-prox by communicating less frequently. Our goal is to design analgorithm that can harness the benefit of similarity in the clients whilerecovering the Minibatch Mirror-prox performance under arbitrary heterogeneity(up to log factors). We give the first federated minimax optimization algorithmthat achieves this goal. The main idea is to combine (i) SCAFFOLD (an algorithmthat performs variance reduction across clients for convex optimization) toerase the worst-case dependency on heterogeneity and (ii) Catalyst (a frameworkfor acceleration based on modifying the objective) to accelerate convergencewithout amplifying client drift. We prove that this algorithm achieves ourgoal  and include experiments to validate the theory.
http://arxiv.org/abs/2102.06307, Dancing along Battery: Enabling Transformer with Run-time  Reconfigurability on Mobile Devices,A pruning-based AutoML framework for run-time reconfigurability  namely RT3 is proposed in this work. This enables Transformer-based large Natural LanguageProcessing (NLP) models to be efficiently executed on resource-constrainedmobile devices and reconfigured (i.e.  switching models for dynamic hardwareconditions) at run-time. Such reconfigurability is the key to save energy forbattery-powered mobile devices  which widely use dynamic voltage and frequencyscaling (DVFS) technique for hardware reconfiguration to prolong battery life.In this work  we creatively explore a hybrid block-structured pruning (BP) andpattern pruning (PP) for Transformer-based models and first attempt to combinehardware and software reconfiguration to maximally save energy forbattery-powered mobile devices. Specifically  RT3 integrates two-leveloptimizations: First  it utilizes an efficient BP as the first-step compressionfor resource-constrained mobile devices; then  RT3 heuristically generates ashrunken search space based on the first level optimization and searchesmultiple pattern sets with diverse sparsity for PP via reinforcement learningto support lightweight software reconfiguration  which corresponds to availablefrequency levels of DVFS (i.e.  hardware reconfiguration). At run-time  RT3 canswitch the lightweight pattern sets within 45ms to guarantee the requiredreal-time constraint at different frequency levels. Results further show thatRT3 can prolong battery life over 4x improvement with less than 1% accuracyloss for Transformer and 1.5% score decrease for DistilBERT.
http://arxiv.org/abs/2102.06311, Personalized Visualization Recommendation,Visualization recommendation work has focused solely on scoringvisualizations based on the underlying dataset and not the actual user andtheir past visualization feedback. These systems recommend the samevisualizations for every user  despite that the underlying user interests intent  and visualization preferences are likely to be fundamentally different yet vitally important. In this work  we formally introduce the problem ofpersonalized visualization recommendation and present a generic learningframework for solving it. In particular  we focus on recommendingvisualizations personalized for each individual user based on their pastvisualization interactions (e.g.  viewed  clicked  manually created) along withthe data from those visualizations. More importantly  the framework can learnfrom visualizations relevant to other users  even if the visualizations aregenerated from completely different datasets. Experiments demonstrate theeffectiveness of the approach as it leads to higher quality visualizationrecommendations tailored to the specific user intent and preferences. Tosupport research on this new problem  we release our user-centric visualizationcorpus consisting of 17.4k users exploring 94k datasets with 2.3 millionattributes and 32k user-generated visualizations.
http://arxiv.org/abs/2102.06314, Generating cryptographically-strong random lattice bases and recognizing  rotations of  \mathbb{Z}^n ,Lattice-based cryptography relies on generating random bases which aredifficult to fully reduce. Given a lattice basis (such as the private basis fora cryptosystem)  all other bases are related by multiplication by matrices in GL(n \mathbb{Z}) . How can one sample random elements from  GL(n \mathbb{Z}) ?We consider various methods  finding some are stronger than others with respectto the problem of recognizing rotations of the  \mathbb{Z}^n  lattice. Inparticular  the standard algorithm of multiplying unipotent generators together(as implemented in Magma's RandomSLnZ command) generates instances of this lastproblem which can be efficiently broken  even in dimensions nearing 1 500.Similar weaknesses for this problem are found with the random basis generationmethod in one of the NIST Post-Quantum Cryptography competition submissions(DRS). Other algorithms are described which appear to be much stronger.
http://arxiv.org/abs/2102.06315, A Visual Analysis Approach to Update Systematic Reviews,Context: In order to preserve the value of Systematic Reviews (SRs)  theyshould be frequently updated considering new evidence that has been producedsince the completion of the previous version of the reviews. However  theupdate of an SR is a time consuming  manual task. Thus  many SRs have not beenupdated as they should be and  therefore  they are currently outdated.Objective: The main contribution of this paper is to support the update of SRs.Method: We propose USR-VTM  an approach based on Visual Text Mining (VTM)techniques  to support selection of new evidence in the form of primarystudies. We then present a tool  named Revis  which supports our approach.Finally  we evaluate our approach through a comparison of outcomes achievedusing USR-VTM versus the traditional (manual) approach. Results: Our resultsshow that USR-VTM increases the number of studies correctly included comparedto the traditional approach. Conclusions: USR-VTM effectively supports theupdate of SRs.
http://arxiv.org/abs/2102.06317, Physics-Informed Graphical Neural Network for Parameter & State  Estimations in Power Systems,Parameter Estimation (PE) and State Estimation (SE) are the most wide-spreadtasks in the system engineering. They need to be done automatically  fast andfrequently  as measurements arrive. Deep Learning (DL) holds the promise oftackling the challenge  however in so far  as PE and SE in power systems isconcerned  (a) DL did not win trust of the system operators because of the lackof the physics of electricity based  interpretations and (b) DL remainedillusive in the operational regimes were data is scarce. To address this  wepresent a hybrid scheme which embeds physics modeling of power systems intoGraphical Neural Networks (GNN)  therefore empowering system operators with areliable and explainable real-time predictions which can then be used tocontrol the critical infrastructure. To enable progress towards trustworthy DLfor PE and SE  we build a physics-informed method  named Power-GNN  whichreconstructs physical  thus interpretable  parameters within Effective PowerFlow (EPF) models  such as admittances of effective power lines  and NNparameters  representing implicitly unobserved elements of the system. In ourexperiments  we test the Power-GNN on different realistic power networks including these with thousands of loads and hundreds of generators. We showthat the Power-GNN outperforms vanilla NN scheme unaware of the EPF physics.
http://arxiv.org/abs/2102.06318, Projected Wasserstein gradient descent for high-dimensional Bayesian  inference,We propose a projected Wasserstein gradient descent method (pWGD) forhigh-dimensional Bayesian inference problems. The underlying density functionof a particle system of WGD is approximated by kernel density estimation (KDE) which faces the long-standing curse of dimensionality. We overcome thischallenge by exploiting the intrinsic low-rank structure in the differencebetween the posterior and prior distributions. The parameters are projectedinto a low-dimensional subspace to alleviate the approximation error of KDE inhigh dimensions. We formulate a projected Wasserstein gradient flow and analyzeits convergence property under mild assumptions. Several numerical experimentsillustrate the accuracy  convergence  and complexity scalability of pWGD withrespect to parameter dimension  sample size  and processor cores.
http://arxiv.org/abs/2102.06320, An approximation method for electromagnetic wave models based on  fractional partial derivative,The present article is devoting a numerical approach for solving a fractionalpartial differential equation (FPDE) arising from electromagnetic waves indielectric media (EMWDM). The truncated Bernoulli and Hermite wavelets serieswith unknown coefficients have been used to approximate the solution in boththe temporal and spatial variables. The basic idea for discretizing the FPDE iswavelet approximation based on the Bernoulli and Hermite wavelets operationalmatrices of integration and differentiation. The resulted system of a linearalgebraic equation has been solved by the collocation method. Moreover convergence and error analysis have been discussed. Finally  several numericalexperiments with different fractional-order derivatives have been provided andcompared with the exact analytical solutions to illustrate the accuracy andefficiency of the method.
http://arxiv.org/abs/2102.06322, Same File  Different Changes: The Potential of Meta-Maintenance on  GitHub,Online collaboration platforms such as GitHub have provided softwaredevelopers with the ability to easily reuse and share code betweenrepositories. With clone-and-own and forking becoming prevalent  maintainingthese shared files is important  especially for keeping the most up-to-dateversion of reused code. Different to related work  we propose the concept ofmeta-maintenance -- i.e.  tracking how the same files evolve in differentrepositories with the aim to provide useful maintenance opportunities to thosefiles. We conduct an exploratory study by analyzing repositories from sevendifferent programming languages to explore the potential of meta-maintenance.Our results indicate that a majority of active repositories on GitHub containsat least one file which is also present in another repository  and that asignificant minority of these files are maintained differently in the differentrepositories which contain them. We manually analyzed a representative sampleof shared files and their variants to understand which changes might be usefulfor meta-maintenance. Our findings support the potential of meta-maintenanceand open up avenues for future work to capitalize on this potential.
http://arxiv.org/abs/2102.06326, A Large Batch Optimizer Reality Check: Traditional  Generic Optimizers  Suffice Across Batch Sizes,Recently the LARS and LAMB optimizers have been proposed for training neuralnetworks faster using large batch sizes. LARS and LAMB add layer-wisenormalization to the update rules of Heavy-ball momentum and Adam respectively  and have become popular in prominent benchmarks and deep learninglibraries. However  without fair comparisons to standard optimizers  it remainsan open question whether LARS and LAMB have any benefit over traditional generic algorithms. In this work we demonstrate that standard optimizationalgorithms such as Nesterov momentum and Adam can match or exceed the resultsof LARS and LAMB at large batch sizes. Our results establish new  strongerbaselines for future comparisons at these batch sizes and shed light on thedifficulties of comparing optimizers for neural network training moregenerally.
http://arxiv.org/abs/2102.06328, Contrastive Unsupervised Learning for Speech Emotion Recognition,Speech emotion recognition (SER) is a key technology to enable more naturalhuman-machine communication. However  SER has long suffered from a lack ofpublic large-scale labeled datasets. To circumvent this problem  we investigatehow unsupervised representation learning on unlabeled datasets can benefit SER.We show that the contrastive predictive coding (CPC) method can learn salientrepresentations from unlabeled datasets  which improves emotion recognitionperformance. In our experiments  this method achieved state-of-the-artconcordance correlation coefficient (CCC) performance for all emotionprimitives (activation  valence  and dominance) on IEMOCAP. Additionally  onthe MSP- Podcast dataset  our method obtained considerable performanceimprovements compared to baselines.
http://arxiv.org/abs/2102.06329, Min-Max-Plus Neural Networks,We present a new model of neural networks called Min-Max-Plus Neural Networks(MMP-NNs) based on operations in tropical arithmetic. In general  an MMP-NN iscomposed of three types of alternately stacked layers  namely linear layers min-plus layers and max-plus layers. Specifically  the latter two types oflayers constitute the nonlinear part of the network which is trainable and moresophisticated compared to the nonlinear part of conventional neural networks.In addition  we show that with higher capability of nonlinearity expression MMP-NNs are universal approximators of continuous functions  even when thenumber of multiplication operations is tremendously reduced (possibly to nonein certain extreme cases). Furthermore  we formulate the backpropagationalgorithm in the training process of MMP-NNs and introduce an algorithm ofnormalization to improve the rate of convergence in training.
http://arxiv.org/abs/2102.06333, DeepPseudo: Deep Pseudo-code Generation via Transformer and Code Feature  Extraction,Pseudo-code written by natural language is helpful for novice developers'program comprehension. However  writing such pseudo-code is time-consuming andlaborious. Motivated by the research advancements of sequence-to-sequencelearning and code semantic learning  we propose a novel deep pseudo-codegeneration method DeepPseudo via Transformer and code feature extraction. Inparticular  DeepPseudo utilizes both Transformer encoder and code featureextractor to perform encoding for source code. Then it uses a pseudo-codegenerator to perform decoding  which can generate the correspondingpseudo-code. We choose corpus gathered from a web application framework Django which contains 18 805 pairs of Python statements and corresponding pseudo-code.We first compare DeepPseudo with seven baselines from pseudo-code generationand neural machine translation domains in terms of four performance measures.Results show the competitiveness of DeepPseudo. Later  we analyze therationality of the component settings (i.e.  the usage of code featureextractor  the attention mechanism  and the positional encoding method) inDeepPseudo. Finally  we perform a human study to verify the effectiveness ofDeepPseudo.
http://arxiv.org/abs/2102.06336, SCOUT: Socially-COnsistent and UndersTandable Graph Attention Network  for Trajectory Prediction of Vehicles and VRUs,Autonomous vehicles navigate in dynamically changing environments under awide variety of conditions  being continuously influenced by surroundingobjects. Modelling interactions among agents is essential for accuratelyforecasting other agents' behaviour and achieving safe and comfortable motionplanning. In this work  we propose SCOUT  a novel Attention-based Graph NeuralNetwork that uses a flexible and generic representation of the scene as a graphfor modelling interactions  and predicts socially-consistent trajectories ofvehicles and Vulnerable Road Users (VRUs) under mixed traffic conditions. Weexplore three different attention mechanisms and test our scheme with bothbird-eye-view and on-vehicle urban data  achieving superior performance thanexisting state-of-the-art approaches on InD and ApolloScape Trajectorybenchmarks. Additionally  we evaluate our model's flexibility andtransferability by testing it under completely new scenarios on RounD dataset.The importance and influence of each interaction in the final prediction isexplored by means of Integrated Gradients technique and the visualization ofthe attention learned.
http://arxiv.org/abs/2102.06343, A Decentralized Approach Towards Responsible AI in Social Ecosystems,For AI technology to fulfill its full promises  we must design effectivemechanisms into the AI systems to support responsible AI behavior and curtailpotential irresponsible use  e.g. in areas of privacy protection  humanautonomy  robustness  and prevention of biases and discrimination in automateddecision making. In this paper  we present a framework that providescomputational facilities for parties in a social ecosystem to produce thedesired responsible AI behaviors. To achieve this goal  we analyze AI systemsat the architecture level and propose two decentralized cryptographicmechanisms for an AI system architecture: (1) using Autonomous Identity toempower human users  and (2) automating rules and adopting conventions withinsocial institutions. We then propose a decentralized approach and outline thekey concepts and mechanisms based on Decentralized Identifier (DID) andVerifiable Credentials (VC) for a general-purpose computational infrastructureto realize these mechanisms. We argue the case that a decentralized approach isthe most promising path towards Responsible AI from both the computer scienceand social science perspectives.
http://arxiv.org/abs/2102.06344, Distributed Source Coding with Encryption Using Correlated Keys,We pose and investigate the distributed secure source coding based on thecommon key cryptosystem. This cryptosystem includes the secrecy amplificationproblem for distributed encrypted sources with correlated keys usingpost-encryption-compression  which was posed investigated by Santoso andOohama. In this paper we propose another new security criterion which isgenerally more strict compared to the commonly used security criterion which isbased on the upper-bound of mutual information between the plaintext and theciphertext. Under this criterion  we establish the necessary and sufficientcondition for the secure transmission of correlated sources.
http://arxiv.org/abs/2102.06345, Software Estimations Risk in Pakistan Software Industry,Software and IT industry in Pakistan have seen a dramatic growth and successin past few years and is expected to get doubled by 2020  according to aresearch. Software development life cycle comprises of multiple phases activities and techniques that can lead to successful projects  and softwareevaluation is one of the vital and important parts of that. Software estimationcan alone be the reason of product success factor or the products failurefactor. To estimate the right cost  effort and resources is an art. But it isalso very important to include the risks that may arise in the in a softwareproject which can affect your estimates. In this paper  we highlight how therisks in Pakistan Software Industry can affect the estimates and how tomitigate them.
http://arxiv.org/abs/2102.06349, Dynamic Precision Analog Computing for Neural Networks,Analog electronic and optical computing exhibit tremendous advantages overdigital computing for accelerating deep learning when operations are executedat low precision. In this work  we derive a relationship between analogprecision  which is limited by noise  and digital bit precision. We proposeextending analog computing architectures to support varying levels of precisionby repeating operations and averaging the result  decreasing the impact ofnoise. Such architectures enable programmable tradeoffs between precision andother desirable performance metrics such as energy efficiency or throughput. Toutilize dynamic precision  we propose a method for learning the precision ofeach layer of a pre-trained model without retraining network weights. Weevaluate this method on analog architectures subject to a variety of noisesources such as shot noise  thermal noise  and weight noise and find thatemploying dynamic precision reduces energy consumption by up to 89% forcomputer vision models such as Resnet50 and by 24% for natural languageprocessing models such as BERT. In one example  we apply dynamic precision to ashot-noise limited homodyne optical neural network and simulate inference at anoptical energy consumption of 2.7 aJ/MAC for Resnet50 and 1.6 aJ/MAC for BERTwith <2% accuracy degradation.
http://arxiv.org/abs/2102.06350, Confounding Tradeoffs for Neural Network Quantization,Many neural network quantization techniques have been developed to decreasethe computational and memory footprint of deep learning. However  these methodsare evaluated subject to confounding tradeoffs that may affect inferenceacceleration or resource complexity in exchange for higher accuracy. In thiswork  we articulate a variety of tradeoffs whose impact is often overlooked andempirically analyze their impact on uniform and mixed-precision post-trainingquantization  finding that these confounding tradeoffs may have a larger impacton quantized network accuracy than the actual quantization methods themselves.Because these tradeoffs constrain the attainable hardware acceleration fordifferent use-cases  we encourage researchers to explicitly report these designchoices through the structure of  quantization cards.  We expect quantizationcards to help researchers compare methods more effectively and engineersdetermine the applicability of quantization techniques for their hardware.
http://arxiv.org/abs/2102.06352, Multiplex Bipartite Network Embedding using Dual Hypergraph  Convolutional Networks,A bipartite network is a graph structure where nodes are from two distinctdomains and only inter-domain interactions exist as edges. A large number ofnetwork embedding methods exist to learn vectorial node representations fromgeneral graphs with both homogeneous and heterogeneous node and edge types including some that can specifically model the distinct properties of bipartitenetworks. However  these methods are inadequate to model multiplex bipartitenetworks (e.g.  in e-commerce)  that have multiple types of interactions (e.g. click  inquiry  and buy) and node attributes. Most real-world multiplexbipartite networks are also sparse and have imbalanced node distributions thatare challenging to model. In this paper  we develop an unsupervised DualHyperGraph Convolutional Network (DualHGCN) model that scalably transforms themultiplex bipartite network into two sets of homogeneous hypergraphs and usesspectral hypergraph convolutional operators  along with intra- andinter-message passing strategies to promote information exchange within andacross domains  to learn effective node embedding. We benchmark DualHGCN usingfour real-world datasets on link prediction and node classification tasks. Ourextensive experiments demonstrate that DualHGCN significantly outperformsstate-of-the-art methods  and is robust to varying sparsity levels andimbalanced node distributions.
http://arxiv.org/abs/2102.06355, Speculating Ineffective UI Exploration via Trace Analysis,With the prosperity of mobile apps  quality assurance of mobile apps becomescrucially important. Automated mobile User Interface (UI) testing had arisen asa key technique for app quality assurance. However  despite years of efforts existing mobile UI testing techniques still cannot achieve high code coverage especially for industrial-quality apps. To substantially improve the efficacyof mobile UI testing  we investigate state-of-the-art techniques and find afundamental limitation--each testing technique attempts to apply one predefinedstrategy to explore the UI space of all mobile apps. However  we observe thatdifferent UI design characteristics require customized UI explorationstrategies in practice. With this finding in mind  in this paper  we propose anew direction for mobile UI testing--automatic customization of UI explorationstrategies for each app under test. As a first step in this direction  wetarget ineffective exploration behavior  which refers to cases where UI testingtools fail to make progress effectively. We present Vet as a general frameworkfor applying the idea of trace analysis on UI testing history to identifyineffective exploration behavior for a given UI testing tool on a given app.Vet embraces specialized algorithms for speculating subsequences in the tracethat manifest ineffective exploration behavior of UI space exploration. Vetthen enables enhancing the testing tool by guiding the exploration to avoidineffective exploration. We evaluate Vet by applying it to threestate-of-the-art Android UI testing tools. Vet locates ineffective explorationbehaviors that reveal various tool-app applicability issues hindering testingefficacy. Vet automatically fixes the applicability issues and achieves up to46.8% code coverage relative improvements on 11 of 15 industrial-quality appsunder evaluation.
http://arxiv.org/abs/2102.06356, Neural Inverse Text Normalization,While there have been several contributions exploring state of the arttechniques for text normalization  the problem of inverse text normalization(ITN) remains relatively unexplored. The best known approaches leverage finitestate transducer (FST) based models which rely on manually curated rules andare hence not scalable. We propose an efficient and robust neural solution forITN leveraging transformer based seq2seq models and FST-based textnormalization techniques for data preparation. We show that this can be easilyextended to other languages without the need for a linguistic expert tomanually curate them. We then present a hybrid framework for integrating NeuralITN with an FST to overcome common recoverable errors in productionenvironments. Our empirical evaluations show that the proposed solutionminimizes incorrect perturbations (insertions  deletions and substitutions) toASR output and maintains high quality even on out of domain data. A transformerbased model infused with pretraining consistently achieves a lower WER acrossseveral datasets and is able to outperform baselines on English  Spanish German and Italian datasets.
http://arxiv.org/abs/2102.06357, Uncertainty-of-Information Scheduling: A Restless Multi-armed Bandit  Framework,This paper proposes using the uncertainty of information (UoI)  measured byShannon's entropy  as a metric for information freshness. We consider a systemin which a central monitor observes multiple binary Markov processes through acommunication channel. The UoI of a Markov process corresponds to the monitor'suncertainty about its state. At each time step  only one Markov process can beselected to update its state to the monitor; hence there is a tradeoff amongthe UoIs of the processes that depend on the scheduling policy used to selectthe process to be updated. The age of information (AoI) of a processcorresponds to the time since its last update. In general  the associated UoIcan be a non-increasing function  or even an oscillating function  of its AoI making the scheduling problem particularly challenging. This paper investigatesscheduling policies that aim to minimize the average sum-UoI of the processesover the infinite time horizon. We formulate the problem as a restlessmulti-armed bandit (RMAB) problem  and develop a Whittle index policy that isnear-optimal for the RMAB after proving its indexability. We further provide aniterative algorithm to compute the Whittle index for the practical deploymentof the policy. Although this paper focuses on UoI scheduling  our results applyto a general class of RMABs for which the UoI scheduling problem is a specialcase. Specifically  this paper's Whittle index policy is valid for any RMAB inwhich the bandits are binary Markov processes and the penalty is a concavefunction of the belief state of the Markov process. Numerical resultsdemonstrate the excellent performance of the Whittle index policy for thisclass of RMABs.
http://arxiv.org/abs/2102.06358, The Symmetry between Bandits and Knapsacks: A Primal-Dual LP-based  Approach,In this paper  we study the bandits with knapsacks (BwK) problem and developa primal-dual based algorithm that achieves a problem-dependent logarithmicregret bound. The BwK problem extends the multi-arm bandit (MAB) problem tomodel the resource consumption associated with playing each arm  and theexisting BwK literature has been mainly focused on deriving asymptoticallyoptimal distribution-free regret bounds. We first study the primal and duallinear programs underlying the BwK problem. From this primal-dual perspective we discover symmetry between arms and knapsacks  and then propose a new notionof sub-optimality measure for the BwK problem. The sub-optimality measurehighlights the important role of knapsacks in determining algorithm regret andinspires the design of our two-phase algorithm. In the first phase  thealgorithm identifies the optimal arms and the binding knapsacks  and in thesecond phase  it exhausts the binding knapsacks via playing the optimal armsthrough an adaptive procedure. Our regret upper bound involves the proposedsub-optimality measure and it has a logarithmic dependence on length of horizon T  and a polynomial dependence on  m  (the numbers of arms) and  d  (thenumber of knapsacks). To the best of our knowledge  this is the firstproblem-dependent logarithmic regret bound for solving the general BwK problem.
http://arxiv.org/abs/2102.06360, Multi-source Pseudo-label Learning of Semantic Segmentation for the  Scene Recognition of Agricultural Mobile Robots,This paper describes a novel method of training a semantic segmentation modelfor environment recognition of agricultural mobile robots by unsuperviseddomain adaptation exploiting publicly available datasets of outdoor scenes thatare different from our target environments i.e.  greenhouses. In conventionalsemantic segmentation methods  the labels are given by manual annotation  whichis a tedious and time-consuming task. A method to work around the necessity ofthe manual annotation is unsupervised domain adaptation (UDA) that transferknowledge from labeled source datasets to unlabeled target datasets. Most ofthe UDA methods of semantic segmentation are validated by tasks of adaptationfrom non-photorealistic synthetic images of urban scenes to real ones. However the effectiveness of the methods is not well studied in the case of adaptationto other types of environments  such as greenhouses. In addition  it is notalways possible to prepare appropriate source datasets for such environments.In this paper  we adopt an existing training method of UDA to a task oftraining a model for greenhouse images. We propose to use multiple publiclyavailable datasets of outdoor images as source datasets  and also propose asimple yet effective method of generating pseudo-labels by transferringknowledge from the source datasets that have different appearance and a labelset from the target datasets. We demonstrate in experiments that by combiningour proposed method of pseudo-label generation with the existing trainingmethod  the performance was improved by up to 14.3% of mIoU compared to thebest score of the single-source training.
http://arxiv.org/abs/2102.06361, The Distributed Discrete Gaussian Mechanism for Federated Learning with  Secure Aggregation,We consider training models on private data that is distributed across userdevices. To ensure privacy  we add on-device noise and use secure aggregationso that only the noisy sum is revealed to the server. We present acomprehensive end-to-end system  which appropriately discretizes the data andadds discrete Gaussian noise before performing secure aggregation. We provide anovel privacy analysis for sums of discrete Gaussians. We also analyze theeffect of rounding the input data and the modular summation arithmetic. Ourtheoretical guarantees highlight the complex tension between communication privacy  and accuracy. Our extensive experimental results demonstrate that oursolution is essentially able to achieve a comparable accuracy to centraldifferential privacy with 16 bits of precision per value.
http://arxiv.org/abs/2102.06362, The Software Heritage Filesystem (SwhFS): Integrating Source Code  Archival with Development,We introduce the Software Heritage filesystem (SwhFS)  a user-spacefilesystem that integrates large-scale open source software archival withdevelopment workflows. SwhFS provides a POSIX filesystem view of SoftwareHeritage  the largest public archive of software source code and versioncontrol system (VCS) development history.Using SwhFS  developers can quickly checkout  any of the 2 billion commits archived by Software Heritage  evenafter they disappear from their previous known location and without incurringthe performance cost of repository cloning. SwhFS works across unrelatedrepositories and different VCS technologies. Other source code artifactsarchived by Software Heritage-individual source code files and trees  releases and branches-can also be accessed using common programming tools and customscripts  as if they were locally available.A screencast of SwhFS is availableonline at dx.doi.org/10.5281/zenodo.4531411.
http://arxiv.org/abs/2102.06363, Multiversal views on language models,The virtuosity of language models like GPT-3 opens a new world of possibilityfor human-AI collaboration in writing. In this paper  we present a framework inwhich generative language models are conceptualized as multiverse generators.This framework also applies to human imagination and is core to how we read andwrite fiction. We call for exploration into this commonality through new formsof interfaces which allow humans to couple their imagination to AI to write explore  and understand non-linear fiction. We discuss the early insights wehave gained from actively pursuing this approach by developing and testing anovel multiversal GPT-3-assisted writing interface.
http://arxiv.org/abs/2102.06364, Complete Power Reallocation for MU-MIMO under Per-Antenna Power  Constraint,This paper proposes a beamforming method under a per-antenna power constraint(PAPC). Although many beamformer designs with the PAPC need to solve complexoptimization problems  the proposed complete power reallocation (CPR) methodcan generate beamformers with excellent performance only with linearoperations. CPR is designed to have a simple structure  making it highlyflexible and practical. In this paper  three CPR variations consideringalgorithm convergence speed  sum-rate maximization  and robustness to channeluncertainty are developed. Simulation results verify that CPR and itsvariations satisfy their design criteria  and  hence  CPR can be readilyutilized for various purposes.
http://arxiv.org/abs/2102.06365, White-Box Performance-Influence Models: A Profiling and Learning  Approach,Many modern software systems are highly configurable  allowing the user totune them for performance and more. Current performance modeling approaches aimat finding performance-optimal configurations by building performance models ina black-box manner. While these models provide accurate estimates  they cannotpinpoint causes of observed performance behavior to specific code regions. Thisdoes not only hinder system understanding  but it also complicates tracing theinfluence of configuration options to individual methods.We propose a white-box approach that models configuration-dependentperformance behavior at the method level. This allows us to predict theinfluence of configuration decisions on individual methods  supporting systemunderstanding and performance debugging. The approach consists of two steps:First  we use a coarse-grained profiler and learn performance-influence modelsfor all methods  potentially identifying some methods that are highlyconfiguration- and performance-sensitive  causing inaccurate predictions.Second  we re-measure these methods with a fine-grained profiler and learn moreaccurate models  at higher cost  though. By means of 9 real-world Java softwaresystems  we demonstrate that our approach can efficiently identifyconfiguration-relevant methods and learn accurate performance-influence models.
http://arxiv.org/abs/2102.06366, Well-posedness theory for nonlinear scalar conservation laws on networks,We consider nonlinear scalar conservation laws posed on a network. Weestablish  L^1  stability  and thus uniqueness  for weak solutions satisfyingthe entropy condition. We apply standard finite volume methods and showstability and convergence to the unique entropy solution  thus establishingexistence of a solution in the process. Both our existence andstability/uniqueness theory is centred around families of stationary states forthe equation. In one important case -- for monotone fluxes with an upwinddifference scheme -- we show that the set of (discrete) stationary solutions isindeed sufficiently large to suit our general theory. We demonstrate themethod's properties through several numerical experiments.
http://arxiv.org/abs/2102.06371, SceneRec: Scene-Based Graph Neural Networks for Recommender Systems,Collaborative filtering has been largely used to advance modern recommendersystems to predict user preference. A key component in collaborative filteringis representation learning  which aims to project users and items into a lowdimensional space to capture collaborative signals. However  the sceneinformation  which has effectively guided many recommendation tasks  is rarelyconsidered in existing collaborative filtering methods. To bridge this gap  wefocus on scene-based collaborative recommendation and propose a novelrepresentation model SceneRec. SceneRec formally defines a scene as a set ofpre-defined item categories that occur simultaneously in real-life situationsand creatively designs an item-category-scene hierarchical structure to build ascene-based graph. In the scene-based graph  we adopt graph neural networks tolearn scene-specific representation on each item node  which is furtheraggregated with latent representation learned from collaborative interactionsto make recommendations. We perform extensive experiments on real-worldE-commerce datasets and the results demonstrate the effectiveness of theproposed method.
http://arxiv.org/abs/2102.06377, Rate-Splitting Multiple Access to Mitigate the Curse of Mobility in  (Massive) MIMO Networks,Rate-Splitting Multiple Access (RSMA) is a flexible and robust multipleaccess scheme for downlink multi-antenna wireless networks. RSMA relies onmulti-antenna Rate-Splitting (RS) at the transmitter and SuccessiveInterference Cancellation (SIC) at the receivers. In this work  we study theperformance of RSMA under the practical important setup of imperfect ChannelState Information at Transmitter (CSIT) originating from user mobility andlatency in the network. First  we derive a lower bound on the ergodic sum-rateof RSMA for an arbitrary number of transmit antennas  number of users  userspeeds and transmit power. Then  we study the power allocation between commonand private streams and obtain a closed-form solution for the optimal powerallocation that maximizes the obtained lower bound. The proposed powerallocation greatly reduces precoder design complexity for RSMA. By Link-LevelSimulations (LLS)  we demonstrate that RSMA with the proposed power allocationis robust to degrading effects of user mobility and has significantly higherperformance compared to conventional multi-user (massive) Multiple-InputMultiple-Output (MIMO) strategies. The work has important practicalsignificance as results demonstrate that  in contrast to conventionalmulti-user (massive) MIMO whose performance collapse under mobility  RSMA canmaintain reliable multi-user connectivity in mobile deployments.
http://arxiv.org/abs/2102.06380, A Too-Good-to-be-True Prior to Reduce Shortcut Reliance,Despite their impressive performance in object recognition and other tasksunder standard testing conditions  deep convolutional neural networks (DCNNs)often fail to generalize to out-of-distribution (o.o.d.) samples. One cause forthis shortcoming is that modern architectures tend to rely on  shortcuts  -superficial features that correlate with categories without capturing deeperinvariants that hold across contexts. Real-world concepts often possess acomplex structure that can vary superficially across contexts  which can makethe most intuitive and promising solutions in one context not generalize toothers. One potential way to improve o.o.d. generalization is to assume simplesolutions are unlikely to be valid across contexts and downweight them  whichwe refer to as the too-good-to-be-true prior. We implement this inductive biasin a two-stage approach that uses predictions from a low-capacity network (LCN)to inform the training of a high-capacity network (HCN). Since the shallowarchitecture of the LCN can only learn surface relationships  which includesshortcuts  we downweight training items for the HCN that the LCN can master thereby encouraging the HCN to rely on deeper invariant features that shouldgeneralize broadly. Using a modified version of the CIFAR-10 dataset in whichwe introduced shortcuts  we found that the two-stage LCN-HCN approach reducedreliance on shortcuts and facilitated o.o.d. generalization.
http://arxiv.org/abs/2102.06384, Densely Deformable Efficient Salient Object Detection Network,Salient Object Detection (SOD) domain using RGB-D data has lately emergedwith some current models' adequately precise results. However  they haverestrained generalization abilities and intensive computational complexity. Inthis paper  inspired by the best background/foreground separation abilities ofdeformable convolutions  we employ them in our Densely Deformable Network(DDNet) to achieve efficient SOD. The salient regions from densely deformableconvolutions are further refined using transposed convolutions to optimallygenerate the saliency maps. Quantitative and qualitative evaluations using therecent SOD dataset against 22 competing techniques show our method's efficiencyand effectiveness. We also offer evaluation using our own createdcross-dataset  surveillance-SOD (S-SOD)  to check the trained models' validityin terms of their applicability in diverse scenarios. The results indicate thatthe current models have limited generalization potentials  demanding furtherresearch in this direction. Our code and new dataset will be publicly availableat https://github.com/tanveer-hussain/EfficientSOD
http://arxiv.org/abs/2102.06385, Supervised training of spiking neural networks for robust deployment on  mixed-signal neuromorphic processors,Mixed-signal analog/digital electronic circuits can emulate spiking neuronsand synapses with extremely high energy efficiency  following an approach knownas  neuromorphic engineering . However  analog circuits are sensitive tovariation in fabrication among transistors in a chip ( device mismatch ). Inthe case of neuromorphic implementation of Spiking Neural Networks (SNNs) mismatch is expressed as differences in effective parameters betweenidentically-configured neurons and synapses. Each fabricated chip thereforeprovides a different distribution of parameters such as time constants orsynaptic weights. Without the expensive overhead in terms of area and power ofextra on-chip learning or calibration circuits  device mismatch and other noisesources represent a critical challenge for the deployment of pre-trained neuralnetwork chips. Here we present a supervised learning approach that addressesthis challenge by maximizing robustness to mismatch and other common sources ofnoise.The proposed method trains (SNNs) to perform temporal classification tasks bymimicking a pre-trained dynamical system  using a local learning rule adaptedfrom non-linear control theory. We demonstrate the functionality of our modelon two tasks that require memory to perform successfully  and measure therobustness of our approach to several forms of noise and variability present inthe network. We show that our approach is more robust than several commonalternative approaches for training SNNs.Our method provides a viable way to robustly deploy pre-trained networks onmixed-signal neuromorphic hardware  without requiring per-device training orcalibration.
http://arxiv.org/abs/2102.06386, Robust Hybrid High-Order method on polytopal meshes with small faces,We design a Hybrid High-Order (HHO) scheme for the Poisson problem that isfully robust on polytopal meshes in the presence of small edges/faces. We stategeneral assumptions on the stabilisation terms involved in the scheme  underwhich optimal error estimates (in discrete and continuous energy norms  as wellas  L^2 -norm) are established with multiplicative constants that do not dependon the maximum number of faces in each element  or the relative size between anelement and its faces. We illustrate the error estimates through numericalsimulations in 2D and 3D on meshes designed by agglomeration techniques (suchmeshes naturally have elements with a very large numbers of faces  and verysmall faces).
http://arxiv.org/abs/2102.06387, Banana for scale: Gauging trends in academic interest by normalising  publication rates to common and innocuous keywords,Many academics use yearly publication numbers to quantify academic interestfor their research topic. While such visualisations are ubiquitous in grantapplications  manuscript introductions  and review articles  they fail toaccount for the rapid growth in scientific publications. As a result  anysearch term will likely show an increase in supposed  academic interest . Oneproposed solution is to normalise yearly publication rates by field size  butthis is arduous and difficult. Here  we propose an simpler index thatnormalises keywords of interest by a ubiquitous and innocuous keyword  such as banana . Alternatively  one could opt for field-specific keywords orhierarchical structures (e.g. PubMed's Medical Subject Headings  MeSH) tocompute  interest market share . Using this approach  we uncovered plausibletrends in academic interest in examples from the medical literature. Inneuroimaging  we found that not the supplementary motor area (as was previouslyclaimed)  but the prefrontal cortex is the most interesting part of the brain.In cancer research  we found a contemporary preference for cancers with highprevalence and clinical severity  and notable declines in interest for moretreatable or likely benign neoplasms. Finally  we found that interest inrespiratory viral infections spiked when strains showed potential for pandemicinvolvement  with SARS-CoV-2 and the COVID-19 pandemic being the most extremeexample. In sum  the time is ripe for a quick and easy method to quantifytrends in academic interest for anecdotal purposes. We provide such a method along with software for researchers looking to implement it in their ownwriting.
http://arxiv.org/abs/2102.06388, Reaction or Speculation: Building Computational Support for Users in  Catching-Up Series Based on an Emerging Media Consumption Phenomenon,A growing number of people are using catch-up TV services rather thanwatching simultaneously with other audience members at the time of broadcast.However  computational support for such catching-up users has not been wellexplored. In particular  we are observing an emerging phenomenon in onlinemedia consumption experiences in which speculation plays a vital role. As thephenomenon of speculation implicitly assumes simultaneity in media consumption there is a gap for catching-up users  who cannot directly appreciate theconsumption experiences. This conversely suggests that there is potential forcomputational support to enhance the consumption experiences of catching-upusers. Accordingly  we conducted a series of studies to pave the way fordeveloping computational support for catching-up users. First  we conductedsemi-structured interviews to understand how people are engaging withspeculation during media consumption. As a result  we discovered thedistinctive aspects of speculation-based consumption experiences in contrast tosocial viewing experiences sharing immediate reactions that have been discussedin previous studies. We then designed two prototypes for supporting catching-upusers based on our quantitative analysis of Twitter data in regard to reaction-and speculation-based media consumption. Lastly  we evaluated the prototypes ina user experiment and  based on its results  discussed ways to empowercatching-up users with computational supports in response to recenttransformations in media consumption.
http://arxiv.org/abs/2102.06390, Emoji-Based Transfer Learning for Sentiment Tasks,Sentiment tasks such as hate speech detection and sentiment analysis especially when performed on languages other than English  are oftenlow-resource. In this study  we exploit the emotional information encoded inemojis to enhance the performance on a variety of sentiment tasks. This is doneusing a transfer learning approach  where the parameters learned by anemoji-based source task are transferred to a sentiment target task. We analysethe efficacy of the transfer under three conditions  i.e. i) the emoji contentand ii) label distribution of the target task as well as iii) the differencebetween monolingually and multilingually learned source tasks. We find i.a.that the transfer is most beneficial if the target task is balanced with highemoji content. Monolingually learned source tasks have the benefit of takinginto account the culturally specific use of emojis and gain up to F1 +0.280over the baseline.
http://arxiv.org/abs/2102.06391, A Subexponential Algorithm for ARRIVAL,The ARRIVAL problem is to decide the fate of a train moving along the edgesof a directed graph  according to a simple (deterministic) pseudorandom walk.The problem is in  NP \cap coNP  but not known to be in  P . The currently bestalgorithms have runtime  2^{\Theta(n)}  where  n  is the number of vertices.This is not much better than just performing the pseudorandom walk. We developa subexponential algorithm with runtime  2^{O(\sqrt{n}\log n)} . We also give apolynomial-time algorithm if the graph is almost acyclic. Both results arederived from a new general approach to solve ARRIVAL instances.
http://arxiv.org/abs/2102.06392, Bootstrapping Large-Scale Fine-Grained Contextual Advertising Classifier  from Wikipedia,Contextual advertising provides advertisers with the opportunity to targetthe context which is most relevant to their ads. However  its power cannot befully utilized unless we can target the page content using fine-grainedcategories  e.g.   coupe  vs.  hatchback  instead of  automotive  vs.  sport .The widely used advertising content taxonomy (IAB taxonomy) consists of 23coarse-grained categories and 355 fine-grained categories. With the largenumber of categories  it becomes very challenging either to collect trainingdocuments to build a supervised classification model  or to composeexpert-written rules in a rule-based classification system. Besides  infine-grained classification  different categories often overlap or co-occur making it harder to classify accurately. In this work  we propose wiki2cat  amethod to tackle the problem of large-scaled fine-grained text classificationby tapping on Wikipedia category graph. The categories in IAB taxonomy arefirst mapped to category nodes in the graph. Then the label is propagatedacross the graph to obtain a list of labeled Wikipedia documents to induce textclassifiers. The method is ideal for large-scale classification problems sinceit does not require any manually-labeled document or hand-curated rules orkeywords. The proposed method is benchmarked with various learning-based andkeyword-based baselines and yields competitive performance on both publiclyavailable datasets and a new dataset containing more than 300 fine-grainedcategories.
http://arxiv.org/abs/2102.06393, VARA-TTS: Non-Autoregressive Text-to-Speech Synthesis based on Very Deep  VAE with Residual Attention,This paper proposes VARA-TTS  a non-autoregressive (non-AR) text-to-speech(TTS) model using a very deep Variational Autoencoder (VDVAE) with ResidualAttention mechanism  which refines the textual-to-acoustic alignmentlayer-wisely. Hierarchical latent variables with different temporal resolutionsfrom the VDVAE are used as queries for residual attention module. By leveragingthe coarse global alignment from previous attention layer as an extra input the following attention layer can produce a refined version of alignment. Thisamortizes the burden of learning the textual-to-acoustic alignment amongmultiple attention layers and outperforms the use of only a single attentionlayer in robustness. An utterance-level speaking speed factor is computed by ajointly-trained speaking speed predictor  which takes the mean-pooled latentvariables of the coarsest layer as input  to determine number of acousticframes at inference. Experimental results show that VARA-TTS achieves slightlyinferior speech quality to an AR counterpart Tacotron 2 but anorder-of-magnitude speed-up at inference; and outperforms an analogous non-ARmodel  BVAE-TTS  in terms of speech quality.
http://arxiv.org/abs/2102.06395, Towards Large Scale Automated Algorithm Design by Integrating Modular  Benchmarking Frameworks,We present a first proof-of-concept use-case that demonstrates the efficiencyof interfacing the algorithm framework ParadisEO with the automated algorithmconfiguration tool irace and the experimental platform IOHprofiler. By combingthese three tools  we obtain a powerful benchmarking environment that allows usto systematically analyze large classes of algorithm spaces on complexbenchmark problems. Key advantages of our pipeline are fast evaluation times the possibility to generate rich data sets to support the analysis of thealgorithms  and a standardized interface that can be used to benchmark verybroad classes of sampling-based optimization heuristics.In addition to enabling systematic algorithm configuration studies  ourapproach paves a way for assessing the contribution of new ideas in interplaywith already existing operators -- a promising avenue for our research domain which at present may have a too strong focus on comparing entire algorithminstances.
http://arxiv.org/abs/2102.06400, Fast Fault Detection on a Quadrotor using Onboard Sensors and a Kalman  Filter Approach,This paper presents a novel method for fast and robust detection of actuatorfailures on quadrotors. The proposed algorithm has very little modeldependency. A Kalman filter estimator estimates a stochastic effectivenessfactor for every actuator  using only onboard RPM  gyro and accelerometermeasurements. Then  a hypothesis test identifies the failed actuator. Thisalgorithm is validated online in real-time  also as part of an active faulttolerant control system. Loss of actuator effectiveness is induced by ejectingthe propellers from the motors. The robustness of this algorithm is furtherinvestigated offline over a range of parameter settings by replaying realflight data containing 26 propeller ejections. The detection delays are foundto be in the 30 to 130 ms range  without missed detections or false alarmsoccurring.
http://arxiv.org/abs/2102.06401, Broad-UNet: Multi-scale feature learning for nowcasting tasks,Weather nowcasting consists of predicting meteorological components in theshort term at high spatial resolutions. Due to its influence in many humanactivities  accurate nowcasting has recently gained plenty of attention. Inthis paper  we treat the nowcasting problem as an image-to-image translationproblem using satellite imagery. We introduce Broad-UNet  a novel architecturebased on the core UNet model  to efficiently address this problem. Inparticular  the proposed Broad-UNet is equipped with asymmetric parallelconvolutions as well as Atrous Spatial Pyramid Pooling (ASPP) module. In thisway  The the Broad-UNet model learns more complex patterns by combiningmulti-scale features while using fewer parameters than the core UNet model. Theproposed model is applied on two different nowcasting tasks  i.e. precipitationmaps and cloud cover nowcasting. The obtained numerical results show that theintroduced Broad-UNet model performs more accurate predictions compared to theother examined architectures.
http://arxiv.org/abs/2102.06405, Data Analytics and Machine Learning Methods  Techniques and Tool for  Model-Driven Engineering of Smart IoT Services,This doctoral dissertation proposes a novel approach to enhance thedevelopment of smart services for the Internet of Things (IoT) and smartCyber-Physical Systems (CPS). The proposed approach offers abstraction andautomation to the software engineering processes  as well as the Data Analytics(DA) and Machine Learning (ML) practices. This is realized in an integrated andseamless manner. We implement and validate the proposed approach by extendingan open source modeling tool  called ThingML. ThingML is a domain-specificlanguage and modeling tool with code generation for the IoT/CPS domain. NeitherThingML nor any other IoT/CPS modeling tool supports DA/ML at the modelinglevel. Therefore  as the primary contribution of the doctoral dissertation  weadd the necessary syntax and semantics concerning DA/ML methods and techniquesto the modeling language of ThingML. Moreover  we support the APIs of severalML libraries and frameworks for the automated generation of the source code ofthe target software in Python and Java. Our approach enablesplatform-independent  as well as platform-specific models. Further  we assistin carrying out semiautomated DA/ML tasks by offering Automated ML (AutoML)  inthe background (in expert mode)  and through model-checking constraints andhints at design-time. Finally  we consider three use case scenarios from thedomains of network security  smart energy systems and energy exchange markets.
http://arxiv.org/abs/2102.06406, Annotation Cleaning for the MSR-Video to Text Dataset,The video captioning task is to describe the video contents with naturallanguage by the machine. Many methods have been proposed for solving this task.A large dataset called MSR Video to Text (MSR-VTT) is often used as thebenckmark dataset for testing the performance of the methods. However  we foundthat the human annotations  i.e.  the descriptions of video contents in thedataset are quite noisy  e.g.  there are many duplicate captions and manycaptions contain grammatical problems. These problems may pose difficulties tovideo captioning models for learning. We cleaned the MSR-VTT annotations byremoving these problems  then tested several typical video captioning models onthe cleaned dataset. Experimental results showed that data cleaning boosted theperformances of the models measured by popular quantitative metrics. Werecruited subjects to evaluate the results of a model trained on the originaland cleaned datasets. The human behavior experiment demonstrated that trainedon the cleaned dataset  the model generated captions that were more coherentand more relevant to contents of the video clips. The cleaned dataset ispublicly available.
http://arxiv.org/abs/2102.06407, Deep Sound Field Reconstruction in Real Rooms: Introducing the ISOBEL  Sound Field Dataset,Knowledge of loudspeaker responses are useful in a number of applications where a sound system is located inside a room that alters the listeningexperience depending on position within the room. Acquisition of sound fieldsfor sound sources located in reverberant rooms can be achieved through laborintensive measurements of impulse response functions covering the room  oralternatively by means of reconstruction methods which can potentially requiresignificantly fewer measurements. This paper extends evaluations of sound fieldreconstruction at low frequencies by introducing a dataset with measurementsfrom four real rooms. The ISOBEL Sound Field dataset is publicly available  andaims to bridge the gap between synthetic and real-world sound fields inrectangular rooms. Moreover  the paper advances on a recent deep learning-basedmethod for sound field reconstruction using a very low number of microphones and proposes an approach for modeling both magnitude and phase response in aU-Net-like neural network architecture. The complex-valued sound fieldreconstruction demonstrates that the estimated room transfer functions are ofhigh enough accuracy to allow for personalized sound zones with contrast ratioscomparable to ideal room transfer functions using 15 microphones below 150 Hz.
http://arxiv.org/abs/2102.06408, Two Elements of Pair Programming Skill,Background: Pair programming (PP) can have many benefits in industry.Researchers and practitioners recognize that successful and productive PPinvolves some skill that might take time to learn and improve.Question: What are the elements of pair programming skill?Method: We perform qualitative analyses of industrial pair programmingsessions following the Grounded Theory Methodology. We look for patterns ofproblematic behavior to conceptualize key elements of what 'good' and 'bad'pairs do differently.Results: Here  we report two elements of pair programming skill: Good pairs(1) manage to maintain their Togetherness and (2) keep an eye on theirsession's Expediency. We identify three problematic behavioral patterns thataffect one or both of these elements: Getting Lost in the Weeds  Losing thePartner  and Drowning the Partner.Conclusion: Pair programming skill is separate from general softwaredevelopment skill. Years of PP experience are neither a prerequisite norsufficient for successful pair programming.
http://arxiv.org/abs/2102.06414, Unified Compact Numerical Quadrature Formulas for Hadamard Finite Parts  of Singular Integrals of Periodic Functions,We consider the numerical computation of finite-range singular integrals  I[f]=\intBar^b_a f(x)\ dx \quad f(x)=\frac{g(x)}{(x-t)^m} \quadm=1 2 \ldots \quada<t<b    that are defined in the sense of Hadamard Finite Part  assuming that g\in C^\infty[a b]  and  f(x)\in C^\infty(\mathbb{R}_t)  is  T -periodic with \mathbb{R}_t=\mathbb{R}\setminus\{t+ kT\}^\infty_{k=-\infty}   T=b-a . Using a generalization of the Euler--Maclaurin expansion developedin [A. Sidi {Euler--Maclaurin} expansions for integrals with arbitrary algebraic endpointsingularities. {\em Math. Comp.}  81:2159--2173  2012]  we unify the treatmentof these integrals. For each  m   we develop a number of numerical quadratureformulas  \widehat{T}^{(s)}_{m n}[f]  of trapezoidal type for  I[f] . Forexample  three numerical quadrature formulas of trapezoidal type result fromthis approach for the case  m=3   and these are\begin{align*}\widehat{T}^{(0)}_{3 n}[f]&=h\sum^{n-1}_{j=1}f(t+jh)-\frac{\pi^2}{3}\ g'(t)\ h^{-1}+\frac{1}{6}\ g'''(t)\ h  \quad h=\frac{T}{n} \widehat{T}^{(1)}_{3 n}[f]&=h\sum^n_{j=1}f(t+jh-h/2)-\pi^2\ g'(t)\ h^{-1} \quadh=\frac{T}{n} \widehat{T}^{(2)}_{3 n}[f]&=2h\sum^n_{j=1}f(t+jh-h/2)-\frac{h}{2}\sum^{2n}_{j=1}f(t+jh/2-h/4) \quad h=\frac{T}{n}.\end{align*}
http://arxiv.org/abs/2102.06416, Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph  Convolutional Neural Networks,Most graph neural networks (GNN) perform poorly in graphs where neighborstypically have different features/classes (heterophily) and when stackingmultiple layers (oversmoothing). These two seemingly unrelated problems havebeen studied independently  but there is recent empirical evidence that solvingone problem may benefit the other. In this work  going beyond empiricalobservations  we theoretically characterize the connections between heterophilyand oversmoothing  both of which lead to indistinguishable noderepresentations. By modeling the change in node representations during messagepropagation  we theoretically analyze the factors (e.g.  degree  heterophilylevel) that make the representations of nodes from different classesindistinguishable. Our analysis highlights that (1) nodes with high heterophilyand nodes with low heterophily and low degrees relative to their neighbors(degree discrepancy) trigger the oversmoothing problem  and (2) allowing negative  messages between neighbors can decouple the heterophily andoversmoothing problems. Based on our insights  we design a model that addressesthe discrepancy in features and degrees between neighbors by incorporatingsigned messages and learned degree corrections. Our experiments on 9 realnetworks show that our model achieves state-of-the-art performance underheterophily  and performs comparably to existing GNNs under lowheterophily(homophily). It also effectively addresses oversmoothing and evenbenefits from multiple layers.
http://arxiv.org/abs/2102.06418, A more accurate view of the Flat Wall Theorem,We introduce a supporting combinatorial framework for the Flat Wall Theorem.In particular  we suggest two variants of the theorem and we introduce a new more versatile  concept of wall homogeneity as well as the notion of regularityin flat walls. All proposed concepts and results aim at facilitating the use ofthe irrelevant vertex technique in future algorithmic applications.
http://arxiv.org/abs/2102.06422, Content-Aware Speaker Embeddings for Speaker Diarisation,Recent speaker diarisation systems often convert variable length speechsegments into fixed-length vector representations for speaker clustering  whichare known as speaker embeddings. In this paper  the content-aware speakerembeddings (CASE) approach is proposed  which extends the input of the speakerclassifier to include not only acoustic features but also their correspondingspeech content  via phone  character  and word embeddings. Compared toalternative methods that leverage similar information  such as multitask oradversarial training  CASE factorises automatic speech recognition (ASR) fromspeaker recognition to focus on modelling speaker characteristics andcorrelations with the corresponding content units to derive more expressiverepresentations. CASE is evaluated for speaker re-clustering with a realisticspeaker diarisation setup using the AMI meeting transcription dataset  wherethe content information is obtained by performing ASR based on an automaticsegmentation. Experimental results showed that CASE achieved a 17.8% relativespeaker error rate reduction over conventional methods.
http://arxiv.org/abs/2102.06423, Exactness and Convergence Properties of Some Recent Numerical Quadrature  Formulas for Supersingular Integrals of Periodic Functions,In a recent work  we developed three new compact numerical quadratureformulas for finite-range periodic supersingular integrals  I[f]=\intBar^b_af(x)\ dx   where  f(x)=g(x)/(x-t)^3   assuming that  g\in C^\infty[a b]  and f(x)  is  T -periodic   T=b-a . With  h=T/n   these numerical quadratureformulas read\begin{align*}\widehat{T}{}^{(0)}_n[f]&=h\sum^{n-1}_{j=1}f(t+jh)-\frac{\pi^2}{3}\ g'(t)\ h^{-1}+\frac{1}{6}\ g'''(t)\ h \widehat{T}{}^{(1)}_n[f]&=h\sum^n_{j=1}f(t+jh-h/2)-\pi^2\ g'(t)\ h^{-1} \widehat{T}{}^{(2)}_n[f]&=2h\sum^n_{j=1}f(t+jh-h/2)-\frac{h}{2}\sum^{2n}_{j=1}f(t+jh/2-h/4).\end{align*}We also showed that these formulas have spectral accuracy; that is   \widehat{T}{}^{(s)}_n[f]-I[f]=O(n^{-\mu})\quad ext{as  n o\infty }\quad\forall \mu>0.  In the present work  we continue our study of these formulas for the specialcase in which f(x)=\frac{\cos\frac{\pi(x-t)}{T}}{\sin^3\frac{\pi(x-t)}{T}}\ u(x)   where u(x)  is in  C^\infty(\mathbb{R})  and is  T -periodic. Actually  we provethat  \widehat{T}{}^{(s)}_n[f]   s=0 1 2   are exact for a class of singular integrals involving  T -periodictrigonometric polynomials of degree at most  n-1 ; that is    \widehat{T}{}^{(s)}_n[f]=I[f]\quad ext{when\ \ f(x)=\frac{\cos\frac{\pi(x-t)}{T}}{\sin^3\frac{\pi(x-t)}{T}}\ \sum^{n-1}_{m=-(n-1)}c_m\exp(\mrm{i}2m\pi x/T) .}   We also prove that  when  u(z)  is analytic in astrip  \big| ext{Im}\ z\big|<\sigma  of the complex  z -plane  the errors inall three  \widehat{T}{}^{(s)}_n[f]  are  O(e^{-2n\pi\sigma/T})  as n o\infty   for all practical purposes.
http://arxiv.org/abs/2102.06427, Transformer Language Models with LSTM-based Cross-utterance Information  Representation,The effective incorporation of cross-utterance information has the potentialto improve language models (LMs) for automatic speech recognition (ASR). Toextract more powerful and robust cross-utterance representations for theTransformer LM (TLM)  this paper proposes the R-TLM which uses hidden states ina long short-term memory (LSTM) LM. To encode the cross-utterance information the R-TLM incorporates an LSTM module together with a segment-wise recurrencein some of the Transformer blocks. In addition to the LSTM module output  ashortcut connection using a fusion layer that bypasses the LSTM module is alsoinvestigated. The proposed system was evaluated on the AMI meeting corpus  theEval2000 and the RT03 telephone conversation evaluation sets. The best R-TLMachieved 0.9%  0.6%  and 0.8% absolute WER reductions over the single-utteranceTLM baseline  and 0.5%  0.3%  0.2% absolute WER reductions over a strongcross-utterance TLM baseline on the AMI evaluation set  Eval2000 and RT03respectively. Improvements on Eval2000 and RT03 were further supported bysignificance tests. R-TLMs were found to have better LM scores on words whererecognition errors are more likely to occur. The R-TLM WER can be furtherreduced by interpolation with an LSTM-LM.
http://arxiv.org/abs/2102.06429, PVTSI ^{\boldmath(m)} : A Novel Approach to Computation of Hadamard  Finite Parts of Nonperiodic Singular Integrals,We consider the numerical computation of I[f]=\intBar^b_a f(x)\ dx   the Hadamard Finite Part of the finite-rangesingular integral  \int^b_a f(x)\ dx    f(x)=g(x)/(x-t)^{m}  with  a<t<b  and m\in\{1 2 \ldots\}   assuming that (i)\  g\in C^\infty(a b)  and (ii)\  g(x) is allowed to have arbitrary integrable singularities at the endpoints  x=a and  x=b .We first prove that  \intBar^b_a f(x)\ dx  is invariant under any suitablevariable transformation  x=\psi(\xi)    \psi:[\alpha \beta] ightarrow[a b]  hence there holds  \intBar^\beta_\alpha F(\xi)\ d\xi=\intBar^b_a f(x)\ dx  where  F(\xi)=f(\psi(\xi))\ \psi'(\xi) . Based on this result  we next choose \psi(\xi)  such that the transformed integrand  F(\xi)  is sufficientlyperiodic with period  \T=\beta-\alpha   and prove  with the help of some recentextension/generalization of the Euler--Maclaurin expansion  that we can applyto \intBar^\beta_\alpha F(\xi)\ d\xi  the quadrature formulas derived forperiodic singular integrals developed in an earlier work of the author. We givea whole family of numerical quadrature formulas for  \intBar^\beta_\alphaF(\xi)\ d\xi  for each  m   which we denote  \widehat{T}^{(s)}_{m n}[{\calF}]   where  {\cal F}(\xi)  is the  \T -periodic extension of  F(\xi) .
http://arxiv.org/abs/2102.06431, Universal Adversarial Perturbations Through the Lens of Deep  Steganography: Towards A Fourier Perspective,The booming interest in adversarial attacks stems from a misalignment betweenhuman vision and a deep neural network (DNN)  i.e. a human imperceptibleperturbation fools the DNN. Moreover  a single perturbation  often calleduniversal adversarial perturbation (UAP)  can be generated to fool the DNN formost images. A similar misalignment phenomenon has recently also been observedin the deep steganography task  where a decoder network can retrieve a secretimage back from a slightly perturbed cover image. We attempt explaining thesuccess of both in a unified manner from the Fourier perspective. We performtask-specific and joint analysis and reveal that (a) frequency is a key factorthat influences their performance based on the proposed entropy metric forquantifying the frequency distribution; (b) their success can be attributed toa DNN being highly sensitive to high-frequency content. We also perform featurelayer analysis for providing deep insight on model generalization androbustness. Additionally  we propose two new variants of universalperturbations: (1) Universal Secret Adversarial Perturbation (USAP) thatsimultaneously achieves attack and hiding; (2) high-pass UAP (HP-UAP) that isless visible to the human eye.
http://arxiv.org/abs/2102.06435, Safety of Flow Decompositions in DAGs,Network flows are one of the most studied combinatorial optimization problemswith innumerable applications. Any flow on a directed acyclic graph (DAG)  G having  n  vertices and  m  edges can be decomposed into a set of  O(m)  paths with applications from network routing to assembly of biological sequences. Insome applications  the flow decomposition corresponds to some particular datathat need to be reconstructed from the flow  which require finding paths (orsubpaths) appearing in all possible flow decompositions  referred to as safepaths.Recently  Ma et al. [WABI 2020] addressed a related problem in aprobabilistic framework. Later  they gave a quadratic-time algorithm based on aglobal criterion  for a generalized version (AND-Quant) of the correspondingproblem  i.e.  reporting if a given flow path is safe. Our contributions are asfollows:1- A simple characterization for the safety of a given path based on a localcriterion  which can be directly adapted to give an optimal linear timeverification algorithm.2- A simple enumeration algorithm that reports all maximal safe paths on aflow network in  O(mn)  time. The algorithm reports all safe paths using acompact representation of the solution (called  {\cal P}_c )  which is \Omega(mn)  in the worst case  but merely  O(m+n)  in the best case.3- An improved enumeration algorithm where all safe paths ending at everyvertex are represented as funnels using  O(n^2+|{\cal P}_c|)  space. These canbe computed and used to report all maximal safe paths  using time linear in thetotal space required by funnels  with an extra logarithmic factor.Overall we present a simple characterization for the problem leading to anoptimal verification algorithm and a simple enumeration algorithm. Theenumeration algorithm is improved using the funnel structures for safe paths which may be of independent interest.
http://arxiv.org/abs/2102.06439, Leveraging Benchmarking Data for Informed One-Shot Dynamic Algorithm  Selection,A key challenge in the application of evolutionary algorithms in practice isthe selection of an algorithm instance that best suits the problem at hand.What complicates this decision further is that different algorithms may be bestsuited for different stages of the optimization process. Dynamic algorithmselection and configuration are therefore well-researched topics inevolutionary computation. However  while hyper-heuristics and parameter controlstudies typically assume a setting in which the algorithm needs to be chosenwhile running the algorithms  without prior information  AutoML approaches suchas hyper-parameter tuning and automated algorithm configuration assume thepossibility of evaluating different configurations before making a finalrecommendation. In practice  however  we are often in a middle-ground betweenthese two settings  where we need to decide on the algorithm instance beforethe run ( oneshot  setting)  but where we have (possibly lots of) dataavailable on which we can base an informed decision.We analyze in this work how such prior performance data can be used to inferinformed dynamic algorithm selection schemes for the solution of pseudo-Booleanoptimization problems. Our specific use-case considers a family of geneticalgorithms.
http://arxiv.org/abs/2102.06440, Scalable Bayesian Inverse Reinforcement Learning,Bayesian inference over the reward presents an ideal solution to theill-posed nature of the inverse reinforcement learning problem. Unfortunatelycurrent methods generally do not scale well beyond the small tabular settingdue to the need for an inner-loop MDP solver  and even non-Bayesian methodsthat do themselves scale often require extensive interaction with theenvironment to perform well  being inappropriate for high stakes or costlyapplications such as healthcare. In this paper we introduce our method Approximate Variational Reward Imitation Learning (AVRIL)  that addresses bothof these issues by jointly learning an approximate posterior distribution overthe reward that scales to arbitrarily complicated state spaces alongside anappropriate policy in a completely offline manner through a variationalapproach to said latent reward. Applying our method to real medical dataalongside classic control simulations  we demonstrate Bayesian reward inferencein environments beyond the scope of current methods  as well as taskperformance competitive with focused offline imitation learning algorithms.
http://arxiv.org/abs/2102.06442, A space-time discretization of a nonlinear peridynamic model on a 2D  lamina,Peridynamics is a nonlocal theory for dynamic fracture analysis consisting ina second order in time partial integro-differential equation. In this paper  weconsider a nonlinear model of peridynamics in a two-dimensional spatial domain.We implement a spectral method for the space discretization based on theFourier expansion of the solution while we consider the Newmark- \beta  methodfor the time marching. This computational approach takes advantages from theconvolutional form of the peridynamic operator and from the use of the discreteFourier transform. We show a convergence result for the fully discreteapproximation and study the stability of the method applied to the linearperidynamic model. Finally  we perform several numerical tests and comparisonsto validate our results and provide simulations implementing a volumepenalization technique to avoid the limitation of periodic boundary conditionsdue to the spectral approach.
http://arxiv.org/abs/2102.06445, Adaptive Sampling for Fast Constrained Maximization of Submodular  Function,Several large-scale machine learning tasks  such as data summarization  canbe approached by maximizing functions that satisfy submodularity. Theseoptimization problems often involve complex side constraints  imposed by theunderlying application. In this paper  we develop an algorithm withpoly-logarithmic adaptivity for non-monotone submodular maximization undergeneral side constraints. The adaptive complexity of a problem is the minimalnumber of sequential rounds required to achieve the objective.Our algorithm is suitable to maximize a non-monotone submodular functionunder a  p -system side constraint  and it achieves a  (p +O(\sqrt{p})) -approximation for this problem  after only poly-logarithmicadaptive rounds and polynomial queries to the valuation oracle function.Furthermore  our algorithm achieves a  (p + O(1)) -approximation when the givenside constraint is a  p -extendible system.This algorithm yields an exponential speed-up  with respect to theadaptivity  over any other known constant-factor approximation algorithm forthis problem. It also competes with previous known results in terms of thequery complexity. We perform various experiments on various real-worldapplications. We find that  in comparison with commonly used heuristics  ouralgorithm performs better on these instances.
http://arxiv.org/abs/2102.06448, End-to-End Intelligent Framework for Rockfall Detection,Rockfall detection is a crucial procedure in the field of geology  whichhelps to reduce the associated risks. Currently  geologists identify rockfallevents almost manually utilizing point cloud and imagery data obtained fromdifferent caption devices such as Terrestrial Laser Scanner or digital cameras.Multi-temporal comparison of the point clouds obtained with these techniquesrequires a tedious visual inspection to identify rockfall events which impliesinaccuracies that depend on several factors such as human expertise and thesensibility of the sensors. This paper addresses this issue and provides anintelligent framework for rockfall event detection for any individual workingin the intersection of the geology domain and decision support systems. Thedevelopment of such an analysis framework poses significant research challengesand justifies intensive experimental analysis. In particular  we propose anintelligent system that utilizes multiple machine learning algorithms to detectrockfall clusters of point cloud data. Due to the extremely imbalanced natureof the problem  a plethora of state-of-the-art resampling techniquesaccompanied by multiple models and feature selection procedures are beinginvestigated. Various machine learning pipeline combinations have beenbenchmarked and compared applying well-known metrics to be incorporated intoour system. Specifically  we developed statistical and machine learningtechniques and applied them to analyze point cloud data extracted fromTerrestrial Laser Scanner in two distinct case studies  involving differentgeological contexts: the basaltic cliff of Castellfollit de la Roca and theconglomerate Montserrat Massif  both located in Spain. Our experimental datasuggest that some of the above-mentioned machine learning pipelines can beutilized to detect rockfall incidents on mountain walls  with experimentallyproven accuracy.
http://arxiv.org/abs/2102.06454, Customizable Stochastic High Fidelity Model of the Sensors and Camera  onboard a Low SWaP Fixed Wing Autonomous Aircraft,The navigation systems of autonomous aircraft rely on the readings providedby a suite of onboard sensors to estimate the aircraft state. In the case offixed wing vehicles  the sensor suite is composed by triads of accelerometers gyroscopes  and magnetometers  a Global Navigation Satellite System (GNSS)receiver  and an air data system (Pitot tube  air vanes  thermometer  andbarometer)  and is often complemented by one or more digital cameras. Anaccurate representation of the behavior and error sources of each of thesesensors  together with the images generated by the cameras  in indispensablefor flight simulation and the evaluation of novel inertial or visual navigationalgorithms  and more so in the case of low SWaP (size  weight  and power)aircraft  in which the quality and price of the sensors is limited. Thisarticle presents realistic and customizable models for each of these sensors which have been implemented as an open-source C ++ simulation. Provided withthe true variation of the aircraft state with time  the simulation provides atime stamped series of the errors generated by all sensors  as well asrealistic images of the Earth surface that resemble those taken from a realcamera flying along the indicated state positions and attitudes.
http://arxiv.org/abs/2102.06455, On Signings and the Well-Founded Semantics,In this note  we use Kunen's notion of a signing to establish two theoremsabout the well-founded semantics of logic programs  in the case where we areinterested in only (say) the positive literals of a predicate  p  that areconsequences of the program. The first theorem identifies a class of programsfor which the well-founded and Fitting semantics coincide for the positive partof  p . The second theorem shows that if a program has a signing then computingthe positive part of  p  under the well-founded semantics requires thecomputation of only one part of each predicate. This theorem suggests ananalysis for query-answering under the well-founded semantics. In the processof proving these results  we use an alternative formulation of the well-foundedsemantics of logic programs  which might be of independent interest.
http://arxiv.org/abs/2102.06460, Depthwise Separable Convolutions Allow for Fast and Memory-Efficient  Spectral Normalization,An increasing number of models require the control of the spectral norm ofconvolutional layers of a neural network. While there is an abundance ofmethods for estimating and enforcing upper bounds on those during training they are typically costly in either memory or time. In this work  we introducea very simple method for spectral normalization of depthwise separableconvolutions  which introduces negligible computational and memory overhead. Wedemonstrate the effectiveness of our method on image classification tasks usingstandard architectures like MobileNetV2.
http://arxiv.org/abs/2102.06461, Business Model Canvas Should Pay More Attention to the Software Startup  Team,Business Model Canvas (BMC) is a tool widely used to describe startupbusiness models. Despite the various business aspects described  BMC pays alittle emphasis on team-related factors. The importance of team-related factorsin software development has been acknowledged widely in literature. While notas extensively studied  the importance of teams in software startups is alsoknown in both literature and among practitioners. In this paper  we proposepotential changes to BMC to have the tool better reflect the importance of theteam  especially in a software startup environment. Based on a literaturereview  we identify various components related to the team  which we thenfurther support with empirical data. We do so by means of a qualitative casestudy of five startups.
http://arxiv.org/abs/2102.06462, Amidst Uncertainty -- or Not? Decision-Making in Early-Stage Software  Startups,It is commonly claimed that the initial stages of any startup business aredominated by continuous  extended uncertainty  in an environment that has evenbeen described as chaotic. Consequently  decisions are made in uncertaincircumstances  so making the right decision is crucial to successful business.However  little currently exists in the way of empirical studies into thissupposed uncertainty. In this paper  we study decision-making in early-stagesoftware startups by means of a single  in-depth case study. Based on our data we argue that software startups do not work in a chaotic environment  nor arethey characterized by unique uncertainty unlike that experienced by otherfirms.
http://arxiv.org/abs/2102.06463, Reducing Waiting Times at Charging Stations with Adaptive Electric  Vehicle Route Planning,Electric vehicles are becoming more popular all over the world. Withincreasing battery capacities and a growing fast-charging infrastructure  theyare becoming suitable for long distance travel. However  queues at chargingstations could lead to long waiting times  making efficient route planning evenmore important. In general  optimal multi-objective route planning is extremelycomputationally expensive. We propose an adaptive charging and routingstrategy  which considers driving  waiting  and charging time. For this  wedeveloped a multi-criterion shortest-path search algorithm using contractionhierarchies. To further reduce the computational effort  we precomputeshortest-path trees between the known locations of the charging stations. Wepropose a central charging station database (CSDB) that helps estimatingwaiting times at charging stations ahead of time. This enables our adaptivecharging and routing strategy to reduce these waiting times. In an extensiveset of simulation experiments  we demonstrate the advantages of our concept which reduces average waiting times at charging stations by up to 97 %. Even ifonly a subset of the cars uses the CSDB approach  we can substantially reducewaiting times and thereby the total travel time of electric vehicles.
http://arxiv.org/abs/2102.06467, Robust Data-Driven Discovery of Partial Differential Equations under  Uncertainties,Robust physics (e.g.  governing equations and laws) discovery is of greatinterest for many engineering fields and explainable machine learning. Acritical challenge compared with general training is that the term and formatof governing equations is not known as a prior. In addition  significantmeasurement noise and complex algorithm hyperparameter tuning usually reducesthe robustness of existing methods. A robust data-driven method is proposed inthis study for identifying the governing Partial Differential Equations (PDEs)of a given system from noisy data. The proposed method is based on the conceptof Progressive Sparse Identification of PDEs (PSI-PDE or  \psi -PDE). Specialfocus is on the handling of data with huge uncertainties (e.g.  50 \%  noiselevel). Neural Network modeling and fast Fourier transform (FFT) areimplemented to reduce the influence of noise in sparse regression. Followingthis  candidate terms from the prescribed library are progressively selectedand added to the learned PDEs  which automatically promotes parsimony withrespect to the number of terms in PDEs as well as their complexity. Next  thesignificance of each learned terms is further evaluated and the coefficients ofPDE terms are optimized by minimizing the L2 residuals. Results of numericalcase studies indicate that the governing PDEs of many canonical dynamicalsystems can be correctly identified using the proposed  \psi -PDE method withhighly noisy data. One great benefit of proposed algorithm is that it avoidscomplex algorithm modification and hyperparameter tuning in most existingmethods. Limitations of the proposed method and major findings are presented.
http://arxiv.org/abs/2102.06469, When no news is bad news -- Detection of negative events from news media  content,During the first wave of Covid-19 information decoupling could be observed inthe flow of news media content. The corollary of the content alignment withinand between news sources experienced by readers (i.e.  all news transformedinto Corona-news)  was that the novelty of news content went down as mediafocused monotonically on the pandemic event. This all-important Covid-19 newstheme turned out to be quite persistent as the pandemic continued  resulting inthe  from a news media's perspective  paradoxical situation where the same newswas repeated over and over. This information phenomenon  where noveltydecreases and persistence increases  has previously been used to track changein news media  but in this study we specifically test the claim that newinformation decoupling behavior of media can be used to reliably detect changein news media content originating in a negative event  using a Bayesianapproach to change point detection.
http://arxiv.org/abs/2102.06474, Predicting and Attending to Damaging Collisions for Placing Everyday  Objects in Photo-Realistic Simulations,Placing objects is a fundamental task for domestic service robots (DSRs).Thus  inferring the collision-risk before a placing motion is crucial forachieving the requested task. This problem is particularly challenging becauseit is necessary to predict what happens if an object is placed in a cluttereddesignated area. We show that a rule-based approach that uses plane detection to detect free areas  performs poorly. To address this  we develop PonNet which has multimodal attention branches and a self-attention mechanism topredict damaging collisions  based on RGBD images. Our method can visualize therisk of damaging collisions  which is convenient because it enables the user tounderstand the risk. For this purpose  we build and publish an original datasetthat contains 12 000 photo-realistic images of specific placing areas  withdaily life objects  in home environments. The experimental results show thatour approach improves accuracy compared with the baseline methods.
http://arxiv.org/abs/2102.06476, Interpretable Predictive Maintenance for Hard Drives,Existing machine learning approaches for data-driven predictive maintenanceare usually black boxes that claim high predictive power yet cannot beunderstood by humans. This limits the ability of humans to use these models toderive insights and understanding of the underlying failure mechanisms  andalso limits the degree of confidence that can be placed in such a system toperform well on future data. We consider the task of predicting hard drivefailure in a data center using recent algorithms for interpretable machinelearning. We demonstrate that these methods provide meaningful insights aboutshort- and long-term drive health  while also maintaining high predictiveperformance. We also show that these analyses still deliver useful insightseven when limited historical data is available  enabling their use insituations where data collection has only recently begun.
http://arxiv.org/abs/2102.06477, A Non-Intrusive Machine Learning Solution for Malware Detection and Data  Theft Classification in Smartphones,Smartphones contain information that is more sensitive and personal thanthose found on computers and laptops. With an increase in the versatility ofsmartphone functionality  more data has become vulnerable and exposed toattackers. Successful mobile malware attacks could steal a user's location photos  or even banking information. Due to a lack of post-attack strategiesfirms also risk going out of business due to data theft. Thus  there is a needbesides just detecting malware intrusion in smartphones but to also identifythe data that has been stolen to assess  aid in recovery and prevent futureattacks. In this paper  we propose an accessible  non-intrusive machinelearning solution to not only detect malware intrusion but also identify thetype of data stolen for any app under supervision. We do this with Androidusage data obtained by utilising publicly available data collection framework-SherLock. We test the performance of our architecture for multiple users onreal-world data collected using the same framework. Our architecture exhibitsless than 9% inaccuracy in detecting malware and can classify with 83%certainty on the type of data that is being stolen.
http://arxiv.org/abs/2102.06479, Complete Bidirectional Typing for the Calculus of Inductive  Constructions,This article presents a bidirectional type system for the Calculus ofInductive Constructions (CIC). It introduces a novel judgement intermediatebetween the usual inference and checking  dubbed constrained inference  tohandle the presence of computation in types. The key property is thecompleteness of the system with respect to the usual undirected one  which hasbeen formally proven in Coq as a part of the MetaCoq project. Although it playsa central role in an ongoing completeness proof for a realistic typingalgorithm  the interest of bidirectionality is much wider  as it clarifiesprevious works in the area and gives strong insights and structure when tryingto prove properties on CIC or design variations and extensions.
http://arxiv.org/abs/2102.06480, Bootstrapped Representation Learning on Graphs,Current state-of-the-art self-supervised learning methods for graph neuralnetworks (GNNs) are based on contrastive learning. As such  they heavily dependon the construction of augmentations and negative examples. For example  on thestandard PPI benchmark  increasing the number of negative pairs improvesperformance  thereby requiring computation and memory cost quadratic in thenumber of nodes to achieve peak performance. Inspired by BYOL  a recentlyintroduced method for self-supervised learning that does not require negativepairs  we present Bootstrapped Graph Latents  BGRL  a self-supervised graphrepresentation method that gets rid of this potentially quadratic bottleneck.BGRL outperforms or matches the previous unsupervised state-of-the-art resultson several established benchmark datasets. Moreover  it enables the effectiveusage of graph attentional (GAT) encoders  allowing us to further improve thestate of the art. In particular on the PPI dataset  using GAT as an encoder weachieve state-of-the-art 70.49% Micro-F1  using the linear evaluation protocol.On all other datasets under consideration  our model is competitive with theequivalent supervised GNN results  often exceeding them.
http://arxiv.org/abs/2102.06481, How do climate change skeptics engage with opposing views? Understanding  mechanisms of social identity and cognitive dissonance in an online forum,Does engagement with opposing views help break down ideological `echochambers'; or does it backfire and reinforce them? This question remainscritical as academics  policymakers and activists grapple with the question ofhow to regulate political discussion on social media. In this study  wecontribute to the debate by examining the impact of opposing views within amajor climate change skeptic online community on Reddit. A large sample ofposts (N = 3000) was manually coded as either dissonant or consonant whichallowed the automated classification of the full dataset of more than 50 000posts  with codes inferred from linked websites. We find that ideologicallydissonant submissions act as a stimulant to activity in the community: theyreceived more attention (comments) than consonant submissions  even though theyreceived lower scores through up-voting and down-voting. Users who engaged withdissonant submissions were also more likely to return to the forum. Consistentwith identity theory  confrontation with opposing views triggered activity inthe forum  particularly among users that are highly engaged with the community.In light of the findings  theory of social identity and echo chambers isdiscussed and enhanced.
http://arxiv.org/abs/2102.06483, VitrAI -- Applying Explainable AI in the Real World,With recent progress in the field of Explainable Artificial Intelligence(XAI) and increasing use in practice  the need for an evaluation of differentXAI methods and their explanation quality in practical usage scenarios arises.For this purpose  we present VitrAI  which is a web-based service with the goalof uniformly demonstrating four different XAI algorithms in the context ofthree real life scenarios and evaluating their performance andcomprehensibility for humans. This work reveals practical obstacles whenadopting XAI methods and gives qualitative estimates on how well differentapproaches perform in said scenarios.
http://arxiv.org/abs/2102.06485, Leveraging Reinforcement Learning for evaluating Robustness of KNN  Search Algorithms,The problem of finding K-nearest neighbors in the given dataset for a givenquery point has been worked upon since several years. In very high dimensionalspaces the K-nearest neighbor search (KNNS) suffers in terms of complexity incomputation of high dimensional distances. With the issue of curse ofdimensionality  it gets quite tedious to reliably bank on the results ofvariety approximate nearest neighbor search approaches. In this paper  wesurvey some novel K-Nearest Neighbor Search approaches that tackles the problemof Search from the perspectives of computations  the accuracy of approximatedresults and leveraging parallelism to speed-up computations. We attempt toderive a relationship between the true positive and false points for a givenKNNS approach. Finally  in order to evaluate the robustness of a KNNS approachagainst adversarial points  we propose a generic Reinforcement Learning basedframework for the same.
http://arxiv.org/abs/2102.06486, Mutually exciting point process graphs for modelling dynamic networks,A new class of models for dynamic networks is proposed  called mutuallyexciting point process graphs (MEG)  motivated by a practical application incomputer network security. MEG is a scalable network-wide statistical model forpoint processes with dyadic marks  which can be used for anomaly detection whenassessing the significance of previously unobserved connections. The modelcombines mutually exciting point processes to estimate dependencies betweenevents and latent space models to infer relationships between the nodes. Theintensity functions for each network edge are parameterised exclusively bynode-specific parameters  which allows information to be shared across thenetwork. Fast inferential procedures using modern gradient ascent algorithmsare exploited. The model is tested on simulated graphs and real world computernetwork datasets  demonstrating excellent performance.
http://arxiv.org/abs/2102.06489, A Tale of Two Countries: A Longitudinal Cross-Country Study of Mobile  Users' Reactions to the COVID-19 Pandemic Through the Lens of App Popularity,The ongoing COVID-19 pandemic has profoundly impacted people's life aroundthe world  including how they interact with mobile technologies. In this paper we seek to develop an understanding of how the dynamic trajectory of a pandemicshapes mobile phone users' experiences. Through the lens of app popularity  weapproach this goal from a cross-country perspective. We compile a datasetconsisting of six-month daily snapshots of the most popular apps in the iOS AppStore in China and the US  where the pandemic has exhibited distincttrajectories. Using this longitudinal dataset  our analysis provides detailedpatterns of app ranking during the pandemic at both category and individual applevels. We reveal that app categories' rankings are correlated with thepandemic  contingent upon country-specific development trajectories. Our workoffers rich insights into how the COVID-19  a typical global public healthcrisis  has influence people's day-to-day interaction with the Internet andmobile technologies.
http://arxiv.org/abs/2102.06491, Improving Object Detection in Art Images Using Only Style Transfer,Despite recent advances in object detection using deep learning neuralnetworks  these neural networks still struggle to identify objects in artimages such as paintings and drawings. This challenge is known as the crossdepiction problem and it stems in part from the tendency of neural networks toprioritize identification of an object's texture over its shape. In this paperwe propose and evaluate a process for training neural networks to localizeobjects - specifically people - in art images. We generate a large dataset fortraining and validation by modifying the images in the COCO dataset using AdaInstyle transfer. This dataset is used to fine-tune a Faster R-CNN objectdetection network  which is then tested on the existing People-Art testingdataset. The result is a significant improvement on the state of the art and anew way forward for creating datasets to train neural networks to process artimages.
http://arxiv.org/abs/2102.06492, Algebraic cocompleteness and finitary functors,A number of categories is presented that are algebraically complete andcocomplete  i.e.  every endofunctor has an initial algebra and a terminalcoalgebra. For all finitary (and  more generally  all precontinuous) setfunctors the initial algebra and terminal coalgebra are proved to carry acanonical partial order with the same ideal CPO-completion. And they also bothcarry a canonical ultrametric with the same Cauchy completion.
http://arxiv.org/abs/2102.06495, CrossStack: A 3-D Reconfigurable RRAM Crossbar Inference Engine,Deep neural network inference accelerators are rapidly growing in importanceas we turn to massively parallelized processing beyond GPUs and ASICs. Thedominant operation in feedforward inference is the multiply-and-accumlateprocess  where each column in a crossbar generates the current response of asingle neuron. As a result  memristor crossbar arrays parallelize inference andimage processing tasks very efficiently. In this brief  we present a 3-D activememristor crossbar array `CrossStack'  which adopts stacked pairs ofAl/TiO2/TiO2-x/Al devices with common middle electrodes. By designingCMOS-memristor hybrid cells used in the layout of the array  CrossStack canoperate in one of two user-configurable modes as a reconfigurable inferenceengine: 1) expansion mode and 2) deep-net mode. In expansion mode  theresolution of the network is doubled by increasing the number of inputs for agiven chip area  reducing IR drop by 22%. In deep-net mode  inference speedper-10-bit convolution is improved by 29\% by simultaneously using oneTiO2/TiO2-x layer for read processes  and the other for write processes. Weexperimentally verify both modes on our  10 imes10 imes2  array.
http://arxiv.org/abs/2102.06496, Lazy Hermite Reduction and Creative Telescoping for Algebraic Functions,We present criteria on the existence of telescopers for trivariate rationalfunctions in four mixed cases  in which discrete and continuous variablesappear simultaneously. We reduce the existence problem in the trivariate caseto the exactness testing problem  the separation problem and the existenceproblem in the bivariate case. The existence criteria we present help usdetermine the termination of Zeilberger's algorithm for the input functionsstudied in this paper.
http://arxiv.org/abs/2102.06500, Jacobian Determinant of Normalizing Flows,Normalizing flows learn a diffeomorphic mapping between the target and basedistribution  while the Jacobian determinant of that mapping forms anotherreal-valued function. In this paper  we show that the Jacobian determinantmapping is unique for the given distributions  hence the likelihood objectiveof flows has a unique global optimum. In particular  the likelihood for a classof flows is explicitly expressed by the eigenvalues of the auto-correlationmatrix of individual data point  and independent of the parameterization ofneural network  which provides a theoretical optimal value of likelihoodobjective and relates to probabilistic PCA. Additionally  Jacobian determinantis a measure of local volume change and is maximized when MLE is used foroptimization. To stabilize normalizing flows training  it is required tomaintain a balance between the expansiveness and contraction of volume  meaningLipschitz constraint on the diffeomorphic mapping and its inverse. With thesetheoretical results  several principles of designing normalizing flow wereproposed. And numerical experiments on highdimensional datasets (such asCelebA-HQ 1024x1024) were conducted to show the improved stability of training.
http://arxiv.org/abs/2102.06501, Two Training Strategies for Improving Relation Extraction over Universal  Graph,This paper explores how the Distantly Supervised Relation Extraction (DS-RE)can benefit from the use of a Universal Graph (UG)  the combination of aKnowledge Graph (KG) and a large-scale text collection. A straightforwardextension of a current state-of-the-art neural model for DS-RE with a UG maylead to degradation in performance. We first report that this degradation isassociated with the difficulty in learning a UG and then propose two trainingstrategies: (1) Path Type Adaptive Pretraining  which sequentially trains themodel with different types of UG paths so as to prevent the reliance on asingle type of UG path; and (2) Complexity Ranking Guided Attention mechanism which restricts the attention span according to the complexity of a UG path soas to force the model to extract features not only from simple UG paths butalso from complex ones. Experimental results on both biomedical and NYT10datasets prove the robustness of our methods and achieve a new state-of-the-artresult on the NYT10 dataset. The code and datasets used in this paper areavailable at https://github.com/baodaiqin/UGDSRE.
http://arxiv.org/abs/2102.06502, Computing Betweenness Centrality in Link Streams,Betweeness centrality is one of the most important concepts in graphanalysis. It was recently extended to link streams  a graph generalizationwhere links arrive over time. However  its computation raises non-trivialissues  due in particular to the fact that time is considered as continuous. Weprovide here the first algorithms to compute this generalized betweennesscentrality  as well as several companion algorithms that have their owninterest. They work in polynomial time and space  we illustrate them on typicalexamples  and we provide an implementation.
http://arxiv.org/abs/2102.06503, A Little Pretraining Goes a Long Way: A Case Study on Dependency Parsing  Task for Low-resource Morphologically Rich Languages,Neural dependency parsing has achieved remarkable performance for manydomains and languages. The bottleneck of massive labeled data limits theeffectiveness of these approaches for low resource languages. In this work  wefocus on dependency parsing for morphological rich languages (MRLs) in alow-resource setting. Although morphological information is essential for thedependency parsing task  the morphological disambiguation and lack of powerfulanalyzers pose challenges to get this information for MRLs. To address thesechallenges  we propose simple auxiliary tasks for pretraining. We performexperiments on 10 MRLs in low-resource settings to measure the efficacy of ourproposed pretraining method and observe an average absolute gain of 2 points(UAS) and 3.6 points (LAS). Code and data available at:https://github.com/jivnesh/LCM
http://arxiv.org/abs/2102.06504, From System Level Synthesis to Robust Closed-loop Data-enabled  Predictive Control,The Willem's fundamental lemma and the system level synthesis characterizetrajectories generated by a linear system. While the former method is valid fordeterministic LTI systems  the latter one is further effective for stochasticlinear time varying systems. In this paper  these two methods offers equivalentcharacterization of stochastic LTI systems. Inspired by this observation  arobust closed-loop data-enabled predictive control scheme is proposed forstochastic LTI systems. A causal feedback structure is further derived  leadingto an computational cost similar to standard robust MPC with full statemeasurements.
http://arxiv.org/abs/2102.06505, Exploiting Spline Models for the Training of Fully Connected Layers in  Neural Network,The fully connected (FC) layer  one of the most fundamental modules inartificial neural networks (ANN)  is often considered difficult and inefficientto train due to issues including the risk of overfitting caused by its largeamount of parameters. Based on previous work studying ANN from linear splineperspectives  we propose a spline-based approach that eases the difficulty oftraining FC layers. Given some dataset  we first obtain a continuous piece-wiselinear (CPWL) fit through spline methods such as multivariate adaptiveregression spline (MARS). Next  we construct an ANN model from the linearspline model and continue to train the ANN model on the dataset using gradientdescent optimization algorithms. Our experimental results and theoreticalanalysis show that our approach reduces the computational cost  accelerates theconvergence of FC layers  and significantly increases the interpretability ofthe resulting model (FC layers) compared with standard ANN training with randomparameter initialization followed by gradient descent optimizations.
http://arxiv.org/abs/2102.06507, Online Graph Dictionary Learning,Dictionary learning is a key tool for representation learning  that explainsthe data as linear combination of few basic elements. Yet  this analysis is notamenable in the context of graph learning  as graphs usually belong todifferent metric spaces. We fill this gap by proposing a new online GraphDictionary Learning approach  which uses the Gromov Wasserstein divergence forthe data fitting term. In our work  graphs are encoded through their nodes'pairwise relations and modeled as convex combination of graph atoms  i.e.dictionary elements  estimated thanks to an online stochastic algorithm  whichoperates on a dataset of unregistered graphs with potentially different numberof nodes. Our approach naturally extends to labeled graphs  and is completed bya novel upper bound that can be used as a fast approximation of GromovWasserstein in the embedding space. We provide numerical evidences showing theinterest of our approach for unsupervised embedding of graph datasets and foronline graph subspace estimation and tracking.
http://arxiv.org/abs/2102.06509, Updatable Materialization of Approximate Constraints,Modern big data applications integrate data from various sources. As aresult  these datasets may not satisfy perfect constraints  leading to sparseschema information and non-optimal query performance. The existing approach ofPatchIndexes enable the definition of approximate constraints and improve queryperformance by exploiting the materialized constraint information. As realworld data warehouse workloads are often not limited to read-only queries  weenhance the PatchIndex structure towards an update-conscious design in thispaper. Therefore  we present a sharded bitmap as the underlying data structurewhich offers efficient update operations  and describe approaches to maintainapproximate constraints under updates  avoiding index recomputations and fulltable scans. In our evaluation  we prove that PatchIndexes significantly impactquery performance while achieving lightweight update support.
http://arxiv.org/abs/2102.06511, Continuous Learning in Neural Machine Translation using Bilingual  Dictionaries,While recent advances in deep learning led to significant improvements inmachine translation  neural machine translation is often still not able tocontinuously adapt to the environment. For humans  as well as for machinetranslation  bilingual dictionaries are a promising knowledge source tocontinuously integrate new knowledge. However  their exploitation poses severalchallenges: The system needs to be able to perform one-shot learning as well asmodel the morphology of source and target language.In this work  we proposed an evaluation framework to assess the ability ofneural machine translation to continuously learn new phrases. We integrateone-shot learning methods for neural machine translation with different wordrepresentations and show that it is important to address both in order tosuccessfully make use of bilingual dictionaries. By addressing both challengeswe are able to improve the ability to translate new  rare words and phrasesfrom 30% to up to 70%. The correct lemma is even generated by more than 90%.
http://arxiv.org/abs/2102.06513, How Far Should We Look Back to Achieve Effective Real-Time Time-Series  Anomaly Detection?,Anomaly detection is the process of identifying unexpected events orab-normalities in data  and it has been applied in many different areas such assystem monitoring  fraud detection  healthcare  intrusion detection  etc.Providing real-time  lightweight  and proactive anomaly detection for timeseries with neither human intervention nor domain knowledge could be highlyvaluable since it reduces human effort and enables appropriate countermeasuresto be undertaken before a disastrous event occurs. To our knowledge  RePAD(Real-time Proactive Anomaly Detection algorithm) is a generic approach withall above-mentioned features. To achieve real-time and lightweight detection RePAD utilizes Long Short-Term Memory (LSTM) to detect whether or not eachupcoming data point is anomalous based on short-term historical data points.However  it is unclear that how different amounts of historical data pointsaffect the performance of RePAD. Therefore  in this paper  we investigate theimpact of different amounts of historical data on RePAD by introducing a set ofperformance metrics that cover novel detection accuracy measures  timeefficiency  readiness  and resource consumption  etc. Empirical experimentsbased on real-world time series datasets are conducted to evaluate RePAD indifferent scenarios  and the experimental results are presented and discussed.
http://arxiv.org/abs/2102.06514, Querying collections of tree-structured records in the presence of  within-record referential constraints,In this paper  we consider a tree-structured data model used in manycommercial databases like Dremel  F1  JSON. We define identity and referentialconstraints within each tree-structured record. The query language is a variantof SQL and flattening is used as an evaluation mechanism. We investigatequerying in the presence of these constraints  and point out the challengesthat arise from taking them into account during query evaluation.
http://arxiv.org/abs/2102.06515, Analysis of Interpolation based Image In-painting Approaches,Interpolation and internal painting are one of the basic approaches in imageinternal painting  which is used to eliminate undesirable parts that occur indigital images or to enhance faulty parts. This study was designed to comparethe interpolation algorithms used in image in-painting in the literature.Errors and noise generated on the colour and grayscale formats of some of thecommonly used standard images in the literature were corrected by using Cubic Kriging  Radial based function and High dimensional model representationapproaches and the results were compared using standard image comparisoncriteria  namely  PSNR (peak signal-to-noise ratio)  SSIM (StructuralSIMilarity)  Mean Square Error (MSE). According to the results obtained fromthe study  the absolute superiority of the methods against each other was notobserved. However  Kriging and RBF interpolation give better results both fornumerical data and visual evaluation for image in-painting problems with largearea losses.
http://arxiv.org/abs/2102.06516, Work-Optimal Parallel Minimum Cuts for Non-Sparse Graphs,We present the first work-optimal polylogarithmic-depth parallel algorithmfor the minimum cut problem on non-sparse graphs. For  m\geq n^{1+\epsilon} for any constant  \epsilon>0   our algorithm requires  O(m \log n)  work and O(\log^3 n)  depth and succeeds with high probability. Its work matches thebest  O(m \log n)  runtime for sequential algorithms [MN STOC 2020  GMW SOSA2021]. This improves the previous best work by Geissmann and Gianinazzi [SPAA2018] by  O(\log^3 n)  factor  while matching the depth of their algorithm. Todo this  we design a work-efficient approximation algorithm and parallelize therecent sequential algorithms [MN STOC 2020; GMW SOSA 2021] that exploit aconnection between 2-respecting minimum cuts and 2-dimensional orthogonal rangesearching.
http://arxiv.org/abs/2102.06518, User manual for bch  a program for the fast computation of the  Baker-Campbell-Hausdorff and similar series,This manual describes bch  an efficient program written in the C programminglanguage for the fast computation of the Baker-Campbell-Hausdorff (BCH) andsimilar Lie series. The Lie series can be represented in the Lyndon basis  inthe classical Hall basis  or in the right-normed basis of E.S. Chibrikov. Inthe Lyndon basis  which proves to be particularly efficient for this purpose the computation of 111013 coefficients for the BCH series up to terms of degree20 takes less than half a second on an ordinary personal computer and requiresnegligible 11MB of memory. Up to terms of degree 30  which is the maximumdegree the program can handle  the computation of 74248451 coefficients takes55 hours but still requires only a modest 5.5GB of memory.
http://arxiv.org/abs/2102.06520, Estimation and Applications of Quantiles in Deep Binary Classification,Quantile regression  based on check loss  is a widely used inferentialparadigm in Econometrics and Statistics. The conditional quantiles provide arobust alternative to classical conditional means  and also allow uncertaintyquantification of the predictions  while making very few distributionalassumptions. We consider the analogue of check loss in the binaryclassification setting. We assume that the conditional quantiles are smoothfunctions that can be learnt by Deep Neural Networks (DNNs). Subsequently  wecompute the Lipschitz constant of the proposed loss  and also show that itscurvature is bounded  under some regularity conditions. Consequently  recentresults on the error rates and DNN architecture complexity become directlyapplicable.We quantify the uncertainty of the class probabilities in terms of predictionintervals  and develop individualized confidence scores that can be used todecide whether a prediction is reliable or not at scoring time. By aggregatingthe confidence scores at the dataset level  we provide two additional metrics model confidence  and retention rate  to complement the widely used classifiersummaries. We also the robustness of the proposed non-parametric binaryquantile classification framework are also studied  and we demonstrate how toobtain several univariate summary statistics of the conditional distributions in particular conditional means  using smoothed conditional quantiles  allowingthe use of explanation techniques like Shapley to explain the mean predictions.Finally  we demonstrate an efficient training regime for this loss based onStochastic Gradient Descent with Lipschitz Adaptive Learning Rates (LALR).
http://arxiv.org/abs/2102.06521, Improving Zero-shot Neural Machine Translation on Language-specific  Encoders-Decoders,Recently  universal neural machine translation (NMT) with sharedencoder-decoder gained good performance on zero-shot translation. Unlikeuniversal NMT  jointly trained language-specific encoders-decoders aim toachieve universal representation across non-shared modules  each of which isfor a language or language family. The non-shared architecture has theadvantage of mitigating internal language competition  especially when theshared vocabulary and model parameters are restricted in their size. However the performance of using multiple encoders and decoders on zero-shottranslation still lags behind universal NMT. In this work  we study zero-shottranslation using language-specific encoders-decoders. We propose to generalizethe non-shared architecture and universal NMT by differentiating theTransformer layers between language-specific and interlingua. By selectivelysharing parameters and applying cross-attentions  we explore maximizing therepresentation universality and realizing the best alignment oflanguage-agnostic information. We also introduce a denoising auto-encoding(DAE) objective to jointly train the model with the translation task in amulti-task manner. Experiments on two public multilingual parallel datasetsshow that our proposed model achieves a competitive or better results thanuniversal NMT and strong pivot baseline. Moreover  we experiment incrementallyadding new language to the trained model by only updating the new modelparameters. With this little effort  the zero-shot translation between thisnewly added language and existing languages achieves a comparable result withthe model trained jointly from scratch on all languages.
http://arxiv.org/abs/2102.06522, Fuzzing Symbolic Expressions,Recent years have witnessed a wide array of results in software testing exploring different approaches and methodologies ranging from fuzzers tosymbolic engines  with a full spectrum of instances in between such as concolicexecution and hybrid fuzzing. A key ingredient of many of these tools isSatisfiability Modulo Theories (SMT) solvers  which are used to reason oversymbolic expressions collected during the analysis. In this paper  weinvestigate whether techniques borrowed from the fuzzing domain can be appliedto check whether symbolic formulas are satisfiable in the context of concolicand hybrid fuzzing engines  providing a viable alternative to classic SMTsolving techniques. We devise a new approximate solver  FUZZY-SAT  and showthat it is both competitive with and complementary to state-of-the-art solverssuch as Z3 with respect to handling queries generated by hybrid fuzzers.
http://arxiv.org/abs/2102.06525, Reviving Iterative Training with Mask Guidance for Interactive  Segmentation,Recent works on click-based interactive segmentation have demonstratedstate-of-the-art results by using various inference-time optimization schemes.These methods are considerably more computationally expensive compared tofeedforward approaches  as they require performing backward passes through anetwork during inference and are hard to deploy on mobile frameworks thatusually support only forward passes. In this paper  we extensively evaluatevarious design choices for interactive segmentation and discover that newstate-of-the-art results can be obtained without any additional optimizationschemes. Thus  we propose a simple feedforward model for click-basedinteractive segmentation that employs the segmentation masks from previoussteps. It allows not only to segment an entirely new object  but also to startwith an external mask and correct it. When analyzing the performance of modelstrained on different datasets  we observe that the choice of a training datasetgreatly impacts the quality of interactive segmentation. We find that themodels trained on a combination of COCO and LVIS with diverse and high-qualityannotations show performance superior to all existing models. The code andtrained models are available athttps://github.com/saic-vul/ritm_interactive_segmentation.
http://arxiv.org/abs/2102.06526, On the Application of BAC-NOMA to 6G umMTC,This letter studies the application of backscatter communications (BackCom)assisted non-orthogonal multiple access (BAC-NOMA) to the envisionedsixth-generation (6G) ultra-massive machine type communications (umMTC). Inparticular  the proposed BAC-NOMA transmission scheme can realize simultaneousenergy and spectrum cooperation between uplink and downlink users  which isimportant to support massive connectivity and stringent energy constraints inumMTC. Furthermore  a resource allocation problem for maximizing the uplinkthroughput and suppressing the interference between downlink and uplinktransmission is formulated as an optimization problem and the correspondingoptimal resource allocation policy is obtained. Computer simulations areprovided to demonstrate the superior performance of BAC-NOMA.
http://arxiv.org/abs/2102.06527, A Computability Perspective on (Verified) Machine Learning,There is a strong consensus that combining the versatility of machinelearning with the assurances given by formal verification is highly desirable.It is much less clear what verified machine learning should mean exactly. Weconsider this question from the (unexpected?) perspective of computableanalysis. This allows us to define the computational tasks underlying verifiedML in a model-agnostic way  and show that they are in principle computable.
http://arxiv.org/abs/2102.06528, Disturbing Reinforcement Learning Agents with Corrupted Rewards,Reinforcement Learning (RL) algorithms have led to recent successes insolving complex games  such as Atari or Starcraft  and to a huge impact inreal-world applications  such as cybersecurity or autonomous driving. In theside of the drawbacks  recent works have shown how the performance of RLalgorithms decreases under the influence of soft changes in the rewardfunction. However  little work has been done about how sensitive thesedisturbances are depending on the aggressiveness of the attack and the learningexploration strategy. In this paper  we propose to fill this gap in theliterature analyzing the effects of different attack strategies based on rewardperturbations  and studying the effect in the learner depending on itsexploration strategy. In order to explain all the behaviors  we choose asub-class of MDPs: episodic  stochastic goal-only-rewards MDPs  and inparticular  an intelligible grid domain as a benchmark. In this domain  wedemonstrate that smoothly crafting adversarial rewards are able to mislead thelearner  and that using low exploration probability values  the policy learnedis more robust to corrupt rewards. Finally  in the proposed learning scenario a counterintuitive result arises: attacking at each learning episode is thelowest cost attack strategy.
http://arxiv.org/abs/2102.06529, A taxonomy for quality in simulation-based development and testing of  automated driving systems,Ensuring the safety and performance requirements of automated driving systemsis a major challenge for the automotive industry. One way to tackle thisproblem is a simulation-based approach. However  to be able to rely on resultsgenerated by this method  the simulation process needs to fulfill certainquality criteria depending on the intended usage. Hence  quality should bemeasured and determined at many different levels and areas of the testing anddeveloping landscape  providing information with varying degrees ofabstraction. Additionally  quality not only has to be assessed for a completeautomated driving system but also for the simulation models that approximatethe vehicles' components before they can be used for simulation. This taxonomyprovides a better understanding of the concept of quality during thedevelopment and test process and introduces the possibility to systematicallyevaluate whether development steps in this process need to be repeated orfurther assessed.
http://arxiv.org/abs/2102.06532, PAC-BUS: Meta-Learning Bounds via PAC-Bayes and Uniform Stability,We are motivated by the problem of providing strong generalization guaranteesin the context of meta-learning. Existing generalization bounds are eitherchallenging to evaluate or provide vacuous guarantees in even relatively simplesettings. We derive a probably approximately correct (PAC) bound forgradient-based meta-learning using two different generalization frameworks inorder to deal with the qualitatively different challenges of generalization atthe  base  and  meta  levels. We employ bounds for uniformly stable algorithmsat the base level and bounds from the PAC-Bayes framework at the meta level.The result is a PAC-bound that is tighter when the base learner adapts quickly which is precisely the goal of meta-learning. We show that our bound provides atighter guarantee than other bounds on a toy non-convex problem on the unitsphere and a text-based classification example. We also present a practicalregularization scheme motivated by the bound in settings where the bound isloose and demonstrate improved performance over baseline techniques.
http://arxiv.org/abs/2102.06535, VSync: Push-Button Verification and Optimization for Synchronization  Primitives on Weak Memory Models (Technical Report),This technical report contains material accompanying our work with same titlepublished at ASPLOS'21. We start in Sec. 1 with a detailed presentation of thecore innovation of this work  Await Model Checking (AMC). The correctnessproofs of AMC can be found in Sec. 2. Next  we discuss three study cases inSec. 3  presenting bugs found and challenges encountered when applying VSync toexisting code bases. Finally  in Sec. 4 we describe the setup details of ourevaluation and report further experimental results.
http://arxiv.org/abs/2102.06536, Outdoor inverse rendering from a single image using multiview  self-supervision,In this paper we show how to perform scene-level inverse rendering to recovershape  reflectance and lighting from a single  uncontrolled image using a fullyconvolutional neural network. The network takes an RGB image as input regresses albedo  shadow and normal maps from which we infer least squaresoptimal spherical harmonic lighting coefficients. Our network is trained usinglarge uncontrolled multiview and timelapse image collections without groundtruth. By incorporating a differentiable renderer  our network can learn fromself-supervision. Since the problem is ill-posed we introduce additionalsupervision. Our key insight is to perform offline multiview stereo (MVS) onimages containing rich illumination variation. From the MVS pose and depthmaps  we can cross project between overlapping views such that Siamese trainingcan be used to ensure consistent estimation of photometric invariants. MVSdepth also provides direct coarse supervision for normal map estimation. Webelieve this is the first attempt to use MVS supervision for learning inverserendering. In addition  we learn a statistical natural illumination prior. Weevaluate performance on inverse rendering  normal map estimation and intrinsicimage decomposition benchmarks.
http://arxiv.org/abs/2102.06538, Neural Architecture Search as Program Transformation Exploration,Improving the performance of deep neural networks (DNNs) is important to boththe compiler and neural architecture search (NAS) communities. Compilers applyprogram transformations in order to exploit hardware parallelism and memoryhierarchy. However  legality concerns mean they fail to exploit the naturalrobustness of neural networks. In contrast  NAS techniques mutate networks byoperations such as the grouping or bottlenecking of convolutions  exploitingthe resilience of DNNs. In this work  we express such neural architectureoperations as program transformations whose legality depends on a notion ofrepresentational capacity. This allows them to be combined with existingtransformations into a unified optimization framework. This unification allowsus to express existing NAS operations as combinations of simplertransformations. Crucially  it allows us to generate and explore new tensorconvolutions. We prototyped the combined framework in TVM and were able to findoptimizations across different DNNs  that significantly reduce inference time -over 3  imes  in the majority of cases.Furthermore  our scheme dramatically reduces NAS search time. Code isavailableat~\href{https://github.com/jack-willturner/nas-as-program-transformation-exploration}{thishttps url}.
http://arxiv.org/abs/2102.06539, 3D-1D coupling on non conforming meshes via three-field optimization  based domain decomposition,A new numerical approach is proposed for the simulation of coupledthree-dimensional and one-dimensional elliptic equations (3D-1D coupling)arising from dimensionality reduction of 3D-3D problems with thin inclusions.The method is based on a well posed mathematical formulation and results in anumerical scheme with high robustness and flexibility in handling geometricalcomplexities. This is achieved by means of a three-field approach to split the1D problems from the bulk 3D problem  and then resorting to the minimization ofa properly designed functional to impose matching conditions at the interfaces.Thanks to the structure of the functional  the method allows the use ofindependent meshes for the various subdomains.
http://arxiv.org/abs/2102.06540, Modeling Dynamic User Interests: A Neural Matrix Factorization Approach,In recent years  there has been significant interest in understanding users'online content consumption patterns. But  the unstructured  high-dimensional and dynamic nature of such data makes extracting valuable insights challenging.Here we propose a model that combines the simplicity of matrix factorizationwith the flexibility of neural networks to efficiently extract nonlinearpatterns from massive text data collections relevant to consumers' onlineconsumption patterns. Our model decomposes a user's content consumption journeyinto nonlinear user and content factors that are used to model their dynamicinterests. This natural decomposition allows us to summarize each user'scontent consumption journey with a dynamic probabilistic weighting over a setof underlying content attributes. The model is fast to estimate  easy tointerpret and can harness external data sources as an empirical prior. Theseadvantages make our method well suited to the challenges posed by moderndatasets. We use our model to understand the dynamic news consumption interestsof Boston Globe readers over five years. Thorough qualitative studies including a crowdsourced evaluation  highlight our model's ability toaccurately identify nuanced and coherent consumption patterns. These resultsare supported by our model's superior and robust predictive performance overseveral competitive baseline methods.
http://arxiv.org/abs/2102.06543, Semantically-Conditioned Negative Samples for Efficient Contrastive  Learning,Negative sampling is a limiting factor w.r.t. the generalization ofmetric-learned neural networks. We show that uniform negative sampling provideslittle information about the class boundaries and thus propose three noveltechniques for efficient negative sampling: drawing negative samples from (1)the top- k  most semantically similar classes  (2) the top- k  mostsemantically similar samples and (3) interpolating between contrastive latentrepresentations to create pseudo negatives. Our experiments on CIFAR-10 CIFAR-100 and Tiny-ImageNet-200 show that our proposed  extit{SemanticallyConditioned Negative Sampling} and Latent Mixup lead to consistent performanceimprovements. In the standard supervised learning setting  on average weincrease test accuracy by 1.52\% percentage points on CIFAR-10 across variousnetwork architectures. In the knowledge distillation setting  (1) theperformance of student networks increase by 4.56\% percentage points onTiny-ImageNet-200 and 3.29\% on CIFAR-100 over student networks trained with noteacher and (2) 1.23\% and 1.72\% respectively over a  extit{hard-to-beat}baseline (Hinton et al.  2015).
http://arxiv.org/abs/2102.06548, Cockpit: A Practical Debugging Tool for Training Deep Neural Networks,When engineers train deep learning models  they are very much  flying blind .Commonly used approaches for real-time training diagnostics  such as monitoringthe train/test loss  are limited. Assessing a network's training process solelythrough these performance indicators is akin to debugging software withoutaccess to internal states through a debugger. To address this  we presentCockpit  a collection of instruments that enable a closer look into the innerworkings of a learning machine  and a more informative and meaningful statusreport for practitioners. It facilitates the identification of learning phasesand failure modes  like ill-chosen hyperparameters. These instruments leveragenovel higher-order information about the gradient distribution and curvature which has only recently become efficiently accessible. We believe that such adebugging tool  which we open-source for PyTorch  represents an important stepto improve troubleshooting the training process  reveal new insights  and helpdevelop novel methods and heuristics.
http://arxiv.org/abs/2102.06551, Unleashing the Power of Contrastive Self-Supervised Visual Models via  Contrast-Regularized Fine-Tuning,Contrastive self-supervised learning (CSL) leverages unlabeled data to trainmodels that provide instance-discriminative visual representations uniformlyscattered in the feature space. In deployment  the common practice is todirectly fine-tune models with the cross-entropy loss  which however may not bean optimal strategy. Although cross-entropy tends to separate inter-classfeatures  the resulted models still have limited capability of reducingintra-class feature scattering that inherits from pre-training  and thus maysuffer unsatisfactory performance on downstream tasks. In this paper  weinvestigate whether applying contrastive learning to fine-tuning would bringfurther benefits  and analytically find that optimizing the supervisedcontrastive loss benefits both class-discriminative representation learning andmodel optimization during fine-tuning. Inspired by these findings  we proposeContrast-regularized tuning (Core-tuning)  a novel approach for fine-tuningcontrastive self-supervised visual models. Instead of simply adding thecontrastive loss to the objective of fine-tuning  Core-tuning also generateshard sample pairs for more effective contrastive learning through a novelfeature mixup strategy  as well as improves the generalizability of the modelby smoothing the decision boundary via mixed samples. Extensive experiments onimage classification and semantic segmentation verify the effectiveness ofCore-tuning.
http://arxiv.org/abs/2102.06553, Intelligent Software Web Agents: A Gap Analysis,Semantic web technologies have shown their effectiveness  especially when itcomes to knowledge representation  reasoning  and data integrations. However the original semantic web vision  whereby machine readable web data could beautomatically actioned upon by intelligent software web agents  has yet to berealised. In order to better understand the existing technological challengesand opportunities  in this paper we examine the status quo in terms ofintelligent software web agents  guided by research with respect torequirements and architectural components  coming from that agents community.We start by collating and summarising requirements and core architecturalcomponents relating to intelligent software agent. Following on from this  weuse the identified requirements to both further elaborate on the semantic webagent motivating use case scenario  and to summarise different perspectives onthe requirements when it comes to semantic web agent literature. Finally  wepropose a hybrid semantic web agent architecture  discuss the role played byexisting semantic web standards  and point to existing work in the broadersemantic web community any beyond that could help us to make the semantic webagent vision a reality.
http://arxiv.org/abs/2102.06554, Improved LP-based Approximation Algorithms for Facility Location with  Hard Capacities,We present LP-based approximation algorithms for the capacitated facilitylocation problem (CFL)  a long-standing problem with intriguing unsettledcomplexity and literature dated back to the 90s. We present an elegantiterative rounding scheme for the MFN relaxation that yields an approximationguarantee of  \left(10+\sqrt{67} ight)/2 \approx 9.0927   a significantimprovement upon the previous LP-based ratio due to An et al in~2014. For CFLwith cardinality facility cost (CFL-CFC)  we present an LP-based 4 -approximation algorithm  which surpasses the long-standing ratio of~ 5  dueto Levi et al that ages up for decades since 2004. Our result considerablydeepens the current understanding for the CFL problem and indicates that anLP-based ratio strictly better than  5  in polynomial time for the generalproblem may still be possible to pursue.
http://arxiv.org/abs/2102.06555, TerraWatt: Sustaining Sustainable Computing of Containers in Containers,Each day the world inches closer to a climate catastrophe and asustainability revolution. To avoid the former and achieve the latter we musttransform our use of energy. Surprisingly  today's growing problem is thatthere is too much wind and solar power generation at the wrong times and in thewrong places.We argue for the construction of TerraWatt: a geographically-distributed large-scale  zero-carbon compute infrastructure using renewable energy andolder hardware. Delivering zero-carbon compute for general cloud workloads ischallenging due to spatiotemporal power variability. We describe the systemschallenges in using intermittent renewable power at scale to fuel such anolder  decentralized compute infrastructure.
http://arxiv.org/abs/2102.06557, Multi-access Coded Caching Scheme with Linear Sub-packetization using  PDAs,We consider multi-access coded caching problem introduced by Hachem et.al. where each user has access to  L  neighboring caches in a cyclic wrap-aroundfashion. We focus on the deterministic schemes for a specific class ofmulti-access coded caching problem based on the concept of PDA. We constructnew PDAs which specify the delivery scheme for the specific class ofmulti-access coded caching problem discussed in this paper. For the proposedscheme  the coding gain is larger than that of the state-of-the-art while thesub-packetization level varies only linearly with the number of users. Hence we achieve a lower transmission rate with the least sub-packetization levelcompared to the existing schemes.
http://arxiv.org/abs/2102.06558, Optimizing Inference Performance of Transformers on CPUs,The Transformer architecture revolutionized the field of natural languageprocessing (NLP). Transformers-based models (e.g.  BERT) power many importantWeb services  such as search  translation  question-answering  etc. Whileenormous research attention is paid to the training of those models  relativelylittle efforts are made to improve their inference performance. This papercomes to address this gap by presenting an empirical analysis of scalabilityand performance of inferencing a Transformer-based model on CPUs. Focusing onthe highly popular BERT model  we identify key components of the Transformerarchitecture where the bulk of the computation happens  and propose threeoptimizations to speed them up. The optimizations are evaluated using theinference benchmark from HuggingFace  and are shown to achieve the speedup ofup to x2.36. The considered optimizations do not require any changes to theimplementation of the models nor affect their accuracy.
http://arxiv.org/abs/2102.06559, MetaGrad: Adaptation using Multiple Learning Rates in Online Learning,We provide a new adaptive method for online convex optimization  MetaGrad that is robust to general convex losses but achieves faster rates for a broadclass of special functions  including exp-concave and strongly convexfunctions  but also various types of stochastic and non-stochastic functionswithout any curvature. We prove this by drawing a connection to the Bernsteincondition  which is known to imply fast rates in offline statistical learning.MetaGrad further adapts automatically to the size of the gradients. Its mainfeature is that it simultaneously considers multiple learning rates  which areweighted directly proportional to their empirical performance on the data usinga new meta-algorithm. We provide three versions of MetaGrad. The full matrixversion maintains a full covariance matrix and is applicable to learning tasksfor which we can afford update time quadratic in the dimension. The other twoversions provide speed-ups for high-dimensional learning tasks with an updatetime that is linear in the dimension: one is based on sketching  the other onrunning a separate copy of the basic algorithm per coordinate. We evaluate allversions of MetaGrad on benchmark online classification and regression tasks on which they consistently outperform both online gradient descent and AdaGrad.
http://arxiv.org/abs/2102.06560, Do-calculus enables causal reasoning with latent variable models,Latent variable models (LVMs) are probabilistic models where some of thevariables are hidden during training. A broad class of LVMshave a directedacyclic graphical structure. The directed structure suggests an intuitivecausal explanation of the data generating process. For example  a latent topicmodel suggests that topics cause the occurrence of a token. Despite thisintuitive causal interpretation  a directed acyclic latent variable modeltrained on data is generally insufficient for causal reasoning  as the requiredmodel parameters may not be uniquely identified. In this manuscript wedemonstrate that an LVM can answer any causal query posed post-training provided that the query can be identified from the observed variables accordingto the do-calculus rules. We show that causal reasoning can enhance a broadclass of LVM long established in the probabilistic modeling community  anddemonstrate its effectiveness on several case studies. These include a machinelearning model with multiple causes where there exists a set of latentconfounders and a mediator between the causes and the outcome variable  a studywhere the identifiable causal query cannot be estimated using the front-door orback-door criterion  a case study that captures unobserved crosstalk betweentwo biological signaling pathways  and a COVID-19 expert system that identifiesmultiple causal queries.
http://arxiv.org/abs/2102.06563, Deep Reinforcement Learning for Backup Strategies against Adversaries,Many defensive measures in cyber security are still dominated by heuristics catalogs of standard procedures  and best practices. Considering the case ofdata backup strategies  we aim towards mathematically modeling the underlyingthreat models and decision problems. By formulating backup strategies in thelanguage of stochastic processes  we can translate the challenge of findingoptimal defenses into a reinforcement learning problem. This enables us totrain autonomous agents that learn to optimally support planning of defenseprocesses. In particular  we tackle the problem of finding an optimal backupscheme in the following adversarial setting: Given  k  backup devices  the goalis to defend against an attacker who can infect data at one time but chooses todestroy or encrypt it at a later time  potentially also corrupting multiplebackups made in between. In this setting  the usual round-robin scheme  whichalways replaces the oldest backup  is no longer optimal with respect toavoidable exposure. Thus  to find a defense strategy  we model the problem as ahybrid discrete-continuous action space Markov decision process andsubsequently solve it using deep deterministic policy gradients. We show thatthe proposed algorithm can find storage device update schemes which match orexceed existing schemes with respect to various exposure metrics.
http://arxiv.org/abs/2102.06564, Discrete-Time Consensus Networks: Scalability  Grounding and  Countermeasures,We investigate the disruption of discrete-time consensus problems viagrounding. Loosely speaking  grounding a network occurs if the state of oneagent no longer responds to inputs from other agents and/or changes itsdynamics. Then  the agent becomes a leader or a so-called stubborn agent. Thedisruption of the agent can be caused by internal faults  safety protocols ordue to an external malicious attack. In this paper we investigate how groundingaffects expander graph families that usually exhibit good scaling propertieswith increasing network size. It is shown that the algebraic connectivity andeigenratio of the network decrease due to the grounding causing the performanceand scalability of the network to deteriorate  even to the point of losingconsensusability. We then present possible countermeasures to such disruptionsand discuss their practicality and limitations. In particular  for a specificcountermeasure of deliberately grounding additional nodes  we investigateextensively how to select additional nodes to ground and how many nodes we needto ground to recover the consensus performance. Our findings are supported by awide range of numerical simulations.
http://arxiv.org/abs/2102.06565, An Overview of Recommender Systems and Machine Learning in Feature  Modeling and Configuration,Recommender systems support decisions in various domains ranging from simpleitems such as books and movies to more complex items such as financialservices  telecommunication equipment  and software systems. In this context recommendations are determined  for example  on the basis of analyzing thepreferences of similar users. In contrast to simple items which can beenumerated in an item catalog  complex items have to be represented on thebasis of variability models (e.g.  feature models) since a complete enumerationof all possible configurations is infeasible and would trigger significantperformance issues. In this paper  we give an overview of a potential new lineof research which is related to the application of recommender systems andmachine learning techniques in feature modeling and configuration. In thiscontext  we give examples of the application of recommender systems and machinelearning and discuss future research issues.
http://arxiv.org/abs/2102.06570, ReLU Neural Networks for Exact Maximum Flow Computation,Understanding the great empirical success of artificial neural networks (NNs)from a theoretical point of view is currently one of the hottest researchtopics in computer science. In this paper we study the expressive power of NNswith rectified linear units from a combinatorial optimization perspective. Inparticular  we show that  given a directed graph with  n  nodes and  m  arcs there exists an NN of polynomial size that computes a maximum flow from anypossible real-valued arc capacities as input. To prove this  we develop thepseudo-code language Max-Affine Arithmetic Programs (MAAPs) and showequivalence between MAAPs and NNs concerning natural complexity measures. Wethen design a MAAP to exactly solve the Maximum Flow Problem  which translatesto an NN of size  \mathcal{O}(m^2 n^2) .
http://arxiv.org/abs/2102.06571, UAVs Path Deviation Attacks: Survey and Research Challenges,Recently  Unmanned Aerial Vehicles (UAVs) are employed for a plethora ofcivilian applications. Such flying vehicles can accomplish tasks under thepilot's eyesight within the range of a remote controller  or autonomouslyaccording to a certain pre-loaded path configuration. Different path deviationattacks can be performed by malicious users against UAVs. We classify suchattacks and the relative defenses based on the UAV's flight mode  i.e.  (i)First Person View (FPV)  (ii) civilian Global Navigation Satellite System based(GNSS)  and (iii) GNSS  plus  auxiliary technologies (GNSS+)  and on themultiplicity  i.e.  (i) Single UAV  and (ii) Multiple UAVs. We found that verylittle has been done to secure the FPV flight mode against path deviation. InGNSS mode  spoofing is the most worrisome attack. The best defense againstspoofing seems to be redundancy  such as adding vision chips to single UAV orusing multiple arranged UAVs. No specific attacks and defenses have been foundin literature for GNSS+ or for UAVs moving in group without a pre-orderedarrangement. These aspects require further investigation.
http://arxiv.org/abs/2102.06573, Bayesian Quadrature on Riemannian Data Manifolds,Riemannian manifolds provide a principled way to model nonlinear geometricstructure inherent in data. A Riemannian metric on said manifolds determinesgeometry-aware shortest paths and provides the means to define statisticalmodels accordingly. However  these operations are typically computationallydemanding. To ease this computational burden  we advocate probabilisticnumerical methods for Riemannian statistics. In particular  we focus onBayesian quadrature (BQ) to numerically compute integrals over normal laws onRiemannian manifolds learned from data. In this task  each function evaluationrelies on the solution of an expensive initial value problem. We show that byleveraging both prior knowledge and an active exploration scheme  BQsignificantly reduces the number of required evaluations and thus outperformsMonte Carlo methods on a wide range of integration problems. As a concreteapplication  we highlight the merits of adopting Riemannian geometry with ourproposed framework on a nonlinear dataset from molecular dynamics.
http://arxiv.org/abs/2102.06575, A Critical Look At The Identifiability of Causal Effects with Deep  Latent Variable Models,Using deep latent variable models in causal inference has attractedconsiderable interest recently  but an essential open question is theiridentifiability. While they have yielded promising results and theory exists onthe identifiability of some simple model formulations  we also know that causaleffects cannot be identified in general with latent variables. We investigatethis gap between theory and empirical results with theoretical considerationsand extensive experiments under multiple synthetic and real-world data sets using the causal effect variational autoencoder (CEVAE) as a case study. WhileCEVAE seems to work reliably under some simple scenarios  it does not identifythe correct causal effect with a misspecified latent variable or a complex datadistribution  as opposed to the original goals of the model. Our results showthat the question of identifiability cannot be disregarded  and we argue thatmore attention should be paid to it in future work.
http://arxiv.org/abs/2102.06578, Robust White Matter Hyperintensity Segmentation on Unseen Domain,Typical machine learning frameworks heavily rely on an underlying assumptionthat training and test data follow the same distribution. In medical imagingwhich increasingly begun acquiring datasets from multiple sites or scanners this identical distribution assumption often fails to hold due to systematicvariability induced by site or scanner dependent factors. Therefore  we cannotsimply expect a model trained on a given dataset to consistently work well  orgeneralize  on a dataset from another distribution. In this work  we addressthis problem  investigating the application of machine learning models tounseen medical imaging data. Specifically  we consider the challenging case ofDomain Generalization (DG) where we train a model without any knowledge aboutthe testing distribution. That is  we train on samples from a set ofdistributions (sources) and test on samples from a new  unseen distribution(target). We focus on the task of white matter hyperintensity (WMH) predictionusing the multi-site WMH Segmentation Challenge dataset and our local in-housedataset. We identify how two mechanically distinct DG approaches  namely domainadversarial learning and mix-up  have theoretical synergy. Then  we showdrastic improvements of WMH prediction on an unseen target domain.
http://arxiv.org/abs/2102.06580, Barriers for recent methods in geodesic optimization,We study a class of optimization problems including matrix scaling  matrixbalancing  multidimensional array scaling  operator scaling  and tensor scalingthat arise frequently in theory and in practice. Some of these problems  suchas matrix and array scaling  are convex in the Euclidean sense  but others suchas operator scaling and tensor scaling are \emph{geodesically convex} on adifferent Riemannian manifold. Trust region methods  which includebox-constrained Newton's method  are known to produce high precision solutionsvery quickly for matrix scaling and matrix balancing (Cohen et. al.  FOCS 2017 Allen-Zhu et. al. FOCS 2017)  and result in polynomial time algorithms for somegeodesically convex problems like operator scaling (Garg et. al. STOC 2018 B\ urgisser et. al. FOCS 2019). One is led to ask whether these guarantees alsohold for multidimensional array scaling and tensor scaling.We show that this is not the case by exhibiting instances with exponential\emph{diameter bound}: we construct polynomial-size instances of 3-dimensionalarray scaling and 3-tensor scaling whose approximate solutions all have doublyexponential condition number. Moreover  we study convex-geometric notions ofcomplexity known as margin and gap  which are used to bound the running timesof all existing optimization algorithms for such problems. We show that marginand gap are exponentially small for several problems including array scaling tensor scaling and polynomial scaling. Our results suggest that it isimpossible to prove polynomial running time bounds for tensor scaling based ondiameter bounds alone. Therefore  our work motivates the search for analoguesof more sophisticated algorithms  such as interior point methods  forgeodesically convex optimization that do not rely on polynomial diameterbounds.
http://arxiv.org/abs/2102.06583, Responsibility and verification: Importance value in temporal logics,We aim at measuring the influence of the nondeterministic choices of a partof a system on its ability to satisfy a specification. For this purpose  weapply the concept of Shapley values to verification as a means to evaluate howimportant a part of a system is. The importance of a component is measured bygiving its control to an adversary  alone or along with other components  andtesting whether the system can still fulfill the specification. We study thisidea in the framework of model-checking with various classical types oflinear-time specification  and propose several ways to transpose it tobranching ones. We also provide tight complexity bounds in almost every case.
http://arxiv.org/abs/2102.06584, Leveraging Artificial Intelligence to Analyze the COVID-19 Distribution  Pattern based on Socio-economic Determinants,The spatialization of socioeconomic data can be used and integrated withother sources of information to reveal valuable insights. Such data can beutilized to infer different variations  such as the dynamics of city dwellersand their spatial and temporal variability. This work focuses on suchapplications to explore the underlying association between socioeconomiccharacteristics of different geographical regions in Dublin  Ireland  and thenumber of confirmed COVID cases in each area. Our aim is to implement a machinelearning approach to identify demographic characteristics and spatial patterns.Spatial analysis was used to describe the pattern of interest in ElectoralDivisions (ED)  which are the legally defined administrative areas in theRepublic of Ireland for which population statistics are published from thecensus data. We used the most informative variables of the census data to modelthe number of infected people in different regions at ED level.
http://arxiv.org/abs/2102.06585, End-to-end Audio-visual Speech Recognition with Conformers,In this work  we present a hybrid CTC/Attention model based on a ResNet-18and Convolution-augmented transformer (Conformer)  that can be trained in anend-to-end manner. In particular  the audio and visual encoders learn toextract features directly from raw pixels and audio waveforms  respectively which are then fed to conformers and then fusion takes place via a Multi-LayerPerceptron (MLP). The model learns to recognise characters using a combinationof CTC and an attention mechanism. We show that end-to-end training  instead ofusing pre-computed visual features which is common in the literature  the useof a conformer  instead of a recurrent network  and the use of atransformer-based language model  significantly improve the performance of ourmodel. We present results on the largest publicly available datasets forsentence-level speech recognition  Lip Reading Sentences 2 (LRS2) and LipReading Sentences 3 (LRS3)  respectively. The results show that our proposedmodels raise the state-of-the-art performance by a large margin in audio-only visual-only  and audio-visual experiments.
http://arxiv.org/abs/2102.06587, Leveraging Artificial Intelligence to Analyze Citizens' Opinions on  Urban Green Space,Continued population growth and urbanization is shifting research to considerthe quality of urban green space over the quantity of these parks  woods  andwetlands. The quality of urban green space has been hitherto measured by expertassessments  including in-situ observations  surveys  and remote sensinganalyses. Location data platforms  such as TripAdvisor  can provide people'sopinion on many destinations and experiences  including UGS. This paperleverages Artificial Intelligence techniques for opinion mining and textclassification using such platform's reviews as a novel approach to urban greenspace quality assessments. Natural Language Processing is used to analyzecontextual information given supervised scores of words by implementingcomputational analysis. Such an application can support local authorities andstakeholders in their understanding of and justification for future investmentsin urban green space.
http://arxiv.org/abs/2102.06588, Numerical investigation of Mach number consistent Roe solvers for the  Euler equations of gas dynamics,While traditional approaches to prevent the carbuncle phenomenon in gasdynamics simulations increase the viscosity on entropy and shear waves nearshocks  it was quite recently suggested to instead decrease the viscosity onthe acoustic waves for low Mach numbers. The goal is to achieve what  in thispaper  we call Mach number consistency: for all waves  the numerical viscositydecreases with the same order of the Mach number when the Mach number tends tozero. We take the simple approach that was used for the proof of concepttogether with the simple model for the increased numerical viscosity on linearwaves and investigate the possibilities of combining both in an adaptive mannerwhile locally maintaining Mach number consistency.
http://arxiv.org/abs/2102.06589, Qualifying Software Engineers Undergraduates in DevOps -- Challenges of  Introducing Technical and Non-technical Concepts in a Project-oriented Course,The constant changes in the software industry  practices  and methodologiesimpose challenges to teaching and learning current software engineeringconcepts and skills. DevOps is particularly challenging because it coverstechnical concepts  such as pipeline automation  and non-technical ones  suchas team roles and project management. The present study investigates a coursesetup to introduce these concepts to software engineering undergraduates. Wedesigned the course by employing coding to associate DevOps concepts to Agile Lean  and Open source practices and tools. We present the main aspects of thisproject-oriented DevOps course  with 240 students enrolled in it since itsfirst offering in 2016. We conducted an empirical study  with both aquantitative and qualitative analysis  to evaluate this project-oriented coursesetup. We collected the data from the projects repository and studentsperceptions from a questionnaire. We mined 148 repositories (corresponding to72 projects) and obtained 86 valid responses to the questionnaire. We alsomapped the concepts which are more challenging to students learn fromexperience. The results evidence that first-hand experience facilitates thecomprehension of DevOps concepts and enriches classes discussions. We present aset of lessons learned  which may help professors better design and conductproject-oriented courses to cover DevOps concepts.
http://arxiv.org/abs/2102.06590, What helped  and what did not? An Evaluation of the Strategies to  Improve Continuous Integration,Continuous integration (CI) is a widely used practice in modern softwareengineering. Unfortunately  it is also an expensive practice - Google andMozilla estimate their CI systems in millions of dollars. There are a number oftechniques and tools designed to or having the potential to save the cost of CIor expand its benefit - reducing time to feedback. However  their benefits insome dimensions may also result in drawbacks in others. They may also bebeneficial in other scenarios where they are not designed to help. In thispaper  we perform the first exhaustive comparison of techniques to improve CI evaluating 14 variants of 10 techniques using selection and prioritizationstrategies on build and test granularity. We evaluate their strengths andweaknesses with 10 different cost and time-tofeedback saving metrics on 100real-world projects. We analyze the results of all techniques to understand thedesign decisions that helped different dimensions of benefit. We alsosynthesized those results to lay out a series of recommendations for thedevelopment of future research techniques to advance this area.
http://arxiv.org/abs/2102.06591, Online voluntary mentoring: Optimising the assignment of students and  mentors,After the closure of the schools in Hungary from March 2020 due to thepandemic  many students were left at home with no or not enough parental helpfor studying  and in the meantime some people had more free time andwillingness to help others in need during the lockdown. In this paper wedescribe the optimisation aspects of a joint NGO project for allocatingvoluntary mentors to students using a web-based coordination mechanism. Thegoal of the project has been to form optimal pairs and study groups by takinginto the preferences and the constraints of the participants. In this paper wepresent the optimisation concept  and the integer programming techniques usedfor solving the allocation problems. Furthermore  we conducted computationsimulations on real and generated data for evaluate the performance of thisdynamic matching scheme under different parameter settings.
http://arxiv.org/abs/2102.06593, Proof complexity of positive branching programs,We investigate the proof complexity of systems based on positive branchingprograms  i.e. non-deterministic branching programs (NBPs) where  for any0-transition between two nodes  there is also a 1-transition. Positive NBPscompute monotone Boolean functions  just like negation-free circuits orformulas  but constitute a positive version of (non-uniform) NL  rather than Por NC1  respectively.The proof complexity of NBPs was investigated in previous work by Buss  Dasand Knop  using extension variables to represent the dag-structure  over alanguage of (non-deterministic) decision trees  yielding the system eLNDT. Oursystem eLNDT+ is obtained by restricting their systems to a positive syntax similarly to how the 'monotone sequent calculus' MLK is obtained from the usualsequent calculus LK by restricting to negation-free formulas.Our main result is that eLNDT+ polynomially simulates eLNDT over positivesequents. Our proof method is inspired by a similar result for MLK by Atserias Galesi and Pudl\'ak  that was recently improved to a bona fide polynomialsimulation via works of Je\v{r}\'abek and Buss  Kabanets  Kolokolova andKouck\'y. Along the way we formalise several properties of counting functionswithin eLNDT+ by polynomial-size proofs and  as a case study  give explicitpolynomial-size poofs of the propositional pigeonhole principle.
http://arxiv.org/abs/2102.06599, A model for traffic incident prediction using emergency braking data,This article presents a model for traffic incident prediction. Specifically we address the fundamental problem of data scarcity in road traffic accidentprediction by training our model on emergency braking events instead ofaccidents. Based on relevant risk factors for traffic accidents andcorresponding data categories  we evaluate different options for preprocessingsparse data and different Machine Learning models. Furthermore  we present aprototype implementing a traffic incident prediction model for Germany based onemergency braking data from Mercedes-Benz vehicles as well as weather  trafficand road data  respectively. After model evaluation and optimisation  we foundthat a Random Forest model trained on artificially balanced (under-sampled)data provided the highest classification accuracy of 85% on the originalimbalanced data. Finally  we present our conclusions and discuss further work;from gathering more data over a longer period of time to build strongerclassification systems  to addition of internal factors such as the driver'svisual and cognitive attention.
http://arxiv.org/abs/2102.06601, Adversarial Branch Architecture Search for Unsupervised Domain  Adaptation,Unsupervised Domain Adaptation (UDA) is a key field in visual recognition  asit enables robust performances across different visual domains. In the deeplearning era  the performance of UDA methods has been driven by better lossesand by improved network architectures  specifically the addition of auxiliarydomain-alignment branches to pre-trained backbones. However  all the neuralarchitectures proposed so far are hand-crafted  which might hinder furtherprogress.The current copious offspring of Neural Architecture Search (NAS) onlyalleviates hand-crafting so far  as it requires labels for model selection which are not available in UDA  and is usually applied to the wholearchitecture  while using pre-trained models is a strict requirement for highperformance. No prior work has addressed these aspects in the context of NASfor UDA.Here we propose an Adversarial Branch Architecture Search (ABAS) for UDA  tolearn the auxiliary branch network from data without handcrafting. Our maincontribution include i. a novel data-driven ensemble approach for modelselection  to circumvent the lack of target labels  and ii. a pipeline toautomatically search for the best performing auxiliary branch.To the best of our knowledge  ABAS is the first NAS method for UDA to complywith a pre-trained backbone  a strict requirement for high performance. ABASoutputs both the optimal auxiliary branch and its trained parameters. Whenapplied to two modern UDA techniques  DANN and ALDA  it improves performance onthree standard CV datasets (Office31  Office-Home and PACS). In all cases  ABASrobustly finds the branch architectures which yield best performances. Codewill be released.
http://arxiv.org/abs/2102.06602, Low precision logarithmic number systems: Beyond base-2,Logarithmic number systems (LNS) are used to represent real numbers in manyapplications using a constant base raised to a fixed-point exponent making itsdistribution exponential. This greatly simplifies hardware multiply  divide andsquare root. LNS with base-2 is most common  but in this paper we show that forlow-precision LNS the choice of base has a significant impact.We make four main contributions. First  LNS is not closed under addition andsubtraction  so the result is approximate. We show that choosing a suitablebase can manipulate the distribution to reduce the average error. Second  weshow that low-precision LNS addition and subtraction can be implementedefficiently in logic rather than commonly used ROM lookup tables  thecomplexity of which can be reduced by an appropriate choice of base. A similareffect is shown where the result of arithmetic has greater precision than theinput. Third  where input data from external sources is not expected to be inLNS  we can reduce the conversion error by selecting a LNS base to match theexpected distribution of the input. Thus  there is no one base which gives theglobal optimum  and base selection is a trade-off between different factors.Fourth  we show that circuits realized in LNS require lower area and powerconsumption for short word lengths.
http://arxiv.org/abs/2102.06603, DeepGLEAM: an hybrid mechanistic and deep learning model for COVID-19  forecasting,We introduce DeepGLEAM  a hybrid model for COVID-19 forecasting. DeepGLEAMcombines a mechanistic stochastic simulation model GLEAM with deep learning. Ituses deep learning to learn the correction terms from GLEAM  which leads toimproved performance. We further integrate various uncertainty quantificationmethods to generate confidence intervals. We demonstrate DeepGLEAM onreal-world COVID-19 mortality forecasting tasks.
http://arxiv.org/abs/2102.06604, Learning Depth via Leveraging Semantics: Self-supervised Monocular Depth  Estimation with Both Implicit and Explicit Semantic Guidance,Self-supervised depth estimation has made a great success in learning depthfrom unlabeled image sequences. While the mappings between image and pixel-wisedepth are well-studied in current methods  the correlation between image  depthand scene semantics  however  is less considered. This hinders the network tobetter understand the real geometry of the scene  since the contextual clues contribute not only the latent representations of scene depth  but also thestraight constraints for depth map. In this paper  we leverage the two benefitsby proposing the implicit and explicit semantic guidance for accurateself-supervised depth estimation. We propose a Semantic-aware Spatial FeatureAlignment (SSFA) scheme to effectively align implicit semantic features withdepth features for scene-aware depth estimation. We also propose asemantic-guided ranking loss to explicitly constrain the estimated depth mapsto be consistent with real scene contextual properties. Both semantic labelnoise and prediction uncertainty is considered to yield reliable depthsupervisions. Extensive experimental results show that our method produces highquality depth maps which are consistently superior either on complex scenes ordiverse semantic categories  and outperforms the state-of-the-art methods by asignificant margin.
http://arxiv.org/abs/2102.06605, Destination similarity based on implicit user interest,With the digitization of travel industry  it is more and more important tounderstand users from their online behaviors. However  online travel industrydata are more challenging to analyze due to extra sparseness  dispersed userhistory actions  fast change of user interest and lack of direct or indirectfeedbacks. In this work  a new similarity method is proposed to measure thedestination similarity in terms of implicit user interest. By comparing theproposed method to several other widely used similarity measures in recommendersystems  the proposed method achieves a significant improvement on travel data.Key words: Destination similarity  Travel industry  Recommender System Implicit user interest
http://arxiv.org/abs/2102.06607, Rethinking Eye-blink: Assessing Task Difficulty through Physiological  Representation of Spontaneous Blinking,Continuous assessment of task difficulty and mental workload is essential inimproving the usability and accessibility of interactive systems. Eye trackingdata has often been investigated to achieve this ability  with reports on thelimited role of standard blink metrics. Here  we propose a new approach to theanalysis of eye-blink responses for automated estimation of task difficulty.The core module is a time-frequency representation of eye-blink  which aims tocapture the richness of information reflected on blinking. In our first study we show that this method significantly improves the sensitivity to taskdifficulty. We then demonstrate how to form a framework where the representedpatterns are analyzed with multi-dimensional Long Short-Term Memory recurrentneural networks for their non-linear mapping onto difficulty-relatedparameters. This framework outperformed other methods that used hand-engineeredfeatures. This approach works with any built-in camera  without requiringspecialized devices. We conclude by discussing how Rethinking Eye-blink canbenefit real-world applications.
http://arxiv.org/abs/2102.06610, Bias-Free Scalable Gaussian Processes via Randomized Truncations,Scalable Gaussian Process methods are computationally attractive  yetintroduce modeling biases that require rigorous study. This paper analyzes twocommon techniques: early truncated conjugate gradients (CG) and random Fourierfeatures (RFF). We find that both methods introduce a systematic bias on thelearned hyperparameters: CG tends to underfit while RFF tends to overfit. Weaddress these issues using randomized truncation estimators that eliminate biasin exchange for increased variance. In the case of RFF  we show that thebias-to-variance conversion is indeed a trade-off: the additional varianceproves detrimental to optimization. However  in the case of CG  our unbiasedlearning procedure meaningfully outperforms its biased counterpart with minimaladditional computation.
http://arxiv.org/abs/2102.06613, Efficient Conditional GAN Transfer with Knowledge Propagation across  Classes,Generative adversarial networks (GANs) have shown impressive results in bothunconditional and conditional image generation. In recent literature  it isshown that pre-trained GANs  on a different dataset  can be transferred toimprove the image generation from a small target data. The same  however  hasnot been well-studied in the case of conditional GANs (cGANs)  which providesnew opportunities for knowledge transfer compared to unconditional setup. Inparticular  the new classes may borrow knowledge from the related old classes or share knowledge among themselves to improve the training. This motivates usto study the problem of efficient conditional GAN transfer with knowledgepropagation across classes. To address this problem  we introduce a new GANtransfer method to explicitly propagate the knowledge from the old classes tothe new classes. The key idea is to enforce the popularly used conditionalbatch normalization (BN) to learn the class-specific information of the newclasses from that of the old classes  with implicit knowledge sharing among thenew ones. This allows for an efficient knowledge propagation from the oldclasses to the new classes  with the BN parameters increasing linearly with thenumber of new classes. The extensive evaluation demonstrates the clearsuperiority of the proposed method over state-of-the-art competitors forefficient conditional GAN transfer tasks. The code will be available at:https://github.com/mshahbazi72/cGANTransfer
http://arxiv.org/abs/2102.06614, A Parameterised Quantum Circuit Approach to Point Set Matching,Point set registration is one of the challenging tasks in areas such aspattern recognition  computer vision and image processing. Efficientperformance of this task has been a hot topic of research due to its widespreadapplications. We propose a parameterised quantum circuit learning approach topoint set matching problem. The proposed method benefits from a kernel-basedquantum generative model that: 1) is able to find all possible optimal matchingsolution angles  2) is potentially able to show quantum learning supremacy  and3) benefits from kernel-embedding techniques and integral probability metricsfor the definition of a powerful loss function. Moreover  the theoreticalframework has been backed up by satisfactory preliminary and proof of conceptexperimental results.
http://arxiv.org/abs/2102.06616, Certified Defenses: Why Tighter Relaxations May Hurt Training?,Certified defenses based on convex relaxations are an established techniquefor training provably robust models. The key component is the choice ofrelaxation  varying from simple intervals to tight polyhedra. Paradoxically however  it was empirically observed that training with tighter relaxations canworsen certified robustness. While several methods were designed to partiallymitigate this issue  the underlying causes are poorly understood. In this workwe investigate the above phenomenon and show that tightness may not be thedetermining factor for reduced certified robustness. Concretely  we identifytwo key features of relaxations that impact training dynamics: continuity andsensitivity. We then experimentally demonstrate that these two factors explainthe drop in certified robustness when using popular relaxations. Further  weshow  for the first time  that it is possible to successfully train withtighter relaxations (i.e.  triangle)  a result supported by our two properties.Overall  we believe the insights of this work can help drive the systematicdiscovery of new effective certified defenses.
http://arxiv.org/abs/2102.06621, Explaining Neural Scaling Laws,The test loss of well-trained neural networks often follows precise power-lawscaling relations with either the size of the training dataset or the number ofparameters in the network. We propose a theory that explains and connects thesescaling laws. We identify variance-limited and resolution-limited scalingbehavior for both dataset and model size  for a total of four scaling regimes.The variance-limited scaling follows simply from the existence of awell-behaved infinite data or infinite width limit  while theresolution-limited regime can be explained by positing that models areeffectively resolving a smooth data manifold. In the large width limit  thiscan be equivalently obtained from the spectrum of certain kernels  and wepresent evidence that large width and large dataset resolution-limited scalingexponents are related by a duality. We exhibit all four scaling regimes in thecontrolled setting of large random feature and pretrained models and test thepredictions empirically on a range of standard architectures and datasets. Wealso observe several empirical relationships between datasets and scalingexponents: super-classing image tasks does not change exponents  while changinginput distribution (via changing datasets or adding noise) has a strong effect.We further explore the effect of architecture aspect ratio on scalingexponents.
http://arxiv.org/abs/2102.06622, Proximal and Federated Random Reshuffling,Random Reshuffling (RR)  also known as Stochastic Gradient Descent (SGD)without replacement  is a popular and theoretically grounded method forfinite-sum minimization. We propose two new algorithms: Proximal and FederatedRandom Reshuffing (ProxRR and FedRR). The first algorithm  ProxRR  solvescomposite convex finite-sum minimization problems in which the objective is thesum of a (potentially non-smooth) convex regularizer and an average of  n smooth objectives. We obtain the second algorithm  FedRR  as a special case ofProxRR applied to a reformulation of distributed problems with eitherhomogeneous or heterogeneous data. We study the algorithms' convergenceproperties with constant and decreasing stepsizes  and show that they haveconsiderable advantages over Proximal and Local SGD. In particular  our methodshave superior complexities and ProxRR evaluates the proximal operator once perepoch only. When the proximal operator is expensive to compute  this smalldifference makes ProxRR up to  n  times faster than algorithms that evaluatethe proximal operator in every iteration. We give examples of practicaloptimization tasks where the proximal operator is difficult to compute andProxRR has a clear advantage. Finally  we corroborate our results withexperiments on real data sets.
http://arxiv.org/abs/2102.06624, Higher Order Generalization Error for First Order Discretization of  Langevin Diffusion,We propose a novel approach to analyze generalization error fordiscretizations of Langevin diffusion  such as the stochastic gradient Langevindynamics (SGLD). For an  \epsilon  tolerance of expected generalization error it is known that a first order discretization can reach this target if we run \Omega(\epsilon^{-1} \log (\epsilon^{-1}) )  iterations with \Omega(\epsilon^{-1})  samples. In this article  we show that with additionalsmoothness assumptions  even first order methods can achieve arbitrarilyruntime complexity. More precisely  for each  N>0   we provide a sufficientsmoothness condition on the loss function such that a first orderdiscretization can reach  \epsilon  expected generalization error given \Omega( \epsilon^{-1/N} \log (\epsilon^{-1}) )  iterations with \Omega(\epsilon^{-1})  samples.
http://arxiv.org/abs/2102.06626, An Investigation of End-to-End Models for Robust Speech Recognition,End-to-end models for robust automatic speech recognition (ASR) have not beensufficiently well-explored in prior work. With end-to-end models  one couldchoose to preprocess the input speech using speech enhancement techniques andtrain the model using enhanced speech. Another alternative is to pass the noisyspeech as input and modify the model architecture to adapt to noisy speech. Asystematic comparison of these two approaches for end-to-end robust ASR has notbeen attempted before. We address this gap and present a detailed comparison ofspeech enhancement-based techniques and three different model-based adaptationtechniques covering data augmentation  multi-task learning  and adversariallearning for robust ASR. While adversarial learning is the best-performingtechnique on certain noise types  it comes at the cost of degrading cleanspeech WER. On other relatively stationary noise types  a new speechenhancement technique outperformed all the model-based adaptation techniques.This suggests that knowledge of the underlying noise type can meaningfullyinform the choice of adaptation technique.
http://arxiv.org/abs/2102.06627, A High Speed Integrated Quantum Random Number Generator with on-Chip  Real-Time Randomness Extraction,The security of electronic devices has become a key requisite for therapidly-expanding pervasive and hyper-connected world. Robust securityprotocols ensuring secure communication  device's resilience to attacks authentication control and users privacy need to be implemented. Random NumberGenerators (RNGs) are the fundamental primitive in most secure protocols but often  also the weakest one. Establishing security in billions of devicesrequires high quality random data generated at a sufficiently high throughput.On the other hand  the RNG should exhibit a high integration level with on-chipextraction to remove  in real time  potential imperfections. We present thefirst integrated Quantum RNG (QRNG) in a standard CMOS technology node. TheQRNG is based on a parallel array of independent Single-Photon Avalanche Diodes(SPADs)  homogeneously illuminated by a DC-biased LED  and co-integrated logiccircuits for postprocessing. We describe the randomness generation process andwe prove the quantum origin of entropy. We show that co-integration ofcombinational logic  even of high complexity  does not affect the quality ofrandomness. Our CMOS QRNG can reach up to 400 Mbit/s throughput with low powerconsumption. Thanks to the use of standard CMOS technology and a modulararchitecture  our QRNG is suitable for a highly scalable solution.
http://arxiv.org/abs/2102.06632, Echo State Networks for Reinforcement Learning,Echo State Networks (ESNs) are a type of single-layer recurrent neuralnetwork with randomly-chosen internal weights and a trainable output layer. Weprove under mild conditions that a sufficiently large Echo State Network (ESN)can approximate the value function of a broad class of stochastic anddeterministic control problems. Such control problems are generallynon-Markovian. We describe how the ESN can form the basis for novel (andcomputationally efficient) reinforcement learning algorithms in a non-Markovianframework. We demonstrate this theory with two examples. In the first  we usean ESN to solve a deterministic  partially observed  control problem which is asimple game we call `Bee World'. In the second example  we consider astochastic control problem inspired by a market making problem in mathematicalfinance. In both cases we can compare the dynamics of the algorithms withanalytic solutions to show that even after only a single reinforcement policyiteration the algorithms perform with reasonable skill.
http://arxiv.org/abs/2102.06633, Disentanglement for audio-visual emotion recognition using multitask  setup,Deep learning models trained on audio-visual data have been successfully usedto achieve state-of-the-art performance for emotion recognition. In particular models trained with multitask learning have shown additional performanceimprovements. However  such multitask models entangle information between thetasks  encoding the mutual dependencies present in label distributions in thereal world data used for training. This work explores the disentanglement ofmultimodal signal representations for the primary task of emotion recognitionand a secondary person identification task. In particular  we developed amultitask framework to extract low-dimensional embeddings that aim to captureemotion specific information  while containing minimal information related toperson identity. We evaluate three different techniques for disentanglement andreport results of up to 13% disentanglement while maintaining emotionrecognition performance.
http://arxiv.org/abs/2102.06634, The Univalence Principle,The Univalence Principle is the statement that equivalent mathematicalstructures are indistinguishable. We prove a general version of this principlethat applies to all set-based  categorical  and higher-categorical structuresdefined in a non-algebraic and space-based style  as well as models ofhigher-order theories such as topological spaces. In particular  we formulate ageneral definition of indiscernibility for objects of any such structure  and acorresponding univalence condition that generalizes Rezk's completenesscondition for Segal spaces and ensures that all equivalences of structures arelevelwise equivalences.Our work builds on Makkai's First-Order Logic with Dependent Sorts  but isexpressed in Voevodsky's Univalent Foundations (UF)  extending previous work onthe Structure Identity Principle and univalent categories in UF. This enablesindistinguishability to be expressed simply as identification  and yields aformal theory that is interpretable in classical homotopy theory  but also inother higher topos models. It follows that Univalent Foundations is a fullyequivalence-invariant foundation for higher-categorical mathematics  asintended by Voevodsky.
http://arxiv.org/abs/2102.06635, Unsupervised Ground Metric Learning using Wasserstein Eigenvectors,Optimal Transport (OT) defines geometrically meaningful  Wasserstein distances  used in machine learning applications to compare probabilitydistributions. However  a key bottleneck is the design of a  ground  cost whichshould be adapted to the task under study. In most cases  supervised metriclearning is not accessible  and one usually resorts to some ad-hoc approach.Unsupervised metric learning is thus a fundamental problem to enabledata-driven applications of Optimal Transport. In this paper  we propose forthe first time a canonical answer by computing the ground cost as a positiveeigenvector of the function mapping a cost to the pairwise OT distances betweenthe inputs. This map is homogeneous and monotone  thus framing unsupervisedmetric learning as a non-linear Perron-Frobenius problem. We provide criteriato ensure the existence and uniqueness of this eigenvector. In addition  weintroduce a scalable computational method using entropic regularization  which- in the large regularization limit - operates a principal component analysisdimensionality reduction. We showcase this method on synthetic examples anddatasets. Finally  we apply it in the context of biology to the analysis of ahigh-throughput single-cell RNA sequencing (scRNAseq) dataset  to improve cellclustering and infer the relationships between genes in an unsupervised way.
http://arxiv.org/abs/2102.06638, COVID-19 detection from scarce chest x-ray image data using deep  learning,In the current COVID-19 pandemic situation  there is an urgent need to screeninfected patients quickly and accurately. Using deep learning models trained onchest X-ray images can become an efficient method for screening COVID-19patients in these situations. Deep learning approaches are already widely usedin the medical community. However  they require a large amount of data to beaccurate. The open-source community collectively has made efforts to collectand annotate the data  but it is not enough to train an accurate deep learningmodel. Few-shot learning is a sub-field of machine learning that aims to learnthe objective with less amount of data. In this work  we have experimented withwell-known solutions for data scarcity in deep learning to detect COVID-19.These include data augmentation  transfer learning  and few-shot learning  andunsupervised learning. We have also proposed a custom few-shot learningapproach to detect COVID-19 using siamese networks. Our experimental resultsshowcased that we can implement an efficient and accurate deep learning modelfor COVID-19 detection by adopting the few-shot learning approaches even withless amount of data. Using our proposed approach we were able to achieve 96.4%accuracy an improvement from 83% using baseline models.
http://arxiv.org/abs/2102.06645, Some Hoeffding- and Bernstein-type Concentration Inequalities,We prove concentration inequalities for functions of independent randomvariables {under} sub-gaussian and sub-exponential conditions. The utility ofthe inequalities is demonstrated by an extension of the now classical method ofRademacher complexities to Lipschitz function classes and unboundedsub-exponential distribution.
http://arxiv.org/abs/2102.06648, DEEPF0: End-To-End Fundamental Frequency Estimation for Music and Speech  Signals,We propose a novel pitch estimation technique called DeepF0  which leveragesthe available annotated data to directly learns from the raw audio in adata-driven manner. F0 estimation is important in various speech processing andmusic information retrieval applications. Existing deep learning models forpitch estimations have relatively limited learning capabilities due to theirshallow receptive field. The proposed model addresses this issue by extendingthe receptive field of a network by introducing the dilated convolutionalblocks into the network. The dilation factor increases the network receptivefield exponentially without increasing the parameters of the modelexponentially. To make the training process more efficient and faster  DeepF0is augmented with residual blocks with residual connections. Our empiricalevaluation demonstrates that the proposed model outperforms the baselines interms of raw pitch accuracy and raw chroma accuracy even using 77.4% fewernetwork parameters. We also show that our model can capture reasonably wellpitch estimation even under the various levels of accompaniment noise.
http://arxiv.org/abs/2102.06650, Segmentation-Renormalized Deep Feature Modulation for Unpaired Image  Harmonization,Deep networks are now ubiquitous in large-scale multi-center imaging studies.However  the direct aggregation of images across sites is contraindicated fordownstream statistical and deep learning-based image analysis due toinconsistent contrast  resolution  and noise. To this end  in the absence ofpaired data  variations of Cycle-consistent Generative Adversarial Networkshave been used to harmonize image sets between a source and target domain.Importantly  these methods are prone to instability  contrast inversion intractable manipulation of pathology  and steganographic mappings which limittheir reliable adoption in real-world medical imaging. In this work  based onan underlying assumption that morphological shape is consistent across imagingsites  we propose a segmentation-renormalized image translation framework toreduce inter-scanner heterogeneity while preserving anatomical layout. Wereplace the affine transformations used in the normalization layers withingenerative networks with trainable scale and shift parameters conditioned onjointly learned anatomical segmentation embeddings to modulate features atevery level of translation. We evaluate our methodologies against recentbaselines across several imaging modalities (T1w MRI  FLAIR MRI  and OCT) ondatasets with and without lesions. Segmentation-renormalization for translationGANs yields superior image harmonization as quantified by Inception distances demonstrates improved downstream utility via post-hoc segmentation accuracy and improved robustness to translation perturbation and self-adversarialattacks.
http://arxiv.org/abs/2102.06652, Joint Dereverberation and Separation with Iterative Source Steering,We propose a new algorithm for joint dereverberation and blind sourceseparation (DR-BSS). Our work builds upon the IRLMA-T framework that applies aunified filter combining dereverberation and separation. One drawback of thisframework is that it requires several matrix inversions  an operationinherently costly and with potential stability issues. We leverage the recentlyintroduced iterative source steering (ISS) updates to propose two algorithmsmitigating this issue. Albeit derived from first principles  the firstalgorithm turns out to be a natural combination of weighted prediction error(WPE) dereverberation and ISS-based BSS  applied alternatingly. In this case we manage to reduce the number of matrix inversion to only one per iterationand source. The second algorithm updates the ILRMA-T matrix using onlysequential ISS updates requiring no matrix inversion at all. Its implementationis straightforward and memory efficient. Numerical experiments demonstrate thatboth methods achieve the same final performance as ILRMA-T in terms of severalrelevant objective metrics. In the important case of two sources  the number ofiterations required is also similar.
http://arxiv.org/abs/2102.06655, Uncertainty-Aware Semi-supervised Method using Large Unlabelled and  Limited Labeled COVID-19 Data,The new coronavirus has caused more than 1 million deaths and continues tospread rapidly. This virus targets the lungs  causing respiratory distresswhich can be mild or severe. The X-ray or computed tomography (CT) images oflungs can reveal whether the patient is infected with COVID-19 or not. Manyresearchers are trying to improve COVID-19 detection using artificialintelligence. In this paper  relying on Generative Adversarial Networks (GAN) we propose a Semi-supervised Classification using Limited Labelled Data (SCLLD)for automated COVID-19 detection. Our motivation is to develop learning methodwhich can cope with scenarios that preparing labelled data is time consuming orexpensive. We further improved the detection accuracy of the proposed method byapplying Sobel edge detection. The GAN discriminator output is a probabilityvalue which is used for classification in this work. The proposed system istrained using 10 000 CT scans collected from Omid hospital. Also  we validateour system using the public dataset. The proposed method is compared with otherstate of the art supervised methods such as Gaussian processes. To the best ofour knowledge  this is the first time a COVID-19 semi-supervised detectionmethod is presented. Our method is capable of learning from a mixture oflimited labelled and unlabelled data where supervised learners fail due to lackof sufficient amount of labelled data. Our semi-supervised training methodsignificantly outperforms the supervised training of Convolutional NeuralNetwork (CNN) in case labelled training data is scarce. Our method has achievedan accuracy of 99.60%  sensitivity of 99.39%  and specificity of 99.80% whereCNN (trained supervised) has achieved an accuracy of 69.87%  sensitivity of94%  and specificity of 46.40%.
http://arxiv.org/abs/2102.06656, Mind the beat: detecting audio onsets from EEG recordings of music  listening,We propose a deep learning approach to predicting audio event onsets inelectroencephalogram (EEG) recorded from users as they listen to music. We usea publicly available dataset containing ten contemporary songs and concurrentlyrecorded EEG. We generate a sequence of onset labels for the songs in ourdataset and trained neural networks (a fully connected network (FCN) and arecurrent neural network (RNN)) to parse one second windows of input EEG topredict one second windows of onsets in the audio. We compare our RNN networkto both the standard spectral-flux based novelty function and the FCN. We findthat our RNN was able to produce results that reflected its ability togeneralize better than the other methods.Since there are no pre-existing works on this topic  the numbers presented inthis paper may serve as useful benchmarks for future approaches to thisresearch problem.
http://arxiv.org/abs/2102.06657, Explaining predictive models using Shapley values and non-parametric  vine copulas,The original development of Shapley values for prediction explanation reliedon the assumption that the features being described were independent. If thefeatures in reality are dependent this may lead to incorrect explanations.Hence  there have recently been attempts of appropriately modelling/estimatingthe dependence between the features. Although the proposed methods clearlyoutperform the traditional approach assuming independence  they have theirweaknesses. In this paper we propose two new approaches for modelling thedependence between the features.Both approaches are based on vine copulas  which are flexible tools formodelling multivariate non-Gaussian distributions able to characterise a widerange of complex dependencies.The performance of the proposed methods is evaluated on simulated data setsand a real data set. The experiments demonstrate that the vine copulaapproaches give more accurate approximations to the true Shapley values thanits competitors.
http://arxiv.org/abs/2102.06659, Interview Hoarding,Many centralized matching markets are preceded by interviews between theparticipants. We study the impact on the final match of an increase to thenumber of interviews one side of the market can participate in. Our motivationis the match between residents and hospitals where  due to the COVID-19pandemic  interviews for the 2020-21 season of the NRMP match have switched toa virtual format. This has drastically reduced the cost to applicants ofaccepting interview offers. However  the reduction in cost is not symmetricsince applicants  not programs  bore most of the costs of in-person interviews.We show that if doctors are willing to accept more interviews but the hospitalsdo not increase the number of interviews they offer  no doctor will be betteroff and potentially many doctors will be harmed. This adverse consequenceresults from a mechanism we describe as interview hoarding. We prove thisanalytically and characterize optimal mitigation strategies for special cases.We use simulations to extend the insights from our analytical results to moregeneral settings.
http://arxiv.org/abs/2102.06661, Guided Variational Autoencoder for Speech Enhancement With a Supervised  Classifier,Recently  variational autoencoders have been successfully used to learn aprobabilistic prior over speech signals  which is then used to perform speechenhancement. However  variational autoencoders are trained on clean speechonly  which results in a limited ability of extracting the speech signal fromnoisy speech compared to supervised approaches. In this paper  we propose toguide the variational autoencoder with a supervised classifier separatelytrained on noisy speech. The estimated label is a high-level categoricalvariable describing the speech signal (e.g. speech activity) allowing for amore informed latent distribution compared to the standard variationalautoencoder. We evaluate our method with different types of labels on realrecordings of different noisy environments. Provided that the label betterinforms the latent distribution and that the classifier achieves goodperformance  the proposed approach outperforms the standard variationalautoencoder and a conventional neural network-based supervised approach.
http://arxiv.org/abs/2102.06662, Leveraging Global Parameters for Flow-based Neural Posterior Estimation,Inferring the parameters of a stochastic model based on experimentalobservations is central to the scientific method. A particularly challengingsetting is when the model is strongly indeterminate  i.e.  when distinct setsof parameters yield identical observations. This arises in many practicalsituations  such as when inferring the distance and power of a radio source (isthe source close and weak or far and strong?) or when estimating the amplifiergain and underlying brain activity of an electrophysiological experiment. Inthis work  we present a method for cracking such indeterminacy by exploitingadditional information conveyed by an auxiliary set of observations sharingglobal parameters. Our method extends recent developments in simulation-basedinference(SBI) based on normalizing flows to Bayesian hierarchical models. Wevalidate quantitatively our proposal on a motivating example amenable toanalytical solutions  and then apply it to invert a well known non-linear modelfrom computational neuroscience.
http://arxiv.org/abs/2102.06663, Stability and Convergence of Stochastic Gradient Clipping: Beyond  Lipschitz Continuity and Smoothness,Stochastic gradient algorithms are often unstable when applied to functionsthat do not have Lipschitz-continuous and/or bounded gradients. Gradientclipping is a simple and effective technique to stabilize the training processfor problems that are prone to the exploding gradient problem. Despite itswidespread popularity  the convergence properties of the gradient clippingheuristic are poorly understood  especially for stochastic problems. This paperestablishes both qualitative and quantitative convergence results of theclipped stochastic (sub)gradient method (SGD) for non-smooth convex functionswith rapidly growing subgradients. Our analyses show that clipping enhances thestability of SGD and that the clipped SGD algorithm enjoys finite convergencerates in many cases. We also study the convergence of a clipped method withmomentum  which includes clipped SGD as a special case  for weakly convexproblems under standard assumptions. With a novel Lyapunov analysis  we showthat the proposed method achieves the best-known rate for the considered classof problems  demonstrating the effectiveness of clipped methods also in thisregime. Numerical results confirm our theoretical developments.
http://arxiv.org/abs/2102.06665, Flying V and Reference Aircraft Evacuation Simulation and Comparison,A preliminary comparison of evacuation times of the Flying V and the AirbusA350-900 is presented in this study. A simple simulation tool based on thetechnique of cellular automata was created to model the evacuation process fordifferent closed door configurations. Certification regulations state that thetime to evacuate a civil aircraft in case of an emergency with half of all exitdoors closed must be less than 90 seconds. The results of this study indicatethat the shorter V shaped cabin has advantages over the longer conventionalreference cabin for cases when passengers need to evacuate towards the front orthe back of the aircraft. Disadvantages occur when the passengers in the Vshaped cabin need to evacuate more towards one side (left or right wing) of theaircraft. A more detailed simulation model to further investigate these casesis currently created by the authors.
http://arxiv.org/abs/2102.06666, Mediastinal lymph nodes segmentation using 3D convolutional neural  network ensembles and anatomical priors guiding,As lung cancer evolves  the presence of enlarged and potentially malignantlymph nodes must be assessed to properly estimate disease progression andselect the best treatment strategy. Following the clinical guidelines estimation of short-axis diameter and mediastinum station are paramount forcorrect diagnosis. A method for accurate and automatic segmentation is hencedecisive for quantitatively describing lymph nodes. In this study  the use of3D convolutional neural networks  either through slab-wise schemes or theleveraging of downsampled entire volumes  is investigated. Furthermore  thepotential impact from simple ensemble strategies is considered. As lymph nodeshave similar attenuation values to nearby anatomical structures  we suggestusing the knowledge of other organs as prior information to guide thesegmentation task. To assess the segmentation and instance detectionperformances  a 5-fold cross-validation strategy was followed over a dataset of120 contrast-enhanced CT volumes. For the 1178 lymph nodes with a short-axisdiameter  \geq10  mm  our best performing approach reached a patient-wiserecall of 92%  a false positive per patient ratio of 5  and a segmentationoverlap of 80.5%. The method performs similarly well across all stations.Fusing a slab-wise and a full volume approach within an ensemble schemegenerated the best performances. The anatomical priors guiding strategy ispromising  yet a larger set than four organs appears needed to generate anoptimal benefit. A larger dataset is also mandatory  given the wide range ofexpressions a lymph node can exhibit (i.e.  shape  location  and attenuation) and contrast uptake variations.
http://arxiv.org/abs/2102.06668, Convex Synthesis of Accelerated Gradient Algorithms,We present a convex solution for the design of generalized acceleratedgradient algorithms for strongly convex objective functions with Lipschitzcontinuous gradients. We utilize integral quadratic constraints and the Youlaparameterization from robust control theory to formulate a solution of thealgorithm design problem as a convex semi-definite program. We establishexplicit formulas for the optimal convergence rates and extend the proposedsynthesis solution to extremum control problems.
http://arxiv.org/abs/2102.06671, Robust and integrative Bayesian neural networks for likelihood-free  parameter inference,State-of-the-art neural network-based methods for learning summary statisticshave delivered promising results for simulation-based likelihood-free parameterinference. Existing approaches require density estimation as a post-processingstep building upon deterministic neural networks  and do not take networkprediction uncertainty into account. This work proposes a robust integratedapproach that learns summary statistics using Bayesian neural networks  anddirectly estimates the posterior density using categorical distributions. Anadaptive sampling scheme selects simulation locations to efficiently anditeratively refine the predictive posterior of the network conditioned onobservations. This allows for more efficient and robust convergence oncomparatively large prior spaces. We demonstrate our approach on benchmarkexamples and compare against related methods.
http://arxiv.org/abs/2102.06673, Sequential Neural Posterior and Likelihood Approximation,We introduce the sequential neural posterior and likelihood approximation(SNPLA) algorithm. SNPLA is a normalizing flows-based algorithm for inferencein implicit models. Thus  SNPLA is a simulation-based inference method thatonly requires simulations from a generative model. Compared to similar methods the main advantage of SNPLA is that our method jointly learns both theposterior and the likelihood. SNPLA completely avoid Markov chain Monte Carlosampling and correction-steps of the parameter proposal function that areintroduced in similar methods  but that can be numerically unstable orrestrictive. Over four experiments  we show that SNPLA performs competitivelywhen utilizing the same number of model simulations as used in other methods even though the inference problem for SNPLA is more complex due to the jointlearning of posterior and likelihood function.
http://arxiv.org/abs/2102.06674, Comparison of Machine Learning Classifiers to Predict Patient Survival  and Genetics of GBM: Towards a Standardized Model for Clinical Implementation,Radiomic models have been shown to outperform clinical data for outcomeprediction in glioblastoma (GBM). However  clinical implementation is limitedby lack of parameters standardization. We aimed to compare nine machinelearning classifiers  with different optimization parameters  to predictoverall survival (OS)  isocitrate dehydrogenase (IDH) mutation O-6-methylguanine-DNA-methyltransferase (MGMT) promoter methylation  epidermalgrowth factor receptor (EGFR) VII amplification and Ki-67 expression in GBMpatients  based on radiomic features from conventional and advanced MR. 156adult patients with pathologic diagnosis of GBM were included. Three tumoralregions were analyzed: contrast-enhancing tumor  necrosis and non-enhancingtumor  selected by manual segmentation. Radiomic features were extracted with acustom version of Pyradiomics  and selected through Boruta algorithm. A GridSearch algorithm was applied when computing 4 times K-fold cross validation(K=10) to get the highest mean and lowest spread of accuracy. Once optimalparameters were identified  model performances were assessed in terms of AreaUnder The Curve-Receiver Operating Characteristics (AUC-ROC). Metaheuristic andensemble classifiers showed the best performance across tasks. xGB obtainedmaximum accuracy for OS (74.5%)  AB for IDH mutation (88%)  MGMT methylation(71 7%)  Ki-67 expression (86 6%)  and EGFR amplification (81 6%). Bestperforming features shed light on possible correlations between MR and tumorhistology.
http://arxiv.org/abs/2102.06679, Hybrid quantum convolutional neural networks model for COVID-19  prediction using chest X-Ray images,Despite the great efforts to find an effective way for COVID-19 prediction the virus nature and mutation represent a critical challenge to diagnose thecovered cases. However  developing a model to predict COVID-19 via Chest X-Ray(CXR) images with accurate performance is necessary to help in early diagnosis.In this paper  a hybrid quantum-classical convolutional Neural Networks (HQCNN)model used the random quantum circuits (RQCs) as a base to detect COVID-19patients with CXR images. A collection of 6952 CXR images  including 1161COVID-19  1575 normal  and 5216 pneumonia images  were used as a dataset inthis work. The proposed HQCNN model achieved higher performance with anaccuracy of 98.4\% and a sensitivity of 99.3\% on the first dataset cases.Besides  it obtained an accuracy of 99\% and a sensitivity of 99.7\% on thesecond dataset cases. Also  it achieved accuracy  and sensitivity of 88.6\% and 88.7\%  respectively  on the third multi-class dataset cases. Furthermore the HQCNN model outperforms various models in balanced accuracy  precision F1-measure  and AUC-ROC score. The experimental results are achieved by theproposed model prove its ability in predicting positive COVID-19 cases.
http://arxiv.org/abs/2102.06681, Tightening the Dependence on Horizon in the Sample Complexity of  Q-Learning,Q-learning  which seeks to learn the optimal Q-function of a Markov decisionprocess (MDP) in a model-free fashion  lies at the heart of reinforcementlearning. When it comes to the synchronous setting (such that independentsamples for all state-action pairs are drawn from a generative model in eachiteration)  substantial progress has been made recently towards understandingthe sample efficiency of Q-learning. To yield an entrywise \varepsilon -accurate estimate of the optimal Q-function  state-of-the-arttheory requires at least an order of \frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^5\varepsilon^{2}}  samples for a \gamma -discounted infinite-horizon MDP with state space  \mathcal{S}  andaction space  \mathcal{A} . In this work  we sharpen the sample complexity ofsynchronous Q-learning to an order of \frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^4\varepsilon^2}  (up to somelogarithmic factor) for any  0<\varepsilon <1   leading to an order-wiseimprovement in terms of the effective horizon  \frac{1}{1-\gamma} . Analogousresults are derived for finite-horizon MDPs as well. Our finding unveils theeffectiveness of vanilla Q-learning  which matches that of speedy Q-learningwithout requiring extra computation and storage. A key ingredient of ouranalysis lies in the establishment of novel error decompositions andrecursions  which might shed light on how to analyze finite-sample performanceof other Q-learning variants.
http://arxiv.org/abs/2102.06684, Infinitely Deep Bayesian Neural Networks with Stochastic Differential  Equations,We perform scalable approximate inference in a recently-proposed family ofcontinuous-depth Bayesian neural networks. In this model class  uncertaintyabout separate weights in each layer produces dynamics that follow a stochasticdifferential equation (SDE). We demonstrate gradient-based stochasticvariational inference in this infinite-parameter setting  producingarbitrarily-flexible approximate posteriors. We also derive a novel gradientestimator that approaches zero variance as the approximate posterior approachesthe true posterior. This approach further inherits the memory-efficienttraining and tunable precision of neural ODEs.
http://arxiv.org/abs/2102.06685, Bayesian Neural Network Priors Revisited,Isotropic Gaussian priors are the de facto standard for modern Bayesianneural network inference. However  such simplistic priors are unlikely toeither accurately reflect our true beliefs about the weight distributions  orto give optimal performance. We study summary statistics of neural networkweights in different networks trained using SGD. We find that fully connectednetworks (FCNNs) display heavy-tailed weight distributions  while convolutionalneural network (CNN) weights display strong spatial correlations. Buildingthese observations into the respective priors leads to improved performance ona variety of image classification datasets. Moreover  we find that these priorsalso mitigate the cold posterior effect in FCNNs  while in CNNs we see strongimprovements at all temperatures  and hence no reduction in the cold posterioreffect.
http://arxiv.org/abs/2102.06687, Sparse Bayesian Causal Forests for Heterogeneous Treatment Effects  Estimation,This paper develops a sparsity-inducing version of Bayesian Causal Forests  arecently proposed nonparametric causal regression model that employs BayesianAdditive Regression Trees and is specifically designed to estimateheterogeneous treatment effects using observational data. The sparsity-inducingcomponent we introduce is motivated by empirical studies where the number ofpre-treatment covariates available is non-negligible  leading to differentdegrees of sparsity underlying the surfaces of interest in the estimation ofindividual treatment effects. The extended version presented in this work which we name Sparse Bayesian Causal Forest  is equipped with an additionalpair of priors allowing the model to adjust the weight of each covariatethrough the corresponding number of splits in the tree ensemble. These priorsimprove the model's adaptability to sparse settings and allow to perform fullyBayesian variable selection in a framework for treatment effects estimation and thus to uncover the moderating factors driving heterogeneity. In addition the method allows prior knowledge about the relevant confounding pre-treatmentcovariates and the relative magnitude of their impact on the outcome to beincorporated in the model. We illustrate the performance of our method insimulated studies  in comparison to Bayesian Causal Forest and otherstate-of-the-art models  to demonstrate how it scales up with an increasingnumber of covariates and how it handles strongly confounded scenarios. Finally we also provide an example of application using real-world data.
http://arxiv.org/abs/2102.06690, Pareto Optimal Model Selection in Linear Bandits,We study a model selection problem in the linear bandit setting  where thelearner must adapt to the dimension of the optimal hypothesis class on the flyand balance exploration and exploitation. More specifically  we assume asequence of nested linear hypothesis classes with dimensions  d_1 < d_2 <\dots   and the goal is to automatically adapt to the smallest hypothesis classthat contains the true linear model. Although previous papers provide variousguarantees for this model selection problem  the analysis therein either worksin favorable cases when one can cheaply conduct statistical testing to locatethe right hypothesis class or is based on the idea of  corralling  multiplebase algorithms which often performs relatively poorly in practice. These worksalso mainly focus on upper bounding the regret. In this paper  we firstestablish a lower bound showing that  even with a fixed action set  adaptationto the unknown intrinsic dimension  d_\star  comes at a cost: there is noalgorithm that can achieve the regret bound  \widetilde{O}(\sqrt{d_\star T}) simultaneously for all values of  d_\star . We also bring new ideas  i.e. constructing virtual mixture-arms to effectively summarize useful information into the model selection problem in linear bandits. Under a mild assumption onthe action set  we design a Pareto optimal algorithm with guarantees matchingthe rate in the lower bound. Experimental results confirm our theoreticalresults and show advantages of our algorithm compared to prior work.
http://arxiv.org/abs/2102.06695, Enhancing into the codec: Noise Robust Speech Coding with  Vector-Quantized Autoencoders,Audio codecs based on discretized neural autoencoders have recently beendeveloped and shown to provide significantly higher compression levels forcomparable quality speech output. However  these models are tightly coupledwith speech content  and produce unintended outputs in noisy conditions. Basedon VQ-VAE autoencoders with WaveRNN decoders  we develop compressor-enhancerencoders and accompanying decoders  and show that they operate well in noisyconditions. We also observe that a compressor-enhancer model performs better onclean speech inputs than a compressor model trained only on clean speech.
http://arxiv.org/abs/2102.06696, A Generative Model for Hallucinating Diverse Versions of Super  Resolution Images,Traditionally  the main focus of image super-resolution techniques is onrecovering the most likely high-quality images from low-quality images  using aone-to-one low- to high-resolution mapping. Proceeding that way  we ignore thefact that there are generally many valid versions of high-resolution imagesthat map to a given low-resolution image. We are tackling in this work theproblem of obtaining different high-resolution versions from the samelow-resolution image using Generative Adversarial Models. Our learning approachmakes use of high frequencies available in the training high-resolution imagesfor preserving and exploring in an unsupervised manner the structuralinformation available within these images. Experimental results on the CelebAdataset confirm the effectiveness of the proposed method  which allows thegeneration of both realistic and diverse high-resolution images fromlow-resolution images.
http://arxiv.org/abs/2102.06697, A fast and scalable computational framework for goal-oriented linear  Bayesian optimal experimental design: Application to optimal sensor placement,Optimal experimental design (OED) is a principled framework for maximizinginformation gained from limited data in inverse problems. Unfortunately conventional methods for OED are prohibitive when applied to expensive modelswith high-dimensional parameters  as we target here. We develop a fast andscalable computational framework for goal-oriented OED of large-scale Bayesianlinear inverse problems that finds sensor locations to maximize the expectedinformation gain (EIG) for a predicted quantity of interest. By employinglow-rank approximations of appropriate operators  an online-offlinedecomposition  and a new swapping greedy algorithm  we are able to maximize EIGat a cost measured in model solutions that is independent of the problemdimensions. We demonstrate the efficiency  accuracy  and both data- andparameter-dimension independence of the proposed algorithm for a contaminanttransport inverse problem with infinite-dimensional parameter field.
http://arxiv.org/abs/2102.06700, Potential Singularity Formation of 3D Axisymmetric Navier-Stokes  Equations with Degenerate Variable Diffusion Coefficients,In this paper  we present strong numerical evidences that the  3 Daxisymmetric Navier-Stokes equations with degenerate variable diffusioncoefficients and smooth initial data of finite energy develop a potentialfinite time locally self-similar singularity at the origin. An importantfeature of this potential singularity is that the solution develops a two-scaletraveling wave that travels towards the origin. The two-scale feature ischaracterized by the property that the center of the traveling wave approachesto the origin at a slower rate than the rate of the collapse of thesingularity. The driving mechanism for this potential singularity is due to twoantisymmetric vortex dipoles that generate a strong shearing layer in both theradial and axial velocity fields  which transport the solution first towards z=0  and then towards the symmetry axis  r=0 . The initial condition isdesigned in such a way that it generates a positive feedback loop that enforcesa strong nonlinear alignment of vortex stretching  leading to a stable locallyself-similar blowup at the origin. We perform careful resolution study andasymptotic scaling analysis to provide further support of the potential finitetime locally self-similar blowup.
http://arxiv.org/abs/2102.06701, Bayesian Uncertainty Estimation of Learned Variational MRI  Reconstruction,Recent deep learning approaches focus on improving quantitative scores ofdedicated benchmarks  and therefore only reduce the observation-related(aleatoric) uncertainty. However  the model-immanent (epistemic) uncertainty isless frequently systematically analyzed. In this work  we introduce a Bayesianvariational framework to quantify the epistemic uncertainty. To this end  wesolve the linear inverse problem of undersampled MRI reconstruction in avariational setting. The associated energy functional is composed of a datafidelity term and the total deep variation (TDV) as a learned parametricregularizer. To estimate the epistemic uncertainty we draw the parameters ofthe TDV regularizer from a multivariate Gaussian distribution  whose mean andcovariance matrix are learned in a stochastic optimal control problem. Inseveral numerical experiments  we demonstrate that our approach yieldscompetitive results for undersampled MRI reconstruction. Moreover  we canaccurately quantify the pixelwise epistemic uncertainty  which can serveradiologists as an additional resource to visualize reconstruction reliability.
http://arxiv.org/abs/2102.06702, Numerical analysis of a model of two phase compressible fluid flow,We consider a model of a binary mixture of two immiscible compressiblefluids. We propose a numerical scheme and discuss its basic properties:Stability  consistency  convergence. The convergence is established via themethod of generalized weak solutions combined with the weak-strong uniquenessprinciple.
http://arxiv.org/abs/2102.06704, Material absorption-based carrier generation model for modeling  optoelectronic devices,The generation rate of photocarriers in optoelectronic materials is commonlycalculated using the Poynting vector in the frequency domain. In time-domainapproaches where the nonlinear coupling between electromagnetic (EM) waves andphotocarriers can be accounted for  the Poynting vector model is no longerapplicable. One main reason is that the photocurrent radiates low-frequency EMwaves out of the spectrum of the source  e.g.  terahertz (THz) waves aregenerated in THz photoconductive antennas. These frequency components do notcontribute to the photocarrier generation since the corresponding photon energyis smaller than the optoelectronic material's bandgap energy. However  theinstantaneous Poynting vector does not distinguish the power flux of differentfrequency components. This work proposes a material absorption-based modelcapable of calculating the carrier generation rate accurately in the timedomain. Using the Lorentz dispersion model with poles reside in the opticalfrequency region  the instantaneous optical absorption  which corresponds tothe power dissipation in the polarization  is calculated and used to calculatethe generation rate. The Lorentz model is formulated with an auxiliarydifferential equation method that updates the polarization current density from which the absorbed optical power corresponding to each Lorentz pole isdirectly calculated in the time domain. Examples show that the proposed modelis more accurate than the Poynting vector-based model and is stable even whenthe generated low-frequency component is strong.
